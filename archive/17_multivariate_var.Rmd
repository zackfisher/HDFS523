# Vector Autoregressive Models {#chapter-17}

```{r, echo = F}
button <-  "position: relative; 
            top: -25px; 
            left: 85%;   
            color: white;
            font-weight: bold;
            background: #4B9CD3;
            border: 1px #3079ED solid;
            box-shadow: inset 0 1px 0 #80B0FB"
```

```{r, echo=FALSE, results='asis'}
codefolder::bookdown(init = "show", style = button)
```

This chapter covers the basics of vector autoregressive models. We start with a univariate autoregressive model and proceed to a discussion of vector autoregressive processes. 

## Introduction

Due in part to *technological development* the last few decades have witnessed an explosion in the availability of *time-series data*.

**Common Sources of Time Series**: 

- ecological and immunological phenomena
- sensor-based physiological measurements (e.g. heart rate and skin conductance)
- health and movement data (calorie tracking, fitbit, GPS, etc.)
- daily diary and ecological momentary assessment data
- measures of emotional states
- measures of social interdependence (social network data)

#### Theories of Processes

Theories in the social, health and behavioral sciences often describe mechanisms or systems occurring *within* individuals.

A large portion of applied psychological work is focused on the analysis of variation  within individuals, across time.

**Discussion Question**

In relation to your own work, can you come up with a theory that operates entirely at the between-person level? If so what is theory? If not, why is this difficult? 

#### Interesting Directions for Time Series Analysis

**Questions and Opportunities**: 

- Integration of multiple time-scales and individuals
- Complex interactions across levels of analysis
- *Characterizing heterogeneity between and within individuals*

#### Between-Person Heterogeneity

A number of promising methods exist for addressing between-person heterogeneity in time series.  

**Grouping Individuals with Similar Dynamics**

- Mixture modeling of dynamic processes 
- Clustering individuals by common dynamics 

**Multilevel Approaches**

- Multilevel VAR modeling 

**Common and Unique Variation**

- Joint and Individual Variance Explained 
- Group Iterative Multiple Method Estimation 
- multi-VAR and extensions

## The N=1 AR Model

First, let's consider the most simple time-series model. Here we only show 4 timepoints, typically the time series will be considerably longer.


![N = 1 AR Model](imgs/forum_02.jpg)


### Random Shocks

First, let's consider the random shock $z_t, t=1,\dots,T$.

These random shocks have a number of interesting properties.

- Random shocks are latent variables.
- Random shocks are identically distributed with variance $\psi$, e.g., $z_t \sim \mathcal{N}(0,\psi)$.
- A shock at any time point influences the process variable at two or more time points.
- Intuitively, shocks represent all the unmodeled influences that effect the process variable. 
- Shock are the dynamic part of the time series model. 
- Random shocks perturb the system and change its course over time. 
- Although errors, random shocks differ from measurement  errors in meaningful ways.
  - Random shocks differ from other errors in that they influence the process variable at more than 1 time point. 

### Autoregressive Parameter

Next, let's consider the autorgressive coefficient $\alpha_1$. In an AR(1) model, where only one lag is considered, the AR coefficients are generally the parameter of interest.

**The interest in an individual’s autoregressive parameter stems from the fact that this parameter indexes the time it takes an individual to recover from a random shock and
return to equilibrium.**

- An AR close to zero implies that there is little carryover from one measurement occasion to the next and recovery is thus instant.
- An AR parameter close to one implies that there is considerable carryover
between consecutive measurement occasions, such that perturbations continue to have an effect on subsequent occasions.
- AR parameters can be interpreted as a measure of inertia, stability or regulatory weakness.

**Positive and Negative AR Parameters**

- A positive AR parameter could be expected for many psychological processes, such as that of mood, attitudes symptoms. 
- A negative alpha indicates that if an individual has a high score at one occasion, the score at the next occasion  is likely to be low, and vice versa. 

### AR(1) Model Example

Consider a simple AR(1) model,


$$
y_{t} = \alpha_{1}y_{t-1} + z_{t}\nonumber 
$$
which matches the path diagram:

![N = 1 AR(1) Model](imgs/forum_02.jpg)

Looking at the path diagram above, suppose

- $y_{t}$ are hourly measurements of concentration.
- $z_t$ Unobserved events impacting concentration.

Here we might imagine that during one of these intervals the subject gets some bad news. This might result in a decreased concentration levels at that measurement occasion, and this effect effect may then linger for the next few measurement occasions (as a result of an AR effect). The larger the AR coefficient, the longer it will take for the student to return to their baseline concentration level. 


**Discussion Question:**

Choose a single variable (or construct) from your own research. 

1) How would you measure this variable across time (e.g. self-report via mobile phone)?
2) How often would you measure the variable to capture important fluctuations?
3) If you fit a simple AR(1) model to the repeated measures how might you interpret the autoregressive effect? 


### Fitting a AR(1) Model in R

Let's load the libraries we'll need for this chapter.

```{r}
library("psych")
require("httr")
library("ggplot2")
library("mlVAR")
```

Consider some example data from the Many Analysts project: https://osf.io/h3djy/

The experience sampling methodology (ESM) has been positioned as a promising opportunity for personalized medicine in psychiatry. A requisite for moving ESM towards clinical practice is that outcomes of person-centered analyses are not contingent on the researcher. In this study, we crowdsourced the analysis of one individual patient’s ESM data to 12 prominent research teams to investigate how much researchers vary in their analytical approach towards individual time series data and to what degree outcomes vary based on analytical choices.


This is a description of the project taken from https://osf.io/wfzr7/:

**Research question**: 

What symptom(s) would you advise the treating clinician to target subsequent treatment on, based on a person-centered analysis of this particular patient’s EMA data? To answer this research question, please analyze the dataset in whatever manner your research team sees as best. 

**The Dataset**:

The data used in this project stem from the research lab of Aaron Fisher (UC Berkeley).

From this dataset one subject was chosen for the current project. The data were saved as a csv file (comma delimited). Selection of the dataset was based on the following criteria: the subject has a primary diagnosis of major depressive disorder (MDD), the dataset includes more than 100 time points, and the dataset has some missingness.

The subject (ID 3) was a white 25-year old male with a primary diagnosis of MDD and a comorbid generalized anxiety disorder (GAD). His Hamilton Rating Scale for Depression score was 16 and his Hamilton Rating Scale for Anxiety score was 15. 

In the experiment by Fisher, subjects were asked to fill out a survey on their mobile phones 4 times a day for 30 days. Surveys were conducted at a random time within each of four 3-hourblocks, with the additional constraint that surveys should be given at least 30 minutes apart. At each sampling occasion, subjects were prompted to think about the period of time since the last survey. Items were scored on a visual analogue slider ranging from 0 to 100 with the endpoints “not at all” and “as much as possible”. 

**Variables in Dataset**

- energetic (felt energetic)
- enthusiastic (felt enthusiastic)
- content (felt content)
- irritable (felt irritable)
- restless (felt restless)
- worried (felt worried)
- guilty (felt worthless or guilty)
- afraid (felt frightened or afraid)
- anhedonia (felt a loss of interest or pleasure)
- angry (felt angry)
- hopeless (felt hopeless)
- down (felt down or depressed)
- positive (felt positive)
- fatigue (felt fatigued)
- tension (experienced muscle tension)
- concentrate (experienced difficulty concentrating)
- accepted (felt accepted or supported)
- threatened (felt threatened, judged, or intimidated)
- ruminate (dwelled on the past)
- avoid_act (avoided activities)
- reassure (sought reassurance)
- procrast (procrastinated)
- hours (how many hours did you sleep last night?)
- difficult (experienced difficulty falling or staying asleep)
- unsatisfy (experienced restless or unsatisfying sleep)
- avoid_people (avoided people)

Let's read in the data:

```{r}
url <- 'https://osf.io/tcnpd//?action=download'
filename <- 'osf_dataframe.csv'
GET(url, write_disk(filename, overwrite = TRUE))
data <- read.csv(filename, header=TRUE, na.strings="NA")
head(data)
```

Now, let's use the `concentrate` variable.

```{r}
df <- data[,"concentrate", drop = FALSE]
```

The first step in an autoregressive analysis typically involves lagging the data. We can do this for a single-la as follows:

```{r}
first           <- df[1:(nrow(df)-1), ]
second          <- df[2:(nrow(df)  ), ]
lagged_df       <- data.frame(first, second)
colnames(lagged_df) <- c(paste0(colnames(df), "lag"), colnames(df))
head(lagged_df)
```

Now, with our data lagged we can fit an AR model using OLS as follows:

```{r}
summary(lm(concentrate~concentratelag,data=lagged_df))
```


### AR Coefficients Example

Let's consider what a time series looks like when it is generated with different AR values. 

The time series with an autoregressive coefficient of $0.1$ is the solid black line. The time series with an autoregressive coefficient of $0.9$ is the dashed line.

```{r}
set.seed(1234)
y1 <- arima.sim(list(order=c(1,0,0), ar=.1), n=200)
y2 <- arima.sim(list(order=c(1,0,0), ar=.9), n=200)
ts.plot(y1,y2,lty=c(1:2))
```


## The N=1 VAR Model

Consider extending our AR(1) model to include hours measurements of concentration ($y_{1}$) and fatigue ($y_{1}$). We now have a VAR(1) model where we are interested in two constructs, and their relation to eachother, across time.

![N = 1 VAR Model](imgs/path_var.jpg)

Our interpretation of the AR parameter remains the same except now we also have cross-regressive parameters to consider (e.g. $\alpha_{12}$ and $\alpha_{21}$).

**Interpretation of Directed Paths**

- Auto-regressive paths (e.g. $Attention_{t-1}$ -> $Attention_{t}$ or $\alpha_{11}$)
  -  AR parameters can be interpreted as a measure of inertia, stability or regulatory weakness.
- Cross-regressive paths (e.g. $Attention_{t-1}$ -> $Fatigue_{t}$or $\alpha_{21}$)
  -  cross-regressive paths indicate cross-construct influence, or buffering
  
  
**The Vector Autoregressive (VAR) Model**

In Economics widespread adoption of VAR models in the mid-1980s marked the beginning of a sea change in modeling and forecasting practice (Allen et al., 2006).

**Advantages of the Unrestricted VAR Model**

- Offers a concise interpretation of inter-variable relations
- Visualized easily using path or network connection diagrams
- Allow for the inclusion of many potentially relevant variables


Finally, we can extend the VAR model to handle additional variables we might think are related to our process of interest.

![N = 1 VAR Model](imgs/var1_alone.jpg)


### Fitting an VAR(1) Model in R

Now, let's use the `concentrate` and `fatigue` variables.

```{r}
df <- data[,c("concentrate","fatigue"), drop = FALSE]
```


Many software routines won't require us to lag our data before prior to the analysis because this is done within the program itself. However, the process to lag our data for a multiple variable VAR model follows directly from the AR code. 

```{r}
first           <- df[1:(nrow(df)-1), ]
second          <- df[2:(nrow(df)  ), ]
lagged_df       <- data.frame(first, second)
colnames(lagged_df) <- c(paste0(colnames(df), "lag"), colnames(df))
head(lagged_df)
```

Now, we can use the `vars` package to fit a VAR(1) model for this single individual.

```{r}
fit <- vars::VAR(df[complete.cases(df),], p = 1, type = "const") # p is the lag
coef(fit)

transition_mat <- as.matrix(do.call("rbind",lapply(seq_along(colnames(df)), function(x) {
  fit$varresult[[x]]$coefficients
})))
rownames(transition_mat) <- colnames(df)
transition_mat
```


### Stationarity and Non-Stationarity

Consider a multivariate time series, $\{\mathbf{X}_{t}\}_{t \in \mathbb{Z}} = \{(X_{j,t})_{j=1,\dots,d}\}$, that follows a vector autoregressive model of order $p$, 
$$
\label{varp1}
\mathbf{X}_{t} = 
  \boldsymbol{\Phi}_{1} \mathbf{X}_{t-1} + 
  \ldots + 
  \boldsymbol{\Phi}_{p} \mathbf{X}_{t-p} + 
  \boldsymbol{\varepsilon}_{t}, \quad t \in \mathbb{Z}.
$$


- $\boldsymbol{\Phi}_{1}, \dots, \boldsymbol{\Phi}_{p}$ are $d \times d$ transition matrices.
- $\boldsymbol{\varepsilon}_{t}$ is a white noise series, $\{\boldsymbol{\varepsilon}_{t}\}_{t \in \mathbb{Z}} \sim \text{WN}(\mathbf{0}, \boldsymbol{\Sigma}_{\boldsymbol{\varepsilon}})$ with $\mathbb{E}(\boldsymbol{\varepsilon}_{t})=0$, $\mathbb{E}(\boldsymbol{\varepsilon}_{t}\boldsymbol{\varepsilon}_{s}^{'})=0$ for $s \neq t$.
- For simplicity we assume $\mathbf{X}_{t}$ is of zero mean and \eqref{varp1} satisfies the stability conditions discussed earlier.


As we showed earlier it is common to estimate \eqref{varp1} using ordinary least-squares (OLS) regression,
$$
(\widehat{\boldsymbol{\Phi}}_1,\ldots,\widehat{\boldsymbol{\Phi}}_p) = \argmin_{\boldsymbol{\Phi}_1,\ldots,\boldsymbol{\Phi}_p} \sum_{t=p+1}^T  \| \mathbf{X}_t - \boldsymbol{\Phi}_1 \mathbf{X}_{t-1} - \ldots - \boldsymbol{\Phi}_p \mathbf{X}_{t-p}\|_{2}^2.
$$
where a unique stationary solution can be found when the eigenvalues of $\boldsymbol{\Phi}$ are smaller than one in absolute value.


```{r}
non_stationary_AR <- matrix(c(1,2,3,4), 2, 2)
stationary_AR     <- matrix(c(.1,.2,.3,.4), 2, 2)

max(abs(eigen(non_stationary_AR)$values)) < 1
max(abs(eigen(stationary_AR)$values)) < 1
```


Perhaps more importantly, what do stationary and non-stationary series look like? Here are some simplistic examples

![Examples of (Non)-Stationarity](imgs/nonstation.jpg)

### A Note on path Diagrams

Both examples below represent bivariate VAR(1) models. It is important to be comfortable with both representations as they are each commonly presented in the literature. Note the two diagrams primarily difer in how they represent the autorgressive and cross-regressive effects. In Path Diagram 1, both are illustrated using directed arrows. In Path Diagram 2, autoregressive paths are indicated by directed self-loops, while cross-regressive paths are indicated by directed arrows. 

![Alternative Path Diagram 1](imgs/path_var.jpg)

![Alternative Path Diagram 2](imgs/var_path2.jpg)

Note that Path Diagram 1 does not include information about the means (intercepts), which is indicated by the triangles in Path Diagram 2. Likewise, Path Diagram 2 does not include a visual depiction of the errors, indicated by $z_{1}$ and $z_{2}$ in Path Diagram 1.



## Introducing the Multi-level VAR

We are often in the situation where we collect intensive longitudinal data on multiple individuals. 

Fitting VAR(1) models for every individual in the sample is certainly an option. 

However, this can create a large amount of information to summarize and may lead to the recovery of spurious relations depending on how many observations are available for each subject.

One option to consider in this scenario is the **multi-level VAR model**.

In the multi-level VAR we have the option for each parameter to follow a distribution across individuals. Typically, parameters are expected to follow normal distribution.



**Distribution of Parameters**

![Image from Epskamp (2017)](imgs/distrib.jpg)


**Example Individuals**

![Image from Epskamp (2017)](imgs/twopeeps.jpg)

### Model

Now, for illustrative purposes we will use the `mlVAR` package to demonstrate how one can estimate multi-level VAR models. However, one should note the modeling approach taken in the`mlVAR` package is unique and differs from what we have discussed in class.

For example, the`mlVAR` package also estimates a contemporaneous network following the initial multi-level estimation procedure. We will cover the basics of these routines but one should consults Epskamp, Waldrop, Mottus and Borsboom (2018) for a detailed explanation of the modeling procedures.

The first-stage model can be written as

$$
\mathbf{y}_{t,p} = \boldsymbol{\mu}_{p} + \mathbf{B}_{p}(\mathbf{y}_{t,p}-\boldsymbol{\mu}_{p}) +\boldsymbol{\varepsilon}_{t,p}
$$

where $\boldsymbol{\varepsilon}_{t,p} \sim \mathcal{N}(\mathbf{0},\boldsymbol{\theta}_{p})$ and $\boldsymbol{\theta}_{p}^{-1}=\mathbf{K}_{p}^{(\Theta)}$. Here, $\boldsymbol{\mu}_{p}$ is the mean vector for subject $p$, $\mathbf{B}_{p}$ is the person-specific transition matrix, and $\mathbf{K}_{p}$ is the person-specific contemporaneous relations.

Unfortunately, many multi-level software routines will be unable to estimate $\boldsymbol{\mu}_{p}$ in a straightforward manner. In accordance with Hamaker \& Grasman (2014) and Epskamp et al. (2018) one possibility is to use $\bar{\mathbf{y}}_{p}$ in place of $\boldsymbol{\mu}_{p}$. This simplication allows us to estimate the VAR model as a series of univariate regressions (within-subject centered predictors) using generic multi-level software such as `nlme` or `lme4`.

In this context the Level 1 model can be written as 

$$
y_{t,p,i} = \mu_{p,i} + \mathbf{B}_{p,i}(\mathbf{y}_{t-1,p}-\bar{\mathbf{y}}_{p}) +\varepsilon_{t,p,i}
$$

and the Level 2 model as

$$
\left[ 
\begin{array}{c}
\mu_{p,i} \\
B_{p,i}  
\end{array} 
\right]
\sim \mathcal{N}
\left[ 
\begin{array}{c}
\mathbf{0} \\
B_{*,i}  
\end{array} 
\right],
\left[ 
\begin{array}{cc}
\omega_{\mu_{i}} & \boldsymbol{\omega}^{(\beta_{i}\mu_{i})'}\\
\boldsymbol{\omega}^{(\beta_{i}\mu_{i})}  & \boldsymbol{\Omega}^{(\mathbf{B}_{i})}
\end{array} 
\right]
$$

where $\mathbf{B}_{*}$ and $\mathbf{K}_{*}^{(\Theta)}$ represent the temporal and contemporaneous network for a person selected at random, or the average parameters in the population: the fixed effects.  


## Applied Example

### Loading Data

For the multi-level VAR example we will use the AMIB data available on the QuantDev server. Here, subjects provided self-report data eight times per day, for 21 days, on average. Variable included in the dataset are listed below.

- `id`: person index
- `day`: time indices (1-21) 
- `interaction`: (1-168) 
- `igaff`: interpersonal affection
- `igdom`: interpersonal dominance
- `agval`: affect valence
- `agarous`: affect arousal
- `stress`: stress
- `health`: self-reported health[reverse coded]

Let's take a look at the *health*,*stress*, *affect valence* and *interpersonal affection* variables.


```{r}
filepath <- "https://quantdev.ssri.psu.edu/sites/qdev/files/AMIBshare_phase2_interaction_2019_0501.csv"
#read in the .csv file using the url() function
AMIB_interactionP2 <- read.csv(file=url(filepath),header=TRUE)

#subsetting to vaiables of interest
AMIB_interactionP2 <- AMIB_interactionP2[ ,c("id","day","health","interaction",
                                             "igaff","stress","agval")]
describe(AMIB_interactionP2)
```


```{r}
#plotting intraindividual change 
ggplot(data = AMIB_interactionP2,
       aes(x = interaction, group= id)) +
  #first variable
  geom_line(aes(y=igaff), color=1) +
  geom_line(aes(y=agval), color=2) +
  geom_line(aes(y=health), color=3) +
  geom_line(aes(y=stress), color=5) +
  #plot layouts
  scale_x_continuous(name="Interaction#") +
  scale_y_continuous(name="Raw Values") +  
  theme_classic() +
  theme(axis.title=element_text(size=14),
        axis.text=element_text(size=14),
        plot.title=element_text(size=14, hjust=.5)) +
  facet_wrap(~id, ncol=2) +
  ggtitle("AMIB Phase 2 Data (6 vars)")
```

Now, let's fit our model in the `mlVAR` package as follows:

```{r}
library("mlVAR")

mlvar_all <- mlVAR(
  data = AMIB_interactionP2, 
  vars = c("igaff",  "stress", "agval", "health"),
  idvar = "id",
  lags = 1, 
  dayvar = "day" 
)
```

Note, the `dayvar` argument is simply used to ensure the first measurement of a day is not regressed on the last measurement of the previous day. This argument should only be included in your model if there are multiple observations per day. 


Let's take a look at our results:

```{r}
summary(mlvar_all)
```


### Temporal Effects

```{r,results='asis', eval = FALSE}
Temporal effects:
   from     to lag  fixed    SE     P ran_SD
  igaff  agval   1 -0.015 0.019 0.432  0.041
  igaff health   1  0.029 0.011 0.009  0.008
 stress  igaff   1 -0.078 0.023 0.001  0.032
 stress health   1  0.056 0.016 0.001  0.050
  agval stress   1  0.027 0.016 0.103  0.036
  agval  agval   1  0.138 0.032 0.000  0.125
  agval health   1  0.005 0.014 0.705  0.025
```

For the prototypical person, only affect valence exhibited an autoregressive effect, meaning affect valence tended to persist over time. In addition, affect valence appears to have a small time-dependent deleterious effect on stress levels and health.  Likewise, stress appears to have a deleterious effect on health and interpersonal affection. On the other hand, interpersonal affection tended to have a buffering effect on affect valence and health. 


```{r}
# Plot temporal relations:
plot(
  mlvar_all, "temporal", 
  title = "Within-person temporal (lag-1) relations", 
  layout = "circle", 
  nonsig = "hide"
)
```

### Contemporaneous Effects

```{r,results='asis', eval = FALSE}
Contemporaneous effects (posthoc estimated):
     v1     v2 P 1->2 P 1<-2   pcor ran_SD_pcor    cor ran_SD_cor
 stress  igaff  0.022  0.127 -0.054       0.081 -0.251      0.146
  agval  igaff  0.000  0.000  0.413       0.154  0.471      0.154
  agval stress  0.000  0.000 -0.353       0.150 -0.441      0.169
 health  igaff  0.851  0.967  0.002       0.015 -0.101      0.122
 health stress  0.000  0.001  0.177       0.210  0.244      0.219
 health  agval  0.000  0.000 -0.098       0.068 -0.200      0.145
```

For the prototypical person, contemporaneous stress levels and affect valence were negatively correlated, while interpersonal affection and affect valence were positive correlated.



```{r}
plot(mlvar_all, 
     "contemporaneous", 
     title = "Within-person contemporaneous relations",
     layout = "circle", 
     nonsig = "hide"
)
```

### Between-Subjects Effects

```{r,results='asis', eval = FALSE}
Between-subject effects:
     v1     v2 P 1->2 P 1<-2   pcor    cor
 stress  igaff  0.637  0.215  0.140 -0.386
  agval  igaff  0.000  0.000  0.739  0.804
  agval stress  0.045  0.239 -0.251 -0.531
 health  igaff  0.446  0.454 -0.126 -0.485
 health stress  0.000  0.000  0.587  0.706
 health  agval  0.532  0.384 -0.123 -0.569
```

```{r}
plot(mlvar_all, 
     "between", 
     title = "Between-person relations", 
     layout = "circle", 
     nonsig = "hide"
)
```

In the between-subjects network, we see a strong relationship between interpersonal affection and affect valence, meaning people who, on average, felt more interpersonal affection felt, on average, more affect valence. In addition we see people who, on average, felt more stressed also felt, on average, less health. Furthermore, we see a small negative between-person association between stress and emotional valence, with those who on average felt more stressed, feeling less emotional valence.

## Reference