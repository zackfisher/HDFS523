# Linear Regression {#chapter-7}

```{r, echo = F}
button <-  "position: relative; 
            top: -25px; 
            left: 85%;   
            color: white;
            font-weight: bold;
            background: #4B9CD3;
            border: 1px #3079ED solid;
            box-shadow: inset 0 1px 0 #80B0FB"
```

```{r, echo=FALSE, results='asis'}
codefolder::bookdown(init = "show", style = button)
```

In Chapter 7 we will demonstrate how to estimate the linear regression model in R with an eye towards the longitudinal modeling to follow.

## Example Data

Chapter 6 make use of the same WISC data used in Chapter 3. Here we again read in, subset, and provide descriptives for the WISC data. We will also add a simulated variable `childgrad` indicating whether the student graduated highschool.

```{r}
filepath <- "https://quantdev.ssri.psu.edu/sites/qdev/files/wisc3raw.csv"

wisc3raw <- read.csv(file=url(filepath),header=TRUE)

colnames(wisc3raw) <- c(
  "id", "verb1", "verb2", "verb4", "verb6", "perfo1", "perfo2", 
  "perfo4", "perfo6", "info1", "comp1", "simu1", "voca1", "info6", 
  "comp6", "simu6", "voca6", "daded", "grad", "constant"
)

var_names_sub <- c(
  "id", "verb1", "verb2", "verb4", "verb6",
  "perfo1", "perfo2", "perfo4", "perfo6",
  "daded", "grad"
)

wiscsub <- wisc3raw[,var_names_sub]

set.seed(1234)
wiscsub$childgrad <- sample(c(0,1), replace=TRUE, size=nrow(wiscsub))

psych::describe(wiscsub)
```

## Intercept-Only Model

For our first example, we focus on verbal ability at Grade 2 as an outcome (`verb2` in the data frame `wiscsub`). Examining the distribution for 'verb2'.

```{r}
library("ggplot2")

psych::describe(wiscsub$verb2)

ggplot(data=wiscsub, aes(x=verb2)) + 
  geom_histogram(binwidth=2.5, fill="white", color="black", boundary=0) +
  xlab("Verbal Ability Grade 2") + ylab("Count") +
  xlim(0,50) +
  theme_classic()
```

### Intercept-Only Equation

The simplest model is an intercept only model. In this case, we would fit the model

$$ verb_{2i} = b_0 + \epsilon_{i}$$
 
Written out explicitly with the "silent" 1 in it, we get 
$$ verb_{2i} = b_01_i + \epsilon_{i}$$
This is helpful for explicit translation into the R code, specifically the `formula` within the `lm()` function. 

### Intercept-Only Model in R

We fit the model using the following code. Note that the code has the '1' predictor variable stated explicitly.

```{r}
model1 <- lm(formula = verb2 ~ 1,
              data = wiscsub,
              na.action = na.exclude)
summary(model1)
```

Note that we used `na.exclude` instead of `na.omit` (default); practically speaking `na.omit` deletes missing data entries while `na.exclude` just excludes from the analysis.Therefore with `na.exclude`, in the residuals and fitted values, `NA` will show up where there were missing values.

The output indicates that $b_0$ = 25.4153, and its standard error = 0.4275. 

The intercept reflects the expected value of the outcome variable when all of the predictor variables (i.e. $\left\{ x_{1i}, ..., x_{qi}\right\}$) = 0. 

So, in the absence of any additional information other than the descriptive statistics of $verb_{2i}$, what is our best guess for a person's $verb_{2i}$ score? 

It is the mean of $verb_{2i}$. The regression above confirms this notion; regressing the outcome on a vector of 1s allows us to 'recover' the mean.

```{r,echo=FALSE,engine='tikz',fig.ext=if (knitr:::is_latex_output()) 'pdf' else 'png'}
\usetikzlibrary{shapes,decorations,arrows,calc,arrows.meta,fit,positioning}
\tikzset{
    -Latex,auto,node distance =1 cm and 1 cm,semithick,
    state/.style ={ellipse, draw, minimum width = 0.7 cm},
    point/.style = {circle, draw, inner sep=0.04cm,fill,node contents={}},
    bidirected/.style={Latex-Latex,dashed},
    el/.style = {inner sep=2pt, align=left, sloped}
}
  \begin{tikzpicture}
    \node (1) at (0,0) {Child Maltreatment};
    \node (2) at (5,0) {Externalizing};
    \node (3) at (2.5,-2) {Income};
    \node (4) at (0,-1) {Stress};
    \path (1) edge  (2);
    \path (3) edge (1);
    \path (3) edge (2);
    \path (4) edge (1);
    \path (3) edge (4);
  \end{tikzpicture}
```

### Intercept as Mean of Outcome

Notice we can confirm this finding using matrix algebra, as well.

$$
\mathbb{E}(verb_{2i}) = \mathbb{E}(b_01_i + \epsilon_{i}) 
$$

From the properties of expectation, we have $\mathbb{E}(X+Y)=\mathbb{E}(X) + \mathbb{E}(Y)$.

$$
\mathbb{E}(verb_{2i}) = \mathbb{E}(b_01_i) + \mathbb{E}(\epsilon_{i}) 
$$

Another property of expectation relates to taking the expectation of a constant, $\mathbb{E}(c)=c$, thus $\mathbb{E}(b_01_i)=b_0$, implying

$$
\mathbb{E}(verb_{2i}) = b_01_i + \mathbb{E}(\epsilon_{i}). 
$$
Remembering Assumption 1, $\mathbb{E}(\epsilon_{i})=0$, we have

$$
\mathbb{E}(verb_{2i}) = b_0
$$
We can confirm this by looking at Verbal Scores at Wave 2.

```{r}
mean(wiscsub$verb2)
```


### Intercept-Only Model $R^2$

Yes - we recovered the *mean*, but we did not attempt to explain any of the *variance*. Let's take a look at the variance explained for Model 1. 

```{r}
summary(model1)$r.squared
```


It thus makes sense that we get 0 as the R-square. From the properties of variance, we know that $\mathbb{V}(c)=0$. There is no variability due to the regression model because there are no predictors, only a constant.

## Simple Linear Regression

Let's build up the model further. For example, we could attempt to explain some of the *between-person variance* in the Grade 2 verbal score from the Grade 1 verbal scores. But, before we do, let's examine the distribution of the *between-person differences* in the Grade 1 verbal scores.

```{r}
ggplot(wiscsub, aes(x=verb1)) + 
  geom_histogram(binwidth=2.5, fill="white", color="black", boundary=0) +
  xlab("Verbal Ability Grade 1") + 
  ylab("Count") +
  xlim(0,50) +
  theme_classic()
```
And the relation between the Grade 2 and Grade 1 verbal ability scores.
```{r}
ggplot(wiscsub, aes(x=verb1, y = verb2)) + 
  geom_point() +
  stat_ellipse(color="blue", alpha=.7) +
  xlab("Verbal Ability Grade 1") + 
  ylab("Verbal Ability Grade 2") +
  ylim(0,45) + 
  xlim(0,45) +
  theme_classic()
```

### Regression Equation and Model Fitting

Our regression model becomes 
$$ verb_{2i} = b_01_i + b_1verb_{1i} + \epsilon_{i}$$
```{r}
model2 <- lm(verb2 ~ 1 + verb1,
              data = wiscsub,
              na.action = na.exclude)
summary(model2)
```


### Path Diagram

We might also be interested in a graphical depiction of our model. This can be accomplished with the `semPaths` package.

```{r}
semPlot::semPaths(model2, what = "paths")
```

### Interpreting Model Parameters

How do we interpret the parameters here? 

*The intercept, $b_0$, is the expected value for the outcome variable when all of the predictor variables equal zero.* So, we would expect a child to have a Grade 2 verbal score of 10.62965 *if* they have a Grade 1 verbal score of 0.

*The slope, $b_1$ is the expected difference in the outcome variable for each 1-unit difference in the predictor variable*. So, *across children*, for each 1-point difference in a child's Grade 1 verbal score, we would expect a 0.75 point difference in the Grade 2 verbal score. 

### Plotting Regression Line

We can plot the relation between 'verb1' and 'verb2', and include the predicted line from the analysis.
```{r}
ggplot(data=wiscsub, aes(x=verb1,y=verb2)) +
  geom_point(size = 2, shape=19) +
  geom_smooth(method=lm,se=TRUE,fullrange=TRUE,colour="red", size=2) +
  labs(x= "Verbal Ability Grade 1", y= "Verbal Ability Grade 2") +
  xlim(0,50) +
  ylim(0,50) +
  theme_bw() +
  theme(
    plot.background = element_blank(),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_blank()
  ) +
  #draws x and y axis line
  theme(axis.line = element_line(color = 'black')) +
  #set size of axis labels and titles
  theme(axis.text = element_text(size=12),
        axis.title = element_text(size=14))
```


## Mean Centering Predictors

In this case, and in many other cases, the intercept does not have a 'useful' interpretation for the empirical example. This is because no students had a Grade 1 verbal score equal to 0. 

Therefore, if we want to make the intercept more meaningful, we need to make a Grade 1 verbal score with a more meaningful 0 point. Typically we center the *predictor* variables in regression analysis. 

For example, we create a centered variable, $x^{*}_{1i}$ by subtracting the sample mean, $\bar{x_1}$ from each observation,  
$$ x^{*}_{1i} = x_{1i} - \bar{x_1} $$
Our model becomes  
$$ y_i = b_0(1_i) + b_1(x^{*}_{1i}) + \epsilon_i $$
We can sample-mean center $verb_{1i}$ in R as follows
```{r}
#calculate the mean centered variable
wiscsub$verb1_star <- wiscsub$verb1 - mean(wiscsub$verb1, na.rm = TRUE)
```

Then we can fit a new model using $verb^{*}_{1i}$, such that 

$$ verb_{2i} = b_0(1_i) + b_1(verb^{*}_{1i}) + \epsilon_i $$

```{r}
model3 <- lm(verb2 ~ 1 + verb1_star,
              data = wiscsub,
              na.action = na.exclude)
summary(model3)
```

**Note**: Mean centering should be used to aid interpretation. Historically, it has been suggested that mean centering will reduce multicollinearity, however this is not the case. See ![Olvera & Kroc (2018)](https://journals.sagepub.com/doi/full/10.1177/0013164418817801) for more information.



### Interpreting Model Parameters

Note that the estimate for the slope $b_1$ stays the same, but the estimate for the intercept is different. This is because the variable 'verb1_star' equals 0 when a child has an average 1st grade verbal score. Therefore the expected value for the 2nd grade verbal score, for *a child with an average 1st grade verbal score*, is 25.41534. 

### Plotting Regression Line

```{r}
ggplot(data=wiscsub, aes(x=verb1_star,y=verb2)) +
  geom_point(size = 2, shape=19) +
  geom_smooth(method=lm,se=TRUE,fullrange=TRUE,colour="red", size=2) +
  labs(x= "Sample-Centered Verbal Ability Grade 1", y= "Verbal Ability Grade 2") +
  xlim(-20,20) +
  ylim(0,50) +
  #theme with white background
  theme_bw() +
  #eliminate background, gridlines, and chart border
  theme(
    plot.background = element_blank()
    ,panel.grid.major = element_blank()
    ,panel.grid.minor = element_blank()
    ,panel.border = element_blank()
  ) +
  #draws x and y axis line
  theme(axis.line = element_line(color = 'black')) +
  #set size of axis labels and titles
  theme(axis.text = element_text(size=12),
        axis.title = element_text(size=14))
```

Note the change of scale on the x-axis.

##  Multiple Linear Regression

Now, let's include a second predictor. We have information on the number of years of education for the children's fathers, variable `daded`. The values in `daded` indicate the number of years of education each father completed. First, let's take a look at the distribution of this new predictor variable.

```{r}
psych::describe(wiscsub$daded)

ggplot(data=wiscsub, aes(x=daded)) + 
  geom_histogram(binwidth=2.5, fill="white", color="black") +
  xlim(0,20) +
  xlab("Number of Years of Father's Education") + 
  ylab("Count") +
  theme_classic()
```
And the relation between Grade 2 verbal scores and daded.
```{r}
ggplot(data=wiscsub, aes(x=daded, y = verb2)) + 
  geom_point() +
  xlim(0,20) + 
  ylim(0,50) +
  xlab("Father's Education (Years)") + ylab("Verbal Ability Grade 2") +
  theme_classic()
```

### Regression Equation

Our model now becomes 

$$ verb_{2i} = b_01_{i} + b_1verb^{*}_{1i} + b_2daded^{*}_{i} + \epsilon_{i}$$

where $verb^{*}_{1i}$ is the sample-centered version of $verb_{1i}$, and $daded^{*}_{i}$ is the sample-centered version of $daded_{i}$. 

The slope, $b_2$ is the expected difference in grade 2 verbal score for each 1 year difference in father's education. 

We can also center the `daded` variable.

```{r}
# Calculate mean-centered version of father's education variable
wiscsub$daded_star <- wiscsub$daded - mean(wiscsub$daded)
```

### Fit Model in R

```{r}
model4 <- lm(verb2 ~ 1 + verb1_star + daded_star,
              data = wiscsub,
              na.action = na.exclude)
summary(model4)
```
Now we have an intercept and two slopes. 


### Path Diagram


```{r}
semPlot::semPaths(model4, what = "paths")
```


#### Interpreting Model Parameters

$b_0$ is the expected value of the outcome variable when all other variables are 0. Therefore, in this case, $b_0$ is the expected Grade 2 verbal score for a child with an average Grade 1 verbal score (i.e. $verb^{*}_{1i}$ = 0) *and* whose father had an average education (i.e. $daded^{*}_{i}$ = 0, $\bar{daded_{i}}$ = 10.81 years of education. 

$b_1$ is the expected difference in the outcome for a 1-unit difference in $x_{1i}$. In this example (i.e. 'model4'), $b_1$ is the expected difference in Grade 2 verbal score (outcome variable, $y_i$ = $verb_{2i}$) for a 1 point difference in the Grade 1 verbal score ($x_{1i}$ = $verb^{*}_{1i}$), holding constant the level of father's education.

$b_2$ is the expected difference in the outcome for a 1-unit difference in $x_{2i}$. For this example (i.e. 'model4'), $b_2$ is the expected difference in Grade 2 verbal score (outcome variable, $y_i$ = $verb_{2i}$) for each year difference in father's education ($x_{2i}$ = $daded^{*}_i$), holding constant in Grade 1 verbal score.

### A Note on Interpretation

The distinguishing feature for interpretation in linear models without interactions or higher-order terms is that **the effect of a given change in an independent variable is the same regardless of the value of that variable (at the start of its change) and regardless of the level of the other variables in the model.**

Interpretation only needs to specify which variable is changing, by how much, and that other variables are being held constant.

In regard to our last example, years of father's education does impact the relationship between Grade 1 and 2 verbal scores. Likewise, the effect of father's education on Grade 1 verbal scores does not depend on Grade 1 verbal scores. Said in a different way, no matter what a child's Grade 1 verbal score was, the effect of father's education on Grade 2 verbal scores is the same.

Consider another model with categorical and continuous predictors, `grad` and `verb1_star`, respectively. 


```{r}
library("ggiraphExtra")
model4b <- lm(verb2 ~ 1 + verb1_star + grad,
              data = wiscsub,
              na.action = na.exclude)
ggPredict(model4b,se=TRUE,interactive=FALSE)
```

Notice in the plot, whether a student's father graduated HS does not impact the relationship between Grade 1 and 2 verbal scores. Differences in Grade 1 and 2 verbal scores are not dependent on whether or not the father graduated HS. 

**Most importantly, using the coefficients themselves we can easily interpret the model parameters.** 

## Categorical Variable Interaction

Ok, let's move on to the topic of an *interaction* which uses the product of two predictor variables as a new predictor.

Working up a slightly different example with the 'grad' variable (whether dad graduated high school), 

$$ verb_{2i} = b_01_i + b_1verb^{*}_{1i} + b_2grad_{i} + b_3(verb^{*}_{1i}grad_{i}) + \epsilon_{i}$$

Where $verb^{*}_{1i}$ is the mean-centered version of $verb_{1i}$, and $grad_i$ is a dummy coded variable that equals 0 if the child's father *did not* graduate high school, and equals 1 if the child's father *did* graduate high school.

We did not sample-mean center $grad_i$ in this example because a value of 0 already has substantive meaning for the current example (i.e. when $grad_i$ equals 0, the father *did not* graduate high school).

### Interaction as Moderation

Often, we describe phenomena in terms of *moderation*; or that the relation between two variables (i.e. $y_i$ and $x_{1i}$) is *moderated* by a third variable (i.e. $x_{2i}$). For example, the relation between Grade 1 and Grade 2 verbal scores may be *moderated* by father's graduation status. More specifically, the relation between 1st and 2nd grade verbal score may be different for children whose fathers' did not or did graduate from high school.

The inclusion of product terms (i.e. interactions) allows for a direct investigation of a *moderation* hypothesis.

#### Choosing a Moderator

When we use a product term, we should define one of the variables as the moderator and one of the variables as the predictor of interest. Let's call $verb^{*}_{1i}$ the predictor of interest, and $grad_{i}$ the moderator. 

### Moderation by Categorical Variable

When the moderator is a dummy variable then the form of the moderation becomes fairly simple; we will have one equation for $grad_{i} = 0$, and a second equation for $grad_i = 1$. 

#### Rewriting Equation

To illustrate the notion of two equations, let's rewrite the regression equation  

$$ verb_{2i} = b_01_i + b_1verb^{*}_{1i} + b_2grad_{i} + b_3(verb^{*}_{1i}grad_{i}) + \epsilon_{i}$$

as two separate regression equations, one for fathers who graduated from highschool and one for fathers that did not. We can accomplish this by plugging in $0$ and $1$ into the regression equation and rearranging some of the terms. Doing so we get 

**Equation for Students whose father Graduated Highschool**

$$ verb_{2i} = (b_0 + b_2) + (b_1 +  b_3)verb^{*}_{1i} + \epsilon_{i}$$

**Equation for Students whose father Did Not Graduate from Highschool**

$$ verb_{2i} = b_0 + b_1verb^{*}_{1i} + \epsilon_{i}$$

### Interpretation

Without an interaction, our linear regression model assumes that the only difference between the regression line for each group (graduate HS vs not) is the intercept. That is, it assumes that the relationship between verbal scores at Grades 1 and 2 is the same for both groups. 

*Children Whose father's Did Not Graduate HS*

The expected Grade 2 verbal score for a child whose father did not graduate high school *and* who had an average Grade 1 verbal score is $b_0$. Also, for a child whose father did not graduate high school, $b_1$ is the expected difference in their Grade 2 verbal score for a one-point difference in their Grade 1 verbal score.

*Children Whose father's Did Not Graduate HS*

The parameter estimates $b_0$ and $b_1$ maintain their interpretation from before. But now each of them is *moderated* (i.e. shifted or altered) by $b_2$ or $b_3$. 


Specifically, the expected Grade 2 verbal score for a child whose father did graduate high school *and* who earned an average Grade 1 verbal score is $b_0 + b_2$. 

And, for a child whose father did graduate high school, $b_1 + b_3$ is the expected difference in their Grade 2 verbal score for a one-point change in their Grade 1 verbal score.

### Fit Regression Model in R

OK - let's fit the model! Note that within this model we use the code `I(verb1_star * grad)`. This produces the interaction term within the model. The wrapper function `I()` indicates to R to perform this data computation as-is, otherwise we would need to perform this computation (i.e. the multiplication of `verb1_star` by `grad`) outside of the function `lm()`. 

```{r}
model5 <- lm(verb2 ~ 1 + verb1_star + grad + I(verb1_star*grad),
              data = wiscsub,
              na.action = na.exclude)
summary(model5)
```


### Path Diagram

```{r}
semPlot::semPaths(model5, what = "paths")
```

The parameter estimates from this model indicate that, for children whose father did not graduate high school, the expected Grade 2 verbal score for a child that earned an average 1st grade verbal score equals 25.2663 ($b_0$). 

Also, for children whose father did not graduate high school, a 1-point difference in their Grade 1 verbal score is expected to correspond with a 0.7861 ($b_1$) point difference in the Grade 2 verbal score.

Moreover, the parameter estimates indicate that, for children whose father did graduate high school, the expected Grade 2 verbal score for a child that earned an average Grade 1 verbal score is 25.2663 + 1.4632 = 26.7295 ($b_0 + b_2$). 

Also, for children whose father graduated high school, a 1-point difference in their Grade 1 verbal score is expected to correspond with a ($b_1 + b_3$) = 0.7861 - 0.2430 = 0.5431 point difference in the Grade 2 verbal score.

Even though the interaction is not significant, we can plot it for illustrating the moderation effect: 

```{r}
#plot of moderation
ggplot(data=wiscsub, 
       aes(y=verb2,x=verb1_star, color = factor(grad))) +
  geom_jitter() +
  stat_smooth(method='lm', se=TRUE, fullrange=TRUE) +
  xlab("1st Grade Verbal Score") + 
  ylab("2nd Grade Verbal Score") + 
  guides(color=guide_legend(title="HS Grad")) +
  theme_bw() 
```


The example from 'model5' contained an interaction using a dummy variable (i.e., $grad_i$). Interactions may also occur between two continuous variables (i.e., $verb^{*}_{1i}$ and $daded^{*}_{i}$). We will not cover here, but note that it is still very useful to consider and communicate those interactions as *moderation*. There are many resources on interactions of two (or more) continuous variables.

\textbf{References}
