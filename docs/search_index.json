[["11-chapter-11.html", "Chapter 11 Two-Occassion Change", " Chapter 11 Two-Occassion Change In this section we will introduce longitudinal models in their most basic form: repeated measures collected at two occasions. Two-occasions data are a natural starting point for studying change: all longitudinal data collection begins with two occasions. Given any two-occasion repeated measures data, popular analytic approaches include the ANCOVA (autoregressive) and difference score models. This script works through some basic representations of change, more specifically (1) autoregressive models of change, and (2) difference-score models of change. These two models consider and answer different kinds of research questions: questions about change in interindividual differences and questions about intraindividual change. "],["11.1-introduction.html", "11.1 Introduction", " 11.1 Introduction 11.1.1 A Thought Experiment Now that we are considering repeated measures data it is helpful to thinking about what repeated measures buy us. Here are a few thoughts: With cross-sectional data: No sense of passage of time Cannot control for what happened at earlier timepoints Causality (e.g. cause precedes the effect, cause related to effect, plausible alternatives) Depending on true change process, cross-sectional slice may mislead Must rely more heavily on theory (e.g. equivalent models (MacCallum et al. 1993)) Equivalent Models and Time Dependence In addition to allowing us to model processes unfolding across time, repeated measures also allow us to address an important problem in applied modeling: the existence of equivalent models. Equivalent Models: An Example Excerpt from MacCallum et al. (1993) Examples of Equivalent Models Figure 3 from MacCallum et al. (1993) Now image these variables were collected at four different waves. This would eliminate the possibility of equivalent models under this scenario. References "],["11.2-example-data-i.html", "11.2 Example Data I", " 11.2 Example Data I For our first set of examples we will use the WISC data. Here we again read in, subset, and provide basic descriptives. filepath &lt;- &quot;https://quantdev.ssri.psu.edu/sites/qdev/files/wisc3raw.csv&quot; wisc3raw &lt;- read.csv(file=url(filepath),header=TRUE) var_names_sub &lt;- c( &quot;id&quot;, &quot;verb1&quot;, &quot;verb2&quot;, &quot;verb4&quot;, &quot;verb6&quot;, &quot;perfo1&quot;, &quot;perfo2&quot;, &quot;perfo4&quot;, &quot;perfo6&quot;, &quot;momed&quot;, &quot;grad&quot; ) wiscsub &lt;- wisc3raw[,var_names_sub] psych::describe(wiscsub) ## vars n mean sd median trimmed mad min max range skew ## id 1 204 102.50 59.03 102.50 102.50 75.61 1.00 204.00 203.00 0.00 ## verb1 2 204 19.59 5.81 19.34 19.50 5.41 3.33 35.15 31.82 0.13 ## verb2 3 204 25.42 6.11 25.98 25.40 6.57 5.95 39.85 33.90 -0.06 ## verb4 4 204 32.61 7.32 32.82 32.42 7.18 12.60 52.84 40.24 0.23 ## verb6 5 204 43.75 10.67 42.55 43.46 11.30 17.35 72.59 55.24 0.24 ## perfo1 6 204 17.98 8.35 17.66 17.69 8.30 0.00 46.58 46.58 0.35 ## perfo2 7 204 27.69 9.99 26.57 27.34 10.51 7.83 59.58 51.75 0.39 ## perfo4 8 204 39.36 10.27 39.09 39.28 10.04 7.81 75.61 67.80 0.15 ## perfo6 9 204 50.93 12.48 51.76 51.07 13.27 10.26 89.01 78.75 -0.06 ## momed 10 204 10.81 2.70 11.50 11.00 2.97 5.50 18.00 12.50 -0.36 ## grad 11 204 0.23 0.42 0.00 0.16 0.00 0.00 1.00 1.00 1.30 ## kurtosis se ## id -1.22 4.13 ## verb1 -0.05 0.41 ## verb2 -0.34 0.43 ## verb4 -0.08 0.51 ## verb6 -0.36 0.75 ## perfo1 -0.11 0.58 ## perfo2 -0.21 0.70 ## perfo4 0.59 0.72 ## perfo6 0.18 0.87 ## momed 0.01 0.19 ## grad -0.30 0.03 And some bivariate plots of the two-occasion relations. psych::pairs.panels(wiscsub[,c(&quot;verb1&quot;,&quot;verb6&quot;)]) "],["11.3-two-occassions-of-change.html", "11.3 Two-Occassions of Change", " 11.3 Two-Occassions of Change Although we may prefer to have more timepoints, two waves of data is often what is available or affordable, often directly related to a change of interest, e.g., pretest-posttest an improvement over cross-sectional data for many questions of interest Notions of Stability Stability in the Absolute or Exact Sense A variable may be considered stable to the extent that mean values for \\(y_{1}\\) and \\(y_{2}\\) are equal over time (\\(\\bar{y}_{2}-\\bar{y}_{1}=0\\), \\(y_{2i}-y_{1i}=0\\)) individual values of \\(y_{1i}\\) and \\(y_{2i}\\) are equal over time (\\(y_{2i}-y_{1i}=0\\)) Stability in the Relative Sense A variable may be considered stable to the extent that \\(y_{1i}\\) and \\(y_{2i}\\) are correlated (\\(r_{12}=1\\)) Note that mean or individual values may increase or decrease over time, but correlation will be largely unchanged unless relative position changes. Notions of Change Raw Change In thinking about difference scores (change scores, gain scores) we are often interested in the absolute change in the value of \\(y_{t}\\). Difference scores captures increase or decrease in the mean values or individual values. Correlation between \\(y_{1}\\) and \\(y_{2}\\) can be anywhere between \\(0\\) or \\(1\\) and difference scores may be small or large. For example. if \\(r_{12} = 1.0\\) and we add \\(5\\) points to \\(y_2\\), \\(r_{12}\\) will still be equl to \\(1.0\\) but the mean difference will have changed. y1 &lt;- c(1,1,1,2,2,2) y2 &lt;- c(3,3,3,4,4,4) # cor(y1,y2) = 1 # mean(y2-y1) = 2 y2 &lt;- y2 + 5 # cor(y1,y2) = 1 # mean(y2-y1) = 7 Relative Change When we think about relative change, a lower correlation indicates greater change, but change is relative to other cases in the data and not absolute. For example, if \\(r_{12}=0.45\\), and we add \\(5\\) points to all scores, \\(r_{12}\\) will still be \\(0.45\\). Even though the scores themselves will all change, the relative degree of change is unaffected. Compare Absolute and Relative Change Image from Newsom (2011) "],["11.4-autoregressive-model.html", "11.4 Autoregressive Model", " 11.4 Autoregressive Model Before introducing the model behind each approach is is helpful to consider the data characteristics underlying each notion of change. Residualized Change The first approach uses the measure at the second time point as a dependent variable regressed on the the measure at the first time point. The term residualized change comes from the fact that an outcome, such as verbal scores at grade 6, is regressed on itself at a prior occasion, verbal scores at grade 1. Any variability in the outcome that is explained by the lagged regressor will be set aside and is threfore not explainable by a key predictor. Thus, the autoregressive effect residualizes the outcome leaving only variability that is unexplained by the lagged variable, which can be construed as the variability due to change. In this way it is helpful to view the data as a scatter plot, like we would in a regression model. library(&quot;ggplot2&quot;) ggplot(data = wiscsub, aes(x = verb1, y = verb6)) + geom_point() + xlab(&quot;Verbal Scores at Grade 1&quot;) + ylab(&quot;Verbal Scores at Grade 6&quot;) + theme_bw() Autoregressive Model When researchers refer to the autoregressive (residualized change) model for two occasion data they are referring to the following multiple regression model: \\[ y_{2i} = \\beta_0 + \\beta_1y_{1i} + e_{i} \\] where \\(y_{1i}\\) is the value of the outcome variable for individual \\(i\\) at time \\(1\\) \\(y_{2i}\\) is the value of the outcome variable for individual \\(i\\) at time \\(2\\) \\(\\beta_0\\) is an intercept parameter, the expected value of \\(y_{2i}\\) when \\(y_{1i}=0\\) \\(\\beta_1\\) is a regression parameter indicating the difference in the predicted score of \\(y_{2i}\\) based on a 1-unit difference in \\(y_{1i}\\) \\(e_{i}\\) is the model error for individual \\(i\\) Note, the term residualized change comes from the fact that the autoregressive effect residualizes the outcome. This leaves only the variability that is unexplained by the previous timepoint, or the variability due to change. Autoregressive Residuals With the autoregressive model it is helpful to think more about the residual term. Let’s ignore the scaling constant for now, If we subtract \\(\\beta_1y_{1i}\\) from both sides of the AR equation we isolate the residuals: \\[ e_{i} = y_{2i} -\\beta_1y_{1i} \\] Here, the residualized change is the function of a weighted combination of our time 1 scores. Instead of talking about raw change we are instead asking “Where would we predict you to be at time 2 given your standing relative to the mean at time 1?”** Consider the following scenarios: \\(e_{i}\\) is positive: you changed more in a positive direction than would have been expected. \\(e_{i}\\) is negative: you changed more in a negative direction than would have been expected. Autoregressive Model in R As we said previously, the autoregrssive model is useful for examining questions about change in interindividual differences. The model for verbal scores at grade 6 can be written as \\[ verb6_{i} = \\beta_{0} + \\beta_{1}verb1_{i} + e_{i}\\] We note that this is a model of relations among between-person differences. This model is similar to, but is not a single-subject time-series model (which are also called autoregressive models, but are fit to a different kind of data). Translating the between-person autoregressive model into code and fitting it to the two-occasion WISC data we have ARfit &lt;- lm( formula = verb6 ~ 1 + verb1, data=wiscsub, na.action=na.exclude ) summary(ARfit) ## ## Call: ## lm(formula = verb6 ~ 1 + verb1, data = wiscsub, na.action = na.exclude) ## ## Residuals: ## Min 1Q Median 3Q Max ## -20.2459 -5.8651 0.1781 4.9048 27.9976 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 20.22485 1.99608 10.13 &lt;2e-16 *** ## verb1 1.20117 0.09773 12.29 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.087 on 202 degrees of freedom ## Multiple R-squared: 0.4279, Adjusted R-squared: 0.425 ## F-statistic: 151.1 on 1 and 202 DF, p-value: &lt; 2.2e-16 The intercept term, \\(\\beta_{0}\\) = 20.22 is the expected value of Verbal Ability at the 2nd occasion, for an individual with a Verbal Ability score = 0 at the 1st occasion. The slope term, \\(\\beta_{1}\\) = 1.20 indicates that for every 1-point difference in Verbal Ability at the 1st occasion, we expect a 1.2 point difference at the 2nd occasion. We can plot the autoregressive model prediction with confidence intervals (CI). The function termplot takes the fitted lm object. The CI bounds are plotted with the se option and residuals with partial.resid option. termplot(ARfit,se=TRUE,partial.resid=TRUE, main=&quot;Autoregressive Model&quot;, xlab=&quot;Verbal Score at Grade 1&quot;, ylab=&quot;Verbal Score at Grade 6&quot;) Note that this code makes use of the lm() model object. We can also do something similar with the raw data using ggplot. ggplot(data = wiscsub, aes(x = verb1, y = verb6)) + geom_point() + geom_smooth(method=&quot;lm&quot;, formula= y ~ 1 + x, se=TRUE, fullrange=TRUE, color=&quot;red&quot;, size=2) + xlab(&quot;Verbal Score at Grade 1&quot;) + ylab(&quot;Verbal Score at Grade 6&quot;) + ggtitle(&quot;Autoregressive Model&quot;) + theme_classic() Note that this code embeds an lm() model within the ggplot function. "],["11.5-difference-score-model.html", "11.5 Difference Score Model", " 11.5 Difference Score Model Again, let’s first take a look at the data characteristics. Raw Change The second approach involves computing a change score by subtracting the measure at time 1 from the measure at time 2 (e.g. \\(y_{t}-y_{t-1}\\)) This raw change score is then typically used as the dependent variable in a regression equation. Here we are speaking in terms of difference scores and raw change. We can plot intraindividual change, by putting time along the x-axis. This requires reshaping the data from wide format to long format. To recap, our wide data looks like this: head(round(wiscsub,2)) ## id verb1 verb2 verb4 verb6 perfo1 perfo2 perfo4 perfo6 momed grad ## 1 1 24.42 26.98 39.61 55.64 19.84 22.97 43.90 44.19 9.5 0 ## 2 2 12.44 14.38 21.92 37.81 5.90 13.44 18.29 40.38 5.5 0 ## 3 3 32.43 33.51 34.30 50.18 27.64 45.02 46.99 77.72 14.0 1 ## 4 4 22.69 28.39 42.16 44.72 33.16 29.68 45.97 61.66 14.0 1 ## 5 5 28.23 37.81 41.06 70.95 27.64 44.42 65.48 64.22 11.5 0 ## 6 6 16.06 20.12 38.02 39.94 8.45 15.78 26.99 39.08 14.0 1 We can reshape our data to a long format using the reshape() function as follows wiscsublong &lt;- reshape( data = wiscsub[c(&quot;id&quot;,&quot;verb1&quot;,&quot;verb6&quot;)], varying = c(&quot;verb1&quot;,&quot;verb6&quot;), timevar = &quot;grade&quot;, idvar = &quot;id&quot;, direction = &quot;long&quot;, sep = &quot;&quot; ) wiscsublong &lt;- wiscsublong[order(wiscsublong$id,wiscsublong$grade),] head(round(wiscsublong,2)) ## id grade verb ## 1.1 1 1 24.42 ## 1.6 1 6 55.64 ## 2.1 2 1 12.44 ## 2.6 2 6 37.81 ## 3.1 3 1 32.43 ## 3.6 3 6 50.18 Now, the long data is structured in a manner amenable to plotting. Notice here that each line indicates how an individual’s Grade 6 score differs from their Grade 1 score: intraindividual change. library(&quot;ggplot2&quot;) ggplot(data = wiscsublong, aes(x = grade, y = verb, group = id)) + geom_point() + geom_line() + xlab(&quot;Grade&quot;) + ylab(&quot;WISC Verbal Score&quot;) + ylim(0,100) + scale_x_continuous(breaks=seq(1,6,by=1)) + theme_bw() Calculating Difference Scores Using the same repeated measures notation as above we can think about difference scores in the following way \\[ y_{2i} = y_{1i} + \\Delta_{i} \\] where \\(y_{1i}\\) is the value of the outcome variable for individual \\(i\\) at time \\(1\\) \\(y_{2i}\\) is the value of the outcome variable for individual \\(i\\) at time \\(2\\) \\(\\Delta_i\\) is the difference score for individual \\(i\\) We can calculate the difference score as \\[ \\Delta_{i} = y_{2i} - y_{1i}\\] where \\(\\Delta_{i}\\) is a score just like other scores (we can calculate its mean and covariance with other variables, etc.). Comparison to Residualized Change Remember when we talked about the autoregressive change model we showed the residual was equal to \\[ e_{i} = y_{2i} -\\beta_1y_{1i} \\] For the autoregressive model, change is the function of a weighted combination of the scores. In the difference score approach, we defined the difference scores as \\[ \\Delta_{i} = y_{2i} - y_{1i}\\] What we see from this relationship is that raw change is residualized change when \\(\\beta_1 = 1\\). We can see these concepts are intimately linked. Difference Scores in WISC Data For our empirical example we can write the difference score, or raw change in verbal ability, between Grades 1 and 6, as \\[ verbD_{i} = verb2_{i} - verb1_{i} \\] Furthermore, we can calculate the difference score in R as follows #calculating difference score wiscsub$verbD &lt;- wiscsub$verb6-wiscsub$verb1 head(round(wiscsub,2)) ## id verb1 verb2 verb4 verb6 perfo1 perfo2 perfo4 perfo6 momed grad verbD ## 1 1 24.42 26.98 39.61 55.64 19.84 22.97 43.90 44.19 9.5 0 31.22 ## 2 2 12.44 14.38 21.92 37.81 5.90 13.44 18.29 40.38 5.5 0 25.37 ## 3 3 32.43 33.51 34.30 50.18 27.64 45.02 46.99 77.72 14.0 1 17.75 ## 4 4 22.69 28.39 42.16 44.72 33.16 29.68 45.97 61.66 14.0 1 22.03 ## 5 5 28.23 37.81 41.06 70.95 27.64 44.42 65.48 64.22 11.5 0 42.72 ## 6 6 16.06 20.12 38.02 39.94 8.45 15.78 26.99 39.08 14.0 1 23.88 Difference Score Descriptives Look at the descriptives with the difference score. psych::describe(wiscsub[,c(&quot;verb1&quot;,&quot;verb6&quot;,&quot;verbD&quot;)]) ## vars n mean sd median trimmed mad min max range skew kurtosis ## verb1 1 204 19.59 5.81 19.34 19.50 5.41 3.33 35.15 31.82 0.13 -0.05 ## verb6 2 204 43.75 10.67 42.55 43.46 11.30 17.35 72.59 55.24 0.24 -0.36 ## verbD 3 204 24.16 8.15 23.91 23.85 8.09 4.62 50.88 46.26 0.38 0.14 ## se ## verb1 0.41 ## verb6 0.75 ## verbD 0.57 psych::corr.test(wiscsub[,c(&quot;verb1&quot;,&quot;verb6&quot;,&quot;verbD&quot;)]) ## Call:psych::corr.test(x = wiscsub[, c(&quot;verb1&quot;, &quot;verb6&quot;, &quot;verbD&quot;)]) ## Correlation matrix ## verb1 verb6 verbD ## verb1 1.00 0.65 0.14 ## verb6 0.65 1.00 0.84 ## verbD 0.14 0.84 1.00 ## Sample Size ## [1] 204 ## Probability values (Entries above the diagonal are adjusted for multiple tests.) ## verb1 verb6 verbD ## verb1 0.00 0 0.04 ## verb6 0.00 0 0.00 ## verbD 0.04 0 0.00 ## ## To see confidence intervals of the correlations, print with the short=FALSE option Of particular interest in questions about intraindividual change is the relation between the pre-test score and the amount of intraindividual change. We can look at the bivariate association. psych::pairs.panels(wiscsub[,c(&quot;verb1&quot;,&quot;verbD&quot;)]) A note on computing difference scores: always use raw scores when computing difference scores, Pre-standardizing variables discards important variance information. A Difference Score Regression Model For the purpose of comparison consider a linear model is expressed for \\(i = 1\\) to \\(N\\) as \\[ \\Delta y_{i} = \\beta_{0} + \\beta_{1}y_{1i} + e_{i} \\] where we are looking at change in verbal test scores while controlling for grade 1 scores, \\(\\beta_0\\) is an intercept parameter, the predicted score of \\(\\Delta y\\) when \\(y_{1i}=0\\) \\(\\beta_1\\) is a slope parameter indicating the difference in the predicted score of \\(\\Delta y\\) based on a 1-unit difference in \\(y_{1i}\\) \\(e_{i}\\) is the residual score for individual \\(i\\) #Difference score model DIFfit &lt;- lm(formula = verbD ~ 1 + verb1, data=wiscsub, na.action=na.exclude) summary(DIFfit) ## ## Call: ## lm(formula = verbD ~ 1 + verb1, data = wiscsub, na.action = na.exclude) ## ## Residuals: ## Min 1Q Median 3Q Max ## -20.2459 -5.8651 0.1781 4.9048 27.9976 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 20.22485 1.99608 10.132 &lt;2e-16 *** ## verb1 0.20117 0.09773 2.058 0.0408 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.087 on 202 degrees of freedom ## Multiple R-squared: 0.02054, Adjusted R-squared: 0.0157 ## F-statistic: 4.237 on 1 and 202 DF, p-value: 0.04083 The intercept term, \\(\\beta_{0} = 20.22\\) is the expected value of the difference score (raw change in verbal ability), for an individual with a verbal ability score = 0 at the first occasion. The slope term, \\(\\beta_{1} = 0.20\\) indicates that for every 1-point difference in verbal ability at the first occasion, we expect a \\(0.2\\) point difference in the amount of intraindividual change. The same methods as above can be used to plot the results of the difference score model. termplot(DIFfit,se=TRUE,partial.resid=TRUE, main=&quot;Difference-score Model&quot;, xlab=&quot;Verbal Score at Time 1&quot;, ylab=&quot;Difference in G1 and G6 Verbal Scores&quot;) We can also do something similar using ggplot. #making interindividual regression plot ggplot(data = wiscsub, aes(x = verb1, y = verbD)) + geom_point() + geom_smooth(method=&quot;lm&quot;, formula= y ~ 1 + x, se=TRUE, fullrange=TRUE, color=&quot;red&quot;, size=2) + xlab(&quot;Verbal Score at Grade 1&quot;) + ylab(&quot;Difference Score&quot;) + ggtitle(&quot;Difference Score Model&quot;) + theme_classic() Note that each of these model results plots are regression plots: outcome on the y-axis, predictor on the x-axis. "],["11.6-critiques-and-comparisons.html", "11.6 Critiques and Comparisons", " 11.6 Critiques and Comparisons Critiques of the Autoregressive Model Some interpretational oddities arise from the autoregressive model that are worth considering. Consider the following situation: We are studying a weight loss intervention where we measure weight prior to and after an intervention. The mean weight at time 1 is 250 pounds (\\(\\mu_{t1}=150\\)) and the mean weight at time 2 is 230 pounds (\\(\\mu_{t1}=130\\)). Now consider two people: Individual 1: Weight at time 1 was \\(240lbs\\) and weight at time 2 is \\(240lbs\\). Relative standing has gone down so there is positive residualized change. Individual 2: Weight at time 1 was \\(250lbs\\) and weight at time 2 is \\(230lbs\\). Relative standing is the same so there is no residualized change. Critique of Difference Score Model There have also been some major historical critiques of differences scores (e.g. Cronbach and Furby (1970)). Much of these critiques are based on reliability and the following rationale. Consider a typical model for a set of repeated measures, \\[ y_{1i} = y_{true,i} + e_{1i} \\\\ y_{2i} = y_{true,i} + e_{2i} \\] where \\(y_{true,i}\\) is the unobserved true score at both occasions \\(e_{i}\\) is the unobserved random error that is independent over each occasion Note, in this theoretical model the true score remains the same and all changes are based on random noise. If this model holds then we could write a simple difference score as \\[ D_{i} = y_{2i} - y_{1i} \\\\ \\quad\\quad\\: \\quad\\quad\\:\\quad\\quad\\:\\quad\\quad\\quad= (y_{true,i} + e_{2i}) - (y_{true,i} + e_{1i})\\\\ \\quad\\quad\\: \\quad\\quad\\:\\quad\\quad\\:\\quad\\quad\\quad = (y_{true,i} - y_{true,i}) + (e_{2i} - e_{1i})\\\\ \\quad\\quad = (e_{2i} - e_{1i})\\\\ \\] where Variance of the difference score is entirely based on the variance of the differences in random error scores the reliability of the difference scores is zero Alternative Interpretation This has led many to many historical critiques of difference scores. However, other researchers have pointed out this conclusion is based on how one envisions change. If, for example, we have the following theoretical model for change, \\[ y_{1i} = y_{true,i} + e_{1i}\\\\ y_{2i} = (y_{1i} + \\Delta y_{true,i}) + e_{2i} \\] where \\(y_{true}\\) is the unobserved true score at both occasions \\(\\Delta y_{true}\\) is the unobserved true change score between occasions \\(e_{i}\\) is the unobserved random error that is independent over each occasion If this model holds, as opposed to the alternative model, then the difference scores \\[ D_{i} = y_{2i} - y_{1i} \\\\ \\quad\\quad\\: \\quad\\quad\\:\\quad\\quad\\:\\quad\\quad\\quad= (y_{true,i} + e_{2i}) - (y_{true,i} + e_{1i})\\\\ \\quad\\quad\\: \\quad\\quad\\:\\quad\\quad\\:\\quad\\quad\\quad = (y_{true,i} - y_{true,i}) + (e_{2i} - e_{1i})\\\\ \\quad\\quad \\quad\\quad\\quad= \\Delta y_{1} + (e_{2i} - e_{1i})\\\\ \\] where now the variance of the difference score is based on the variance of the differences in the random error scores and the gain in the true scores the relative size of the true score gain determines variance and reliability of the difference scores this implies difference scores may be an entirely appropriate means for measuring change Comparing Models To compare the autoregressive (residualized change) and difference score models, it is useful to better understand their equivalence. Let’s start with the autoregressive model and see if we can get to the change score model. \\[ \\begin{aligned} y_{2i} &amp;= \\beta_{0} + \\beta^{AR}_{1}y_{1i} + \\epsilon_{i} &amp; (\\text{AR Model}) \\\\ y_{2i} - y_{1i} &amp;= \\beta_{0} + \\beta^{AR}_{1}y_{1i} + \\epsilon_{i} - y_{1i} &amp; (\\text{Subtract} \\: y_{1i}) \\\\ y_{2i} - y_{1i} &amp;= \\beta_{0} + \\beta^{AR}_{1}y_{1i} - y_{1i} + \\epsilon_{i} &amp; (\\text{Rearrange}) \\\\ (y_{2i} - y_{1i}) &amp;= \\beta_{0} + (\\beta^{AR}_{1}-1)y_{1i} + \\epsilon_{i} &amp; (\\text{Factor}) \\\\ \\Delta y_{i} &amp;= \\beta_{0} + (\\beta^{AR}_{1}-1)y_{1i} + \\epsilon_{i} &amp; (\\text{Def. of} \\: \\Delta) \\\\ \\Delta y_{i} &amp;= \\beta_{0} + \\beta^{DIFF}_{1}y_{1i} + \\epsilon_{i} &amp; (\\text{Equivalence}) \\end{aligned} \\] Here we have shown analytically an equivalence relationship between the slope coefficients from the two models, namely, \\[ \\beta^{DIFF}_{1} = (\\beta^{AR}_{1} - 1) \\] We can confirm this relationship look at our model output: coef(ARfit) ## (Intercept) verb1 ## 20.224849 1.201174 coef(DIFfit) ## (Intercept) verb1 ## 20.2248493 0.2011741 References "],["11.7-adding-explanatory-variables.html", "11.7 Adding Explanatory Variables", " 11.7 Adding Explanatory Variables Assessing the effects of an intervention, grouping variable or construct on change in another construct is a central goal in virtually all areas of the social, health and behavioral sciences. developmental researchers interested in how the occurrence of particular life events (e.g., job loss, marriage) affects changes in well-being or personality. educational psychology researchers investigate how teacher characteristics (e.g.,competence, motivation) predict changes in students’ achievement. Most researchers rely on one of two basic strategies when analyzing the prospective effects of one construct on another construct in the context of two-wave data: (1) the ANCOVA model or (2) the difference score model. For example, in this situation the autoregressive model becomes the traditional ANCOVA model \\[ y_{2i} = \\beta_{0} + \\beta_{1} x_{i} + \\beta_{2} y_{1i} + \\epsilon_{i} \\] and we have the following difference score model \\[ \\Delta_{i} = \\beta_{0} + \\beta_{1}x_{i} + \\epsilon_{i} \\] where \\(x_{i}\\) is either zero or one for the \\(i^{th}\\) individual in the sample. Although not immediately obvious, the difference between these two approaches is that \\(\\beta{2}\\) is estimated in the ANCOVA model, and fixed to \\(1\\) in the difference score approach. As pointed out by many different researchers over the year, it can be challenging to decide which of the two approaches is more appropriate. However, the decision is often critical as the two approaches can yield results that substantially differ concerning the magnitude, sign, and statistical significance of the estimated treatment effect. "],["11.8-lords-paradox.html", "11.8 Lord’s Paradox", " 11.8 Lord’s Paradox Consider the summary of Lord’s Paradox from Wikipedia The most famous formulation of Lord’s paradox comes from his 1967 paper: “A large university is interested in investigating the effects on the students of the diet provided in the university dining halls and any sex differences in these effects. &gt; Various types of data are gathered. In particular, the weight of each student at the time of his arrival in September and his weight the following June are recorded.” (Lord &gt; 1967, p. 304) In both September and June, the overall distribution of male weights is the same, although individuals’ weights have changed, and likewise for the distribution of female weights. Lord imagines two statisticians who use different common statistical methods but reach opposite conclusions. One statistician uses difference scores and finds no significant difference between genders: “[A]s far as these data are concerned, there is no evidence of any interesting effect of diet (or of anything else) on student weights. In particular, there is no evidence of any differential effect on the two sexes, since neither group shows any systematic change.” (pg. 305) Visually, the first statistician sees that neither group mean (‘A’ and ‘B’) has changed, and concludes that the new diet had no causal impact. The second statistician uses the ANCOVA (residualized change) model. They find a significant difference between the two sexes. Visually, the second statistician fits a regression model (green dotted lines), finds that the intercept differs for boys vs girls, and concludes that the new diet had a larger impact for males. Lord concluded: “there simply is no logical or statistical procedure that can be counted on to make proper allowance for uncontrolled preexisting differences between groups.” image "],["11.9-example-data-ii.html", "11.9 Example Data II", " 11.9 Example Data II Now, let us compare the common situation where we have a predictor of interest. Castro-Schilo and Grimm (2018) consider a situation common to behavioral science researchers. Sofia, a close relationships’ researcher, is interested in examining change in relationship satisfaction before and after marriage. Specifically, she wants to know whether cohabiting prior to marriage has an effect on changes in relationship satisfaction. As she plans her data analysis strategy, Sofia realizes she has two options: (1) use the postmarriage relationship satisfaction score as her outcome and include the premarriage score in her analysis as a predictor, in addition to whether or not couples cohabited prior to marriage to account for individual differences in baseline relationship satisfaction (known as residualized change approach) or (2) compute the difference between the postmarriage relationship satisfaction score and the premarriage relationship satisfaction score and use it as her outcome to test the effect of cohabiting prior to marriage (known as difference score approach). Which of these two strategies is most appropriate? Should both strategies provide the same results? Sofia has heard difference scores have a bad reputation, why is that? Are there additional strategies Sofia is not aware of that might be preferred over her current options? 11.9.1 Generate Some Data According to (Castro-Schilo and Grimm 2018) Let’s generate some data according to the example from Castro-Schilo and Grimm (2018). Below is one possible data generating model. We will consider two scenarios. In both scenarios the following facts will be true: Both data sets have an equal number of dyads that did and did not cohabit Cohabitation had a null effect on changes in relationship satisfaction. Dyads did not exhibit any changes in relationship satisfaction over time. In this first example dataset, however, there are preexisting groups at the premarriage assessment. That is, because in a real investigation of cohabitation a controlled experiment would not be feasible, we generated one data set with lower relationship satisfaction at baseline for dyads who reported cohabiting. Note that our description of the first dataset corresponds to the assumption made by the difference score model (i.e., no change between groups across time under the assumption of the null hypothesis for the key predictor; there are two populations at baseline). # set a seed so we all have the same data set.seed (1234) # create an id variable (N = 100 in each group) id &lt;- c(1:100, 1:100) # create a time variable (0 = time 1, 1 = time 2) time &lt;- c(rep(0, 100), rep(1, 100)) # create grouping variable with equal number cohabiting (1) and not (0) cohabit &lt;- rep(c(0,1), 100) # put all variables in a dataframe data &lt;- data.frame(id, cohabit, time) # generate dataset A data_A &lt;- data data_A$score &lt;- 4.5 + # intercept -2*data_A$cohabit + # difference in cohabitation 0*data_A$time + # effect of time 0*data_A$cohabit*data_A$time + # interaction rnorm(200, 0, 0.3) # error # Make into wide format for computing difference score data_A &lt;- reshape( data_A, v.names = &#39;score&#39;, timevar = &quot;time&quot;, idvar = &quot;id&quot;, direction= &quot;wide&quot; ) # assign appropriate variable names names(data_A) &lt;- c(&#39;id&#39;, &#39;cohabit&#39;, &#39;score1&#39;, &#39;score2&#39;) # compute difference variable data_A$diff &lt;- data_A$score2 - data_A$score1 # declare cohabiting variable as a factor (categorical variable) data_A$cohabit &lt;- factor(data_A$cohabit) 11.9.2 Plot Data # visualize the data library(ggplot2) ggplot(data_A, aes(x = score1, y = score2, shape= cohabit)) + geom_point(size = 3) + geom_smooth(method = lm, se = F) + xlab(&quot;Relationship Satisfaction Time 1&quot;) + ylab(&quot;Relationship Satisfaction Time 2&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; 11.9.3 Fit Residualized Change Model Recall that the main interest in these analyses is to assess the potential effect of cohabitation on changes in relationship satisfaction. According to the residualized change model we fit to data set A (which is an ANCOVA model in this application because of the categorical predictor), the factors explain 92% of the variance in the outcome and results suggest there is a statistically and practically significant effect of cohabitation in dyad’s relationship satisfaction at Time 2, controlling for baseline relationship satisfaction. # ancova model mylml &lt;- lm(score2 ~ score1 + cohabit, data = data_A) summary (mylml) ## ## Call: ## lm(formula = score2 ~ score1 + cohabit, data = data_A) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.86084 -0.17640 0.00684 0.17019 0.92449 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.66901 0.46562 10.028 &lt; 2e-16 *** ## score1 -0.02875 0.10390 -0.277 0.783 ## cohabit1 -2.11473 0.21860 -9.674 6.78e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3114 on 97 degrees of freedom ## Multiple R-squared: 0.9183, Adjusted R-squared: 0.9167 ## F-statistic: 545.4 on 2 and 97 DF, p-value: &lt; 2.2e-16 11.9.4 Fit Difference Score Model # difference model mylm2 = lm(diff ~ cohabit, data = data_A) summary(mylm2) ## ## Call: ## lm(formula = diff ~ cohabit, data = data_A) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.29920 -0.22982 0.00904 0.28891 1.09081 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.07941 0.06212 1.278 0.204 ## cohabit1 -0.04002 0.08786 -0.455 0.650 ## ## Residual standard error: 0.4393 on 98 degrees of freedom ## Multiple R-squared: 0.002113, Adjusted R-squared: -0.00807 ## F-statistic: 0.2075 on 1 and 98 DF, p-value: 0.6498 In contrast, when we fit the difference score model to the same data, less than 1% of the variance in the outcome is explained by the model, and results suggested that cohabitation did not have an effect on relationship satisfaction change. As expected, the inferences from these models are strikingly different, which is an example of Lord’s paradox and can be attributed to the differences in baseline scores across couples who cohabit and those who do not. References "],["11.10-example-data-iii.html", "11.10 Example Data III", " 11.10 Example Data III Consider a second example dataset, where, again We have an equal number of dyads that did and did not cohabit Cohabitation had a null effect on changes in relationship satisfaction. Dyads did not exhibit any changes in relationship satisfaction over time. Unlike the first dataset, however, this second dataset would have been obtained if Sofia could have randomly assigned dyads to cohabit or live apart. This second data set is in line with the assumption of the residualized change model, where the Time 1 score is uncorrelated with the key predictor, which is equivalent to affirming there are no preexisting group differences at baseline; there is one population of dyads at baseline with respect to relationship satisfaction. 11.10.1 Generate Some Data According to (Castro-Schilo and Grimm 2018) Example B # set a seed so we all have the same data set.seed (1234) # create an id variable (N = 100 in each group) id &lt;- c(1:100, 1:100) # create a time variable (0 = time 1, 1 = time 2) time &lt;- c(rep(0, 100), rep(1, 100)) # create grouping variable with equal number cohabiting (1) and not (0) cohabit &lt;- rep(c(0,1), 100) # put all variables in a dataframe data &lt;- data.frame(id, cohabit, time) # generate dataset A data_B &lt;- data data_B$score &lt;- 4.5 + # intercept 0*data_B$cohabit + # difference in cohabitation 0*data_B$time + # effect of time 0*data_B$cohabit*data_B$time + # interaction rnorm(200, 0, 0.3) # error # Make into wide format for computing difference score data_B &lt;- reshape( data_B, v.names = &#39;score&#39;, timevar = &quot;time&quot;, idvar = &quot;id&quot;, direction= &quot;wide&quot; ) # assign appropriate variable names names(data_B) &lt;- c(&#39;id&#39;, &#39;cohabit&#39;, &#39;score1&#39;, &#39;score2&#39;) # compute difference variable data_B$diff &lt;- data_B$score2 - data_B$score1 # declare cohabiting variable as a factor (categorical variable) data_B$cohabit &lt;- factor(data_B$cohabit) 11.10.2 Plot Data # visualize the data library(ggplot2) ggplot(data_B, aes(x = score1, y = score2, shape= cohabit)) + geom_point(size = 3) + geom_smooth(method = lm, se = F) + xlab(&quot;Relationship Satisfaction Time 1&quot;) + ylab(&quot;Relationship Satisfaction Time 2&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; 11.10.3 Fit Residualized Change Model # ancova model mylml &lt;- lm(score2 ~ score1 + cohabit, data = data_B) summary (mylml) ## ## Call: ## lm(formula = score2 ~ score1 + cohabit, data = data_B) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.86084 -0.17640 0.00684 0.17019 0.92449 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.66901 0.46562 10.028 &lt;2e-16 *** ## score1 -0.02875 0.10390 -0.277 0.783 ## cohabit1 -0.05724 0.06230 -0.919 0.361 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3114 on 97 degrees of freedom ## Multiple R-squared: 0.009265, Adjusted R-squared: -0.01116 ## F-statistic: 0.4535 on 2 and 97 DF, p-value: 0.6367 11.10.4 Fit Difference Score Model # difference model mylm2 = lm(diff ~ cohabit, data = data_B) summary(mylm2) ## ## Call: ## lm(formula = diff ~ cohabit, data = data_B) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.29920 -0.22982 0.00904 0.28891 1.09081 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.07941 0.06212 1.278 0.204 ## cohabit1 -0.04002 0.08786 -0.455 0.650 ## ## Residual standard error: 0.4393 on 98 degrees of freedom ## Multiple R-squared: 0.002113, Adjusted R-squared: -0.00807 ## F-statistic: 0.2075 on 1 and 98 DF, p-value: 0.6498 References "],["11.11-closing-thoughts.html", "11.11 Closing Thoughts", " 11.11 Closing Thoughts Interpretations of change are fundamentally restricted by the choice of model The residualized change and difference score models will often lead to different inferences about the effect of the predictor of interest. The assumptions each of these models make are untestable, and this makes it challenging to argue that either model is more appropriate than the other. Dyads who report cohabiting prior to marriage will never be able to report what their relationship satisfaction after marriage would have been if they had not cohabited. We will never know what level of relationship satisfaction would have been reported by dyads who did not cohabit before marriage if they had in fact chosen to live with their partners prior to getting married. 11.11.0.1 Random Assignment When experimentatal procedures are followed, dyads would be randomly assigned to a grouping variable, and this would bring independence of the grouping variable with the pre-test scores. Thus, proper experimentation prohibits preexisting groups to be formed at baseline. Under such circumstances, the residualized change model and difference score models will arrive at the same inference, but the former has the advantage of having more power because it makes the correct assumption about the distribution of baseline scores. 11.11.0.2 Observational Studies Which should be our choice when experimentation is not possible? If we have different populations with different levels of the outcome at baseline, the residualized change model may not be appropriate. For researchers going this route, a latent change score model may alleviate some concerns about reliability and measurement error. 11.11.0.3 Two-Occassion Change If you found all of this to be inherently confusing it is worth thinking about the following quote: “Two waves of data are better than one, but maybe not much better.” (Rogosa, Brandt, and Zimowski 1982). References "],["11.12-references-4.html", "11.12 References", " 11.12 References "]]
