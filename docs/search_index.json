[["index.html", "HDFS 523: Strategies for Data Analysis in Developmental Research Chapter 1 About This Book", " HDFS 523: Strategies for Data Analysis in Developmental Research Zachary F. Fisher 2024-03-20 Chapter 1 About This Book This book provides the course notes for HDFS 523. It is currently under development, so any feedback is appreciated (e.g., during class, via email, or the edit link in the header). This first chapter is just about how to use the book – the course content starts in Chapter 2. "],["1.1-why-this-book.html", "1.1 Why this book?", " 1.1 Why this book? There are a few goals of moving from “textbook + slides + exercises” to an ebook. For now, the main goal is to update and integrate code contents from the course into one consistent format, rather than having multiple files to sort through on Canvas. "],["1.2-code-folding.html", "1.2 Code Folding", " 1.2 Code Folding The book combines lecture slides and R coding examples. It is often convenient to hide code when introducing new material. This is accomplished using code folding. An example of code folding is given on this page. Below, a histogram integrated into the text. By clicking on the button called “Show Code” on the top of the page, the R code that produced the histogram will also be visible. Notice that you may need to scroll horizontally to see all of the text in the code window. Also notice that when you hover your mouse over the code window, an icon appears in the top right corner – this lets you copy the block of code with one click. # Here is some R code. You don&#39;t have to look at it when reading the book, but it is here when you need it x &lt;- rnorm(200) hist(x, col = &quot;#4B9CD3&quot;) "],["1.3-acknowledgements.html", "1.3 Acknowledgements", " 1.3 Acknowledgements Many people have contributed to the course materials for HDFS 523. Most importantly, many of the original R markdown files for the course were developed by Nilam Ram and Zita Oravecz. "],["2-chapter-2.html", "Chapter 2 Data Cleaning", " Chapter 2 Data Cleaning In Chapter 2 we will work through some basic data cleaning operations useful in longitudinal data analysis. The basic idea is provide a set of scripts to use for exploring new repeated measures data sets. "],["2.1-example-data.html", "2.1 Example Data", " 2.1 Example Data For Chapter 2 we will make use of the longitudinal Wechsler Intelligence Scale for Children [WISC; Wechsler, -Wechsler (1949)] dataset described by Osborne and Suddick (1972). These data have been detailed extensively in a number of papers (McArdle and Epstein 1987; McArdle 1988; Mcardle and Aber 1990; McArdle and Nesselroade 1994) and are used here with with permission. The WISC data contains repeated measures data from 204 children between the ages of 6 and 11 years old (during grades 6, 7, 9 and 11). Thee repeated measures include component scores for the verbal tests and performance subtests at all four occasions, along with verbal subtest scores for the information, comprehension, similarities, and vocabulary domains at the first and last measurement occasion. The demographics variables mother’s education (continuous in years) and mother graduated high school (dichotomous) are also included. References "],["2.2-reading-in-repeated-measures-data.html", "2.2 Reading in Repeated Measures Data", " 2.2 Reading in Repeated Measures Data We can read in the WISC data directly from the QuantDev website. filepath &lt;- &quot;https://quantdev.ssri.psu.edu/sites/qdev/files/wisc3raw.csv&quot; wisc3raw &lt;- read.csv(file=url(filepath), header=TRUE) Additional details on importing different data types into R can be found here: http://www.statmethods.net/input/importingdata.html. "],["2.3-familiarize-yourself-with-the-data.html", "2.3 Familiarize Yourself with the Data", " 2.3 Familiarize Yourself with the Data Let’s take an initial look at the structure of our data object using str() str(wisc3raw) ## &#39;data.frame&#39;: 204 obs. of 20 variables: ## $ id : int 1 2 3 4 5 6 7 8 9 10 ... ## $ verb1 : num 24.4 12.4 32.4 22.7 28.2 ... ## $ verb2 : num 27 14.4 33.5 28.4 37.8 ... ## $ verb4 : num 39.6 21.9 34.3 42.2 41.1 ... ## $ verb6 : num 55.6 37.8 50.2 44.7 71 ... ## $ perfo1 : num 19.8 5.9 27.6 33.2 27.6 ... ## $ perfo2 : num 23 13.4 45 29.7 44.4 ... ## $ perfo4 : num 43.9 18.3 47 46 65.5 ... ## $ perfo6 : num 44.2 40.4 77.7 61.7 64.2 ... ## $ info1 : num 31.3 13.8 35 24.8 25.3 ... ## $ comp1 : num 25.6 14.8 34.7 31.4 30.3 ... ## $ simu1 : num 22.93 7.58 28.05 8.21 15.98 ... ## $ voca1 : num 22.2 15.4 26.8 20.2 35.4 ... ## $ info6 : num 69.9 41.9 60.4 52.9 67.4 ... ## $ comp6 : num 44.4 44.9 50.3 42.7 86.7 ... ## $ simu6 : num 68 33.9 35.8 45.8 72.4 ... ## $ voca6 : num 51.2 37.7 55.5 36 60.4 ... ## $ momed : num 9.5 5.5 14 14 11.5 14 9.5 5.5 9.5 11.5 ... ## $ grad : int 0 0 1 1 0 1 0 0 0 0 ... ## $ constant: int 1 1 1 1 1 1 1 1 1 1 ... From the output, we can also see that the data frame consists of 204 observations (rows) and 20 variables (columns). Each variable’s name and data type is also listed. Methods like the ones above can be an effective way to initially familiarize yourself with the main features of a dataset. "],["2.4-look-for-duplicated-ids.html", "2.4 Look for Duplicated IDs", " 2.4 Look for Duplicated IDs It is always worth looking for non-unique ID numbers when ID labels are included in a dataset. Here we have an id variable indicating the subject number. Since our data is in a long format (more on that later) duplicate IDs may indicate a potential problem with the data source or clues on how the data is structured. any(duplicated(wisc3raw$id)) ## [1] FALSE "],["2.5-using-table-to-spot-irregularities.html", "2.5 Using table() to Spot Irregularities", " 2.5 Using table() to Spot Irregularities When a variable takes on a limited range of values it is often useful to screen for irregularities or invalid values. This is common across all variable types and can occur for character strings, numeric, integer and factor types. For example, we would expect the grad variable to only take the values of zero or one. We can use the table() function to quickly confirm this. By default table() simply omits any values coded as NA. To include a count of the NA values use the useNA argument of table() as follows: table(wisc3raw$grad, useNA = &quot;always&quot;) ## ## 0 1 &lt;NA&gt; ## 158 46 0 "],["2.6-missing-data.html", "2.6 Missing Data", " 2.6 Missing Data Dealing with missing data in a consistent manner is one of the most important aspects of data cleaning. When data are imported into R it is common to discover missing values are coded according to a variety of conventions. Often a first step in handling missing data involves recoding missing values as NA. Writing bespoke code to handle the different types of missing data one might encounter is tedious and unnecessary. naniar (Tierney et al. 2021) is a useful package with many convenience functions for managing missing data in R. Here we demonstrate some of this functionality. 2.6.1 Generating Example Data Since the WISC data does not contain missing values it is helpful to generate a synthetic dataset containing some commonly encountered missing data codes. set.seed(123) wisc_miss &lt;- wisc3raw wisc_miss$verb1[sample(nrow(wisc_miss),100)] &lt;- -99 wisc_miss$comp1[sample(nrow(wisc_miss),75)] &lt;- &quot;N/A&quot; wisc_miss$info1[sample(nrow(wisc_miss),50)] &lt;- &quot;NA&quot; 2.6.2 Recoding Values with NA Now that we have a dataset with missing values we can use naniar to recode these values to NA. na_strings &lt;- c(&quot;NA&quot;, &quot;N/A&quot;, -99) wisc_miss &lt;- naniar::replace_with_na_all( wisc_miss, condition = ~.x %in% na_strings ) See the naniar vignette on recoding NA values for more detailed information on the package functionality. 2.6.3 Missing Data Visualization Once we have recoded our data in a consistent manner we can use visualizations to explore the missing data. The vis_miss() function from naniar is a good starting point for visualizing the amount of missing data in our dataset. The plots shows the missing values in black and non-missing values in gray. In addition, percentages of missing data in both the dataset and individual variables are provided. naniar::vis_miss(wisc_miss) It is often useful to look at combinations of missingness among different variables. naniar::gg_miss_upset(wisc_miss) We can also look at the percentage of missing data across a factor variable. naniar::gg_miss_fct(x = wisc_miss, fct = grad) Many missing data visualizations are described in the naniar vignette on missing data visualization including plots for exploring missing data mechanisms. References "],["2.7-exporting-data.html", "2.7 Exporting Data", " 2.7 Exporting Data Depending on work-flow, you may need to export your dataset for use in another statistical software program. The write.csv() function is a convenient method for outputting comma delimited files. write.csv(wisc3raw, file = &quot;wisc3raw.csv&quot;, row.names = FALSE, na = &quot;-99&quot;) Note that by default the write.csv() function will include an extra column of row numbers and will notate missing data with an NA. More information on exporting data is available at http://www.statmethods.net/input/exportingdata.html. "],["2.8-reshaping-repeated-measures-data.html", "2.8 Reshaping Repeated Measures Data", " 2.8 Reshaping Repeated Measures Data Behavioral science tends to use relational data structures - in basic form, spreadsheets. Typically, the data are stored in a data frame (a “fancy” matrix) with multiple rows and columns. Two common schemata used to accommodate repeated measures data are wide format and long format. Different analysis and plotting functions require different kinds of data input. Thus, it is imperative that one can convert the data back and forth between wide and long formats. There are lots of ways to do this. We illustrate one way. Sidebar: The dput() function provides a convenient method to get the variable names (or any R object) into a format that can be read back into R. For example, this can be helpful when working with a long vector of strings. dput(colnames(wisc3raw)) ## c(&quot;id&quot;, &quot;verb1&quot;, &quot;verb2&quot;, &quot;verb4&quot;, &quot;verb6&quot;, &quot;perfo1&quot;, &quot;perfo2&quot;, ## &quot;perfo4&quot;, &quot;perfo6&quot;, &quot;info1&quot;, &quot;comp1&quot;, &quot;simu1&quot;, &quot;voca1&quot;, &quot;info6&quot;, ## &quot;comp6&quot;, &quot;simu6&quot;, &quot;voca6&quot;, &quot;momed&quot;, &quot;grad&quot;, &quot;constant&quot;) First, let’s subset our data to only include the variables we need for this analysis. var_names_sub &lt;- c( &quot;id&quot;, &quot;verb1&quot;, &quot;verb2&quot;, &quot;verb4&quot;, &quot;verb6&quot;, &quot;perfo1&quot;, &quot;perfo2&quot;, &quot;perfo4&quot;, &quot;perfo6&quot;, &quot;momed&quot;, &quot;grad&quot; ) wiscraw &lt;- wisc3raw[,var_names_sub] head(wiscraw) ## id verb1 verb2 verb4 verb6 perfo1 perfo2 perfo4 perfo6 momed grad ## 1 1 24.42 26.98 39.61 55.64 19.84 22.97 43.90 44.19 9.5 0 ## 2 2 12.44 14.38 21.92 37.81 5.90 13.44 18.29 40.38 5.5 0 ## 3 3 32.43 33.51 34.30 50.18 27.64 45.02 46.99 77.72 14.0 1 ## 4 4 22.69 28.39 42.16 44.72 33.16 29.68 45.97 61.66 14.0 1 ## 5 5 28.23 37.81 41.06 70.95 27.64 44.42 65.48 64.22 11.5 0 ## 6 6 16.06 20.12 38.02 39.94 8.45 15.78 26.99 39.08 14.0 1 2.8.1 Reshape Wide to Long One way to go from wide to long is using the reshape() function from base R. Notice, the varying argument contains the repeated measures columns we want to stack and the timevar is a new variable containing the grade level information previosuly appended at the end of the colnames listed in varying. # reshape data from wide to long wisclong &lt;- reshape( data = wiscraw, varying = c(&quot;verb1&quot;, &quot;verb2&quot;, &quot;verb4&quot;,&quot;verb6&quot;, &quot;perfo1&quot;,&quot;perfo2&quot;,&quot;perfo4&quot;,&quot;perfo6&quot;), timevar = c(&quot;grade&quot;), idvar = c(&quot;id&quot;), direction = &quot;long&quot;, sep = &quot;&quot; ) # reorder by id and day wisclong &lt;- wisclong[ order(wisclong$id, wisclong$grade), ] head(wisclong, 8) ## id momed grad grade verb perfo ## 1.1 1 9.5 0 1 24.42 19.84 ## 1.2 1 9.5 0 2 26.98 22.97 ## 1.4 1 9.5 0 4 39.61 43.90 ## 1.6 1 9.5 0 6 55.64 44.19 ## 2.1 2 5.5 0 1 12.44 5.90 ## 2.2 2 5.5 0 2 14.38 13.44 ## 2.4 2 5.5 0 4 21.92 18.29 ## 2.6 2 5.5 0 6 37.81 40.38 Again, notice how reshape automatically split verb1, verb2, etc. into a string name and a grade variable. 2.8.2 Reshape Long to Wide Now we go from long to wide, again using the reshape() function. The v.names argument specifies the variables to be expanded column wise based on the repeated measure specified in timevar. #reshaping long to wide wiscwide &lt;- reshape( data = wisclong, timevar = c(&quot;grade&quot;), idvar = c(&quot;id&quot;), v.names = c(&quot;verb&quot;,&quot;perfo&quot;), direction = &quot;wide&quot;, sep = &quot;&quot; ) # reordering columns wiscwide &lt;- wiscwide[, c( &quot;id&quot;, &quot;verb1&quot;, &quot;verb2&quot;, &quot;verb4&quot;, &quot;verb6&quot;, &quot;perfo1&quot;, &quot;perfo2&quot;, &quot;perfo4&quot;, &quot;perfo6&quot;, &quot;momed&quot;,&quot;grad&quot; )] head(wiscwide) ## id verb1 verb2 verb4 verb6 perfo1 perfo2 perfo4 perfo6 momed grad ## 1.1 1 24.42 26.98 39.61 55.64 19.84 22.97 43.90 44.19 9.5 0 ## 2.1 2 12.44 14.38 21.92 37.81 5.90 13.44 18.29 40.38 5.5 0 ## 3.1 3 32.43 33.51 34.30 50.18 27.64 45.02 46.99 77.72 14.0 1 ## 4.1 4 22.69 28.39 42.16 44.72 33.16 29.68 45.97 61.66 14.0 1 ## 5.1 5 28.23 37.81 41.06 70.95 27.64 44.42 65.48 64.22 11.5 0 ## 6.1 6 16.06 20.12 38.02 39.94 8.45 15.78 26.99 39.08 14.0 1 Using functions included in base R can be useful in a number of situations. One example is package development where one may wants to limit dependencies. That said, many people find reshape to be unnecessarily complicated. A similar, and potentially more convenient, set of functions have been developed for reshaping data in the tidyr (Wickham 2021) package. For those interested take a look at the pivot_longer() and pivot_wider() functions. For examples using tidyr to reshape data see the tidyr vignette on pivoting. References "],["3-chapter-3.html", "Chapter 3 Describing Longitudinal Data", " Chapter 3 Describing Longitudinal Data In Chapter 3 we will look at some option for describing and visualizing longitudinal data. "],["3.1-example-data-1.html", "3.1 Example Data", " 3.1 Example Data Again we will make use of the WISC data described in Chapter 2. The following commands recreate the wide and long data we will use throughout this chapter. filepath &lt;- &quot;https://quantdev.ssri.psu.edu/sites/qdev/files/wisc3raw.csv&quot; wisc3raw &lt;- read.csv(file=url(filepath),header=TRUE) var_names_sub &lt;- c( &quot;id&quot;, &quot;verb1&quot;, &quot;verb2&quot;, &quot;verb4&quot;, &quot;verb6&quot;, &quot;perfo1&quot;, &quot;perfo2&quot;, &quot;perfo4&quot;, &quot;perfo6&quot;, &quot;momed&quot;, &quot;grad&quot; ) wiscraw &lt;- wisc3raw[,var_names_sub] # reshaping wide to long wisclong &lt;- reshape( data = wiscraw, varying = c(&quot;verb1&quot;, &quot;verb2&quot;, &quot;verb4&quot;,&quot;verb6&quot;, &quot;perfo1&quot;,&quot;perfo2&quot;,&quot;perfo4&quot;,&quot;perfo6&quot;), timevar = c(&quot;grade&quot;), idvar = c(&quot;id&quot;), direction = &quot;long&quot;, sep = &quot;&quot; ) # reorder by id and day wisclong &lt;- wisclong[ order(wisclong$id, wisclong$grade), ] #reshaping long to wide wiscwide &lt;- reshape( data = wisclong, timevar = c(&quot;grade&quot;), idvar = c(&quot;id&quot;), v.names = c(&quot;verb&quot;,&quot;perfo&quot;), direction = &quot;wide&quot;, sep = &quot;&quot; ) # reordering columns wiscwide &lt;- wiscwide[, c( &quot;id&quot;, &quot;verb1&quot;, &quot;verb2&quot;, &quot;verb4&quot;, &quot;verb6&quot;, &quot;perfo1&quot;, &quot;perfo2&quot;, &quot;perfo4&quot;, &quot;perfo6&quot;, &quot;momed&quot;,&quot;grad&quot; )] "],["3.2-describing-means-and-variances.html", "3.2 Describing Means and Variances", " 3.2 Describing Means and Variances Once the wide and long data sets are in place, we can begin describing and plotting the data. Descriptive statistics and visualization are one of the most important aspects of data analysis. Descriptives and plots will be produced from wide data and long data to show the information that can be gleaned from each construction. Having both in place facilitates learning about the data. Continually keep in mind what portions of the data-box are being described (e.g., persons, variables, occasions). We can do a quick look at descriptives using the describe() function from the psych (Revelle 2021) package. Note the n in both outputs. psych::describe(wiscwide) ## vars n mean sd median trimmed mad min max range skew ## id 1 204 102.50 59.03 102.50 102.50 75.61 1.00 204.00 203.00 0.00 ## verb1 2 204 19.59 5.81 19.34 19.50 5.41 3.33 35.15 31.82 0.13 ## verb2 3 204 25.42 6.11 25.98 25.40 6.57 5.95 39.85 33.90 -0.06 ## verb4 4 204 32.61 7.32 32.82 32.42 7.18 12.60 52.84 40.24 0.23 ## verb6 5 204 43.75 10.67 42.55 43.46 11.30 17.35 72.59 55.24 0.24 ## perfo1 6 204 17.98 8.35 17.66 17.69 8.30 0.00 46.58 46.58 0.35 ## perfo2 7 204 27.69 9.99 26.57 27.34 10.51 7.83 59.58 51.75 0.39 ## perfo4 8 204 39.36 10.27 39.09 39.28 10.04 7.81 75.61 67.80 0.15 ## perfo6 9 204 50.93 12.48 51.76 51.07 13.27 10.26 89.01 78.75 -0.06 ## momed 10 204 10.81 2.70 11.50 11.00 2.97 5.50 18.00 12.50 -0.36 ## grad 11 204 0.23 0.42 0.00 0.16 0.00 0.00 1.00 1.00 1.30 ## kurtosis se ## id -1.22 4.13 ## verb1 -0.05 0.41 ## verb2 -0.34 0.43 ## verb4 -0.08 0.51 ## verb6 -0.36 0.75 ## perfo1 -0.11 0.58 ## perfo2 -0.21 0.70 ## perfo4 0.59 0.72 ## perfo6 0.18 0.87 ## momed 0.01 0.19 ## grad -0.30 0.03 psych::describe(wisclong) ## vars n mean sd median trimmed mad min max range skew ## id 1 816 102.50 58.93 102.50 102.50 75.61 1.00 204.00 203.00 0.00 ## momed 2 816 10.81 2.69 11.50 11.00 2.97 5.50 18.00 12.50 -0.36 ## grad 3 816 0.23 0.42 0.00 0.16 0.00 0.00 1.00 1.00 1.31 ## grade 4 816 3.25 1.92 3.00 3.19 2.22 1.00 6.00 5.00 0.28 ## verb 5 816 30.34 11.86 28.46 29.39 11.33 3.33 72.59 69.26 0.71 ## perfo 6 816 33.99 16.14 33.14 33.34 18.14 0.00 89.01 89.01 0.34 ## kurtosis se ## id -1.20 2.06 ## momed 0.03 0.09 ## grad -0.28 0.01 ## grade -1.43 0.07 ## verb 0.33 0.42 ## perfo -0.43 0.56 3.2.1 Verbal Ability (All Persons and Occasions) Let’s focus on the repeated measures of verbal ability. This step is useful to get a general view of what verbal ability scores look like across persons and occasions, but note that we are ignoring Time. In doing so we are not considering how the repeated measures are nested within individuals. psych::describe(wisclong$verb) ## vars n mean sd median trimmed mad min max range skew kurtosis ## X1 1 816 30.34 11.86 28.46 29.39 11.33 3.33 72.59 69.26 0.71 0.33 ## se ## X1 0.42 In addition to the descriptive statistics we can look at a boxplot of verbal ability scores across persons and occasions. Here we will start to use the ggplot2 (Wickham 2016) package. library(&quot;ggplot2&quot;) ## Warning: package &#39;ggplot2&#39; was built under R version 4.3.2 ggplot(data = wisclong, aes(x=verb, y=..density..)) + geom_histogram(binwidth=2.5, fill = &quot;white&quot;, color = &quot;black&quot;) + geom_density(color = &quot;red&quot;) + ggtitle(&quot;Verbal Ability Score (across persons and occasions)&quot;) + xlab(&quot;Verbal Ability (Grade 1 to 6)&quot;) + ylab(&quot;Density&quot;) + theme_bw() + theme( panel.grid.major = element_blank(), panel.grid.minor = element_blank() ) ## Warning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0. ## ℹ Please use `after_stat(density)` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. 3.2.2 Verbal Ability (Across Time) Note that our variable is actually “multivariate” because we have repeated measures. We should really consider the time-dependence when we are looking at descriptive statistics and plots. Let’s now look at verbal ability scores across time collapsed across individuals. This can be done using either the describe() function and the wide data or the describeBy() function and the long data. Let’s look at descriptives using the wide data. psych::describe(wiscwide[,c(&quot;verb1&quot;,&quot;verb2&quot;,&quot;verb4&quot;,&quot;verb6&quot;)]) ## vars n mean sd median trimmed mad min max range skew ## verb1 1 204 19.59 5.81 19.34 19.50 5.41 3.33 35.15 31.82 0.13 ## verb2 2 204 25.42 6.11 25.98 25.40 6.57 5.95 39.85 33.90 -0.06 ## verb4 3 204 32.61 7.32 32.82 32.42 7.18 12.60 52.84 40.24 0.23 ## verb6 4 204 43.75 10.67 42.55 43.46 11.30 17.35 72.59 55.24 0.24 ## kurtosis se ## verb1 -0.05 0.41 ## verb2 -0.34 0.43 ## verb4 -0.08 0.51 ## verb6 -0.36 0.75 Identical results can be obtained using the long data. psych::describeBy(wisclong[,c(&quot;verb&quot;)], group = wisclong$grade) ## ## Descriptive statistics by group ## group: 1 ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 204 19.59 5.81 19.34 19.5 5.41 3.33 35.15 31.82 0.13 -0.05 0.41 ## ------------------------------------------------------------ ## group: 2 ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 204 25.42 6.11 25.98 25.4 6.57 5.95 39.85 33.9 -0.06 -0.34 0.43 ## ------------------------------------------------------------ ## group: 4 ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 204 32.61 7.32 32.82 32.42 7.18 12.6 52.84 40.24 0.23 -0.08 0.51 ## ------------------------------------------------------------ ## group: 6 ## vars n mean sd median trimmed mad min max range skew kurtosis ## X1 1 204 43.75 10.67 42.55 43.46 11.3 17.35 72.59 55.24 0.24 -0.36 ## se ## X1 0.75 We can visualize the distribution of verbal scores across grades in a number of different ways. Here we have a histogram. ggplot(data=wisclong, aes(x=verb)) + geom_histogram(binwidth=5, pad = TRUE, fill=&quot;white&quot;, color=&quot;black&quot;) + facet_grid(grade ~ .) + ggtitle(&quot;Verbal Ability Score (across grades 1, 2, 4, 6)&quot;) + xlab(&quot;Verbal Ability Score&quot;) + ylab(&quot;Density&quot;) + theme_bw() + theme( panel.grid.major = element_blank(), panel.grid.minor = element_blank(), strip.background = element_blank() ) ## Warning: Duplicated aesthetics after name standardisation: pad We can also create notched boxplots of the within-grade distributions (across individuals). From Wikipedia: Notched box plots apply a notch or narrowing of the box around the median. Notches are useful in offering a rough guide of the significance of the difference of medians; if the notches of two boxes do not overlap, this can provide evidence of a statistically significant difference between the medians. Adding the mean value to the plot gives us additonal information about central tendency and skew of the distribution. #boxplot by grade ggplot(data=wisclong, aes(x=factor(grade), y=verb)) + geom_boxplot(notch = TRUE) + stat_summary(fun=&quot;mean&quot;, geom=&quot;point&quot;, shape=23, size=3, fill=&quot;white&quot;) + ggtitle(&quot;Verbal Ability Score (across grades 1, 2, 4, 6)&quot;) + ylab(&quot;Verbal Ability Score&quot;) + xlab(&quot;Grade&quot;) + theme_bw() + theme( panel.grid.major = element_blank(), panel.grid.minor = element_blank(), strip.background = element_blank() ) Finally, we can view overlapping densities of the within-grade distributions of verbal ability scores. ggplot(data=wisclong, aes(x=verb)) + geom_density(aes(group=factor(grade), colour=factor(grade), fill=factor(grade)), alpha=0.3) + guides(colour=&quot;none&quot;, fill=guide_legend(title=&quot;Grade&quot;)) + ggtitle(&quot;Verbal Ability Score (across grades 1, 2, 4, 6)&quot;) + ylab(&quot;Density&quot;) + xlab(&quot;Verbal Ability Score&quot;) + theme_bw() + theme( panel.grid.major = element_blank(), panel.grid.minor = element_blank(), strip.background = element_blank() ) Notice in these plots how much “change” there is at the sample level across grades. Is that expected? References "],["3.3-describing-covariances.html", "3.3 Describing Covariances", " 3.3 Describing Covariances In the previous section we looked at the means and variances. Because these are repeated measures, we can also look at covariances and correlations over time. A simple covariance and correlation matrix of the verbal scores across grades can be produced using the cov() and cor() function. cov(wiscwide[,c(&quot;verb1&quot;,&quot;verb2&quot;,&quot;verb4&quot;,&quot;verb6&quot;)], use=&quot;complete.obs&quot;) ## verb1 verb2 verb4 verb6 ## verb1 33.72932 25.46388 30.88886 40.51478 ## verb2 25.46388 37.28784 33.81957 47.40488 ## verb4 30.88886 33.81957 53.58070 62.25489 ## verb6 40.51478 47.40488 62.25489 113.74332 cor(wiscwide[,c(&quot;verb1&quot;,&quot;verb2&quot;,&quot;verb4&quot;,&quot;verb6&quot;)], use=&quot;complete.obs&quot;) ## verb1 verb2 verb4 verb6 ## verb1 1.0000000 0.7180209 0.7265974 0.6541040 ## verb2 0.7180209 1.0000000 0.7566242 0.7279080 ## verb4 0.7265974 0.7566242 1.0000000 0.7974552 ## verb6 0.6541040 0.7279080 0.7974552 1.0000000 A plot corresponding to the correlation matrix can be obtained in a number of different ways. First, using the pairs() function from base R. pairs(wiscwide[,c(&quot;verb1&quot;,&quot;verb2&quot;,&quot;verb4&quot;,&quot;verb6&quot;)]) There is also a pairs.panel() function in the psych package. Here we see a LOESS smoothed fit line in red. psych::pairs.panels(wiscwide[,c(&quot;verb1&quot;,&quot;verb2&quot;,&quot;verb4&quot;,&quot;verb6&quot;)]) Finally, thescatterplotMatrix() from the car (Fox and Weisberg 2019) package can be used to create scatterplot matrices with confidence bands around the line of best fit. car::scatterplotMatrix(~ verb1 + verb2 + verb4 + verb6, data=wiscwide) Each of these functions can be customized with additional features. Those interested in specifics should consult the help documentation for each function (e.g. ?car::scatterplotMatrix). It is also worth noting the default behavior of these functions is to provide automatic, data-based ranges for each pair of variables separately. References "],["3.4-individual-level-descriptives.html", "3.4 Individual-Level Descriptives", " 3.4 Individual-Level Descriptives Note that our interest is often in individual development, rather than sample development. We need to consider how each individual is changing over time. Thus, we are interested in verbal ability across Time for each individual person. Visualization is typically our best tool for synthesizing the large amounts of information in individual-level data. ggplot(data = wisclong, aes(x = grade, y = verb, group = id)) + geom_point() + geom_line() + scale_x_continuous(breaks=seq(1,6,by=1)) + ylim(0,80) + ggtitle(&quot;Verbal Ability Score (across grades 1, 2, 4, 6)&quot;) + xlab(&quot;Grade&quot;) + ylab(&quot;Verbal Ability Score&quot;) + theme_bw() + theme( panel.grid.major = element_blank(), panel.grid.minor = element_blank(), strip.background = element_blank() ) Sometimes the “blob” gets too dense. This can be fixed by selecting a subset of persons to visualize. ggplot(subset(wisclong, id &lt; 30), aes(x = grade, y = verb, group = id)) + geom_point() + geom_line() + scale_x_continuous(breaks=seq(1,6,by=1)) + ylim(0,80) + ggtitle(&quot;Verbal Ability Score (across grades 1, 2, 4, 6)&quot;) + xlab(&quot;Grade&quot;) + ylab(&quot;Verbal Ability Score&quot;) + theme_bw() + theme( panel.grid.major = element_blank(), panel.grid.minor = element_blank(), strip.background = element_blank() ) We can add some color to our plot using the color argument and treating id as a factor. ggplot(subset(wisclong, id &lt; 30), aes(x = grade, y = verb, group = id, color = factor(id))) + geom_point() + geom_line() + scale_x_continuous(breaks=seq(1,6,by=1)) + ylim(0,80) + ggtitle(&quot;Verbal Ability Score (across grades 1, 2, 4, 6)&quot;) + xlab(&quot;Grade&quot;) + ylab(&quot;Verbal Ability Score&quot;) + theme_bw() + theme( panel.grid.major = element_blank(), panel.grid.minor = element_blank(), strip.background = element_blank(), legend.position = &quot;none&quot; ) We can also get a gradient of colors by treatingid as continuous. ggplot(subset(wisclong, id &lt; 30), aes(x = grade, y = verb, group = id, color = id)) + geom_point() + geom_line() + scale_x_continuous(breaks=seq(1,6,by=1)) + ylim(0,80) + ggtitle(&quot;Verbal Ability Score (across grades 1, 2, 4, 6)&quot;) + xlab(&quot;Grade&quot;) + ylab(&quot;Verbal Ability Score&quot;) + theme_bw() + theme( panel.grid.major = element_blank(), panel.grid.minor = element_blank(), strip.background = element_blank(), legend.position = &quot;none&quot; ) It is also sometimes useful to look at the collection of individual-level plots. ggplot(subset(wisclong, id &lt;= 20), aes(x = grade, y = verb)) + geom_point() + geom_line() + scale_x_continuous(breaks=seq(1,6,by=1)) + ylim(0,80) + ggtitle(&quot;Verbal Ability Score (across grades 1, 2, 4, 6)&quot;) + xlab(&quot;Grade&quot;) + ylab(&quot;Verbal Ability Score&quot;) + theme_bw() + facet_wrap( ~ id) + theme( panel.grid.major = element_blank(), panel.grid.minor = element_blank(), strip.background = element_blank(), legend.position = &quot;none&quot; ) Some other aesthetics to get to the formal APA style. #ggplot version .. see also http://ggplot.yhathq.com/docs/index.html ggplot(subset(wisclong, id &lt;= 20), aes(x = grade, y = verb, group = id)) + geom_point() + geom_line() + xlab(&quot;Grade&quot;) + ylab(&quot;WISC Verbal Score&quot;) + ylim(0,100) + scale_x_continuous(breaks=seq(1,6,by=1)) + ggtitle(&quot;Intraindividual Change in Verbal Ability&quot;) + theme_classic() + #increase font size of axis and point labels theme(axis.title = element_text(size = rel(1.5)), axis.text = element_text(size = rel(1.2)), legend.position = &quot;none&quot;) Saving the plot file. See also outputting plots to a file. ggsave(filename = &quot;wiscverbal.png&quot;, width = 5, height = 5, dpi=300) Now we have a good set of strategies to apply when looking at new longitudinal data. "],["3.5-references.html", "3.5 References", " 3.5 References "],["4-chapter-4.html", "Chapter 4 Matrix Algebra", " Chapter 4 Matrix Algebra In Chapter 4 we will briefly review some basic algebra results useful for this course. Those needing a reliable reference for basic results in matrix algebra should consult the The Matrix Cookbook at https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf. "],["4.1-types-of-matrices.html", "4.1 Types of matrices", " 4.1 Types of matrices Remember that matrices are defined by rows (the first dimension) and columns (the second dimension): \\[ \\underset{m \\times n}{\\mathbf{A}} = \\begin{bmatrix} a_{11} &amp; a_{12} &amp; a_{13} \\\\ a_{21} &amp; a_{22} &amp; a_{23} \\\\ a_{31} &amp; a_{32} &amp; a_{33} \\\\ a_{41} &amp; a_{42} &amp; a_{43} \\end{bmatrix} \\] You can refer to a specific element in matrix using a subscript of the row and column index (e.g. \\(a_{31}\\)). For our purposes there are a few special matrices worth mentioning, 4.1.1 Square A square matrix has the same number of rows and columns. Covariance and correlation matrices are square. \\[ \\underset{n \\times n}{\\mathbf{A}} = \\begin{bmatrix} a_{11} &amp; a_{12} &amp; a_{13} &amp; a_{14} \\\\ a_{21} &amp; a_{22} &amp; a_{23} &amp; a_{24} \\\\ a_{31} &amp; a_{32} &amp; a_{33} &amp; a_{34} \\\\ a_{41} &amp; a_{42} &amp; a_{43} &amp; a_{44} \\end{bmatrix} \\] 4.1.2 Symmetric A symmetric matrix is a square matrix that equals its transpose. This means that corresponding entries on either side of the main diagonal are equal. \\[ \\begin{align} \\underset{n \\times n}{\\mathbf{A}} &amp;= \\begin{bmatrix} a &amp; ab &amp; ac &amp; ad \\\\ ab &amp; b &amp; bc &amp; bd \\\\ ac &amp; bc &amp; c &amp; cd \\\\ ad &amp; bd &amp; cd &amp; d \\end{bmatrix} \\\\ \\cr \\mathbf{A} &amp;= \\mathbf{A}&#39; \\end{align} \\] Matrix Transpose The transpose of a matrix is an operator which flips a matrix over its diagonal. That is, it switches the row and column indices of the matrix \\(A\\) by producing another matrix, often denoted by \\(A&#39;\\) (or \\(A^{T}\\)). Graphical Depiction of a Matrix Transpose https://leetcode.com/problems/transpose-matrix/ 4.1.3 Diagonal A diagonal matrix is a special case of a square symmetric matrix in which there are values along the diagonal, but zeros elsewhere: \\[ \\begin{align} \\underset{n \\times n}{\\mathbf{A}} &amp;= \\begin{bmatrix} a &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; b &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; c &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; d \\end{bmatrix} \\\\ \\cr \\mathbf{A} &amp;= \\mathbf{A}&#39; \\end{align} \\] 4.1.4 Identity An identity matrix is a special case of a diagonal matrix in which the elements of the diagonal are all 1: \\[ \\underset{n \\times n}{\\mathbf{I}} = \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix} \\] Any matrix multiplied by an identity matrix is unchanged. "],["4.2-operations-on-matrices.html", "4.2 Operations on Matrices", " 4.2 Operations on Matrices 4.2.1 Matrix Transpose As stated earlier the transpose of a matrix is an operator which flips a matrix over its diagonal. That is, it switches the row and column indices of the matrix \\(A\\) by producing another matrix, often denoted by \\(A&#39;\\) (or \\(A^{T}\\)). Some useful properties of the matrix transpose include: \\[ (\\mathbf{A + B})&#39; = \\mathbf{A&#39; + B&#39;}\\\\ (c\\mathbf{A&#39;}) = c(\\mathbf{A&#39;}) = (\\mathbf{A&#39;})c \\\\ (\\mathbf{A&#39;B}) = \\mathbf{B&#39;A}\\\\ (\\mathbf{AB})&#39; = \\mathbf{B&#39;A&#39;}\\\\ (\\mathbf{A&#39;})&#39; = \\mathbf{A} \\] Graphical Depiction of a Matrix Transpose https://leetcode.com/problems/transpose-matrix/ 4.2.2 Matrix Trace The trace of a square matrix is the sum of elements along the diagonal. The trace is only defined for a square matrix. For an \\(n \\times n\\) matrix the trace is defined as follows: \\[ tr(\\mathbf{A}) = \\sum_{i=1}^{n}{a_{ii}} = a_{11} + a_{22} + ... + a_{nn} \\] Graphical Depiction of a Matrix Trace Some useful properties of the matrix trace include: \\[ tr(\\mathbf{A + B}) = tr(\\mathbf{A}) + tr(\\mathbf{B})\\\\ tr(c\\mathbf{A}) = c(tr(\\mathbf{A})) \\\\ tr(\\mathbf{A}) = tr(\\mathbf{A&#39;})\\\\ tr(\\mathbf{AB}) = tr(\\mathbf{BA})\\\\ tr(\\mathbf{ABC}) = tr(\\mathbf{CAB})=tr(\\mathbf{BCA}) \\] 4.2.3 Addition For addition, matrices must be of the same order. Addition of two matrices is accomplished by adding corresponding elements, \\(c_{ij}=a_{ij}+b_{ij}\\) \\[ \\mathbf{A} = \\begin{bmatrix} 10 &amp; 5 \\\\ 9 &amp; 1 \\end{bmatrix} , \\enspace \\mathbf{B} = \\begin{bmatrix} 2 &amp; 1 \\\\ 20 &amp; 0 \\end{bmatrix}, \\enspace \\textrm{then } \\mathbf{A}+\\mathbf{B}= \\begin{bmatrix} 12 &amp; 6 \\\\ 29 &amp; 1 \\end{bmatrix} \\] Matrix addition is commutative (gives the same result whatever the order of the quantities involved), \\[ \\mathbf{A + B} = \\mathbf{B + A} \\] and associative (gives the same result whatever grouping their is, as long as order remains the same), \\[ \\mathbf{A + (B + C)} = \\mathbf{(A + B) + C} \\] and \\[ \\mathbf{A + (-B)} = \\mathbf{(A - B)}. \\] 4.2.4 Subtraction Like addition, subtraction requires matrices of the same order. Elements in the difference matrix are given by the algebraic difference between corresponding elements in matrices being subtracted: \\[ \\mathbf{A} = \\begin{bmatrix} 10 &amp; 5 \\\\ 9 &amp; 1 \\end{bmatrix} , \\enspace \\mathbf{B} = \\begin{bmatrix} 2 &amp; 1 \\\\ 20 &amp; 0 \\end{bmatrix}, \\enspace \\textrm{then } \\mathbf{A}-\\mathbf{B}= \\begin{bmatrix} 8 &amp; 4 \\\\ -11 &amp; 1 \\end{bmatrix} \\] 4.2.5 Matrix Multiplication Three useful rules to keep in mind regarding matrix multiplication: Only matrices of the form \\((m \\times n) * (n \\times p)\\) are conformable for multiplication. The number of columns in the premultiplier must equal the number of rows in the post multiplier. The product matrix will have the following order: \\(\\mathbf{A}_{m\\times n} \\mathbf{B}_{n\\times p} = \\mathbf{C}_{m \\times p}\\). Graphical Depiction of Rules 1 and 2 The element \\(c_{ij}\\) in the product matrix is the result of multiplying row \\(i\\) of the premultiplier matrix, and row \\(j\\) of the post multiplier matrix (e.g. (\\(c_{ij}=a_{i1}b_{1j} + a_{i2}b_{2j} + a_{i3}b_{3j}\\))). Graphical Depiction of Rule 3 https://code.kx.com/q/ref/mmu/ Matrix multiplication is associative (i.e. rearranging the parentheses in an expression will not change the result). That is, \\[ \\mathbf{(AB)C} = \\mathbf{A(BC)} \\] and is distributive with respect to addition, \\[ \\mathbf{A(B+C)} = \\mathbf{AB + AC} \\\\ \\mathbf{(B+C)A} = \\mathbf{BA + CA} \\\\ \\] If \\(c\\) is a scalar, then \\[ c(\\mathbf{AB})=c(\\mathbf{A})\\mathbf{B}=\\mathbf{A}(c\\mathbf{B})=(\\mathbf{AB})c \\] or equivalently, \\[ \\mathbf{A} = \\begin{bmatrix} 10 &amp; 5 \\\\ 9 &amp; 1 \\end{bmatrix}, \\enspace k=2, \\enspace k\\mathbf{A} = \\begin{bmatrix} 20 &amp; 10 \\\\ 18 &amp; 2 \\end{bmatrix}. \\] In general, matrices that can be multiplied are called ‘compatible’ or ‘comformable.’ Matrices in which the inner dimensions (i.e., columns of \\(\\mathbf{A}\\), rows of \\(\\mathbf{B}\\)) do not match are called ‘incompatible’ or ‘non-conformable.’ These cannot be multiplied. 4.2.6 Matrix Division Division is not defined for matrix operations, but may be accomplished by multiplication by the inverse matrix. In algebra, the reciprocal of a scalar is, by definition, the scalar raised to the minus one power (e.g. \\(5^{-1} = 1/5\\)), and equations may be solved by multiplication by reciprocals. For example: \\[ 5^{-1} = 1/5\\\\ 5x=35\\\\ 5^{-1}(5x)=5^{-1}(35)\\\\ x = 7 \\] Now consider the following equation where the vector \\(\\mathbf{x}\\) is unknown, \\[ \\mathbf{A}_{p \\times p} \\mathbf{x}_{p \\times 1} = \\mathbf{b}_{p \\times 1} \\] Each element in the column vector \\(\\mathbf{x}\\) is unknown and the solution involves solving a set of simultaneous equations for the unknown element of \\(\\mathbf{x}\\), \\[ a_{11}x_{1} + a_{12}x_{2} + \\dots + a_{1p}x_{p} = b1 \\\\ a_{21}x_{1} + a_{22}x_{2} + \\dots + a_{2p}x_{p} = b2 \\\\ \\vdots \\\\ a_{p1}x_{1} + a_{p2}x_{2} + \\dots + a_{pp}x_{p} = bp \\] A solution analogous to the scalar equations above would give the following solution for the elements of the vector \\(\\mathbf{x}\\): \\[ \\mathbf{A}_{p \\times p} \\mathbf{x}_{p \\times 1} = \\mathbf{b}_{p \\times 1} \\\\ \\mathbf{A}^{-1}_{p \\times p}\\mathbf{A}_{p \\times p} \\mathbf{x}_{p \\times 1} = \\mathbf{A}^{-1}_{p \\times p}\\mathbf{b}_{p \\times 1} \\\\ \\mathbf{I}_{p \\times p}\\mathbf{x}_{p \\times 1} = \\mathbf{A}^{-1}_{p \\times p}\\mathbf{b}_{p \\times 1} \\\\ \\mathbf{x}_{p \\times 1} = \\mathbf{A}^{-1}_{p \\times p}\\mathbf{b}_{p \\times 1} \\] The inverse of a matrix must satisfy the following properties: \\[ \\mathbf{AA^{-1}} = \\mathbf{A^{-1}A} = \\mathbf{I} \\] where \\(I\\) is the identity matrix with 1’s along the diagonal and 0’s elsewhere. So, why is division undefined for matrices. Here is a quick example. Suppose, \\(\\mathbf{A}\\) is a matrix and \\(\\mathbf{B}\\) is the inverse of \\(\\mathbf{A}\\), such that \\[ \\mathbf{AB} = \\mathbf{BA} = \\mathbf{I} \\] Now, let \\[ \\mathbf{A} = \\begin{bmatrix} 1 &amp; 2 \\\\ 1 &amp; 2 \\end{bmatrix} , \\enspace \\mathbf{B} = \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\end{bmatrix}, \\] Then, \\[ \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\end{bmatrix} \\begin{bmatrix} 1 &amp; 2 \\\\ 1 &amp; 2 \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 0\\\\ 0 &amp; 1 \\end{bmatrix}. \\] This means that \\(a+b=1\\) and \\(2a+2b = 0\\), which is a contradiction, suggesting A does not have an inverse. "],["4.3-references-1.html", "4.3 References", " 4.3 References "],["5-chapter-5.html", "Chapter 5 Ordinary Least Squares", " Chapter 5 Ordinary Least Squares In Chapter 5 we will briefly review the assumptions and properties of Ordinary Least Squares (OLS) regression, a cornerstone method that supports many of the other methods we will consider. We will present the regression model in both scalar and matrix forms to facilitate the material to follow. "],["5.1-linear-regression-model.html", "5.1 Linear Regression Model", " 5.1 Linear Regression Model Generally, the regression model is written as \\[ y_{i} = \\beta_0 + \\beta_1x_{1i} + \\beta_2x_{2i} + ... + \\beta_qx_{qi} + \\epsilon_{i} \\] where \\(y_{i}\\) is the value of the outcome variable for individual \\(i\\) \\(\\beta_0\\) is an intercept parameter, the expected value of \\(y_i\\) when the predictor variables are all \\(0\\) \\(\\beta_q\\) is a regression parameter indicating the relation between \\(x_{qi}\\) and the outcome variable, \\(y_i\\) \\(\\epsilon_{i}\\) are errors or disturbances "],["5.2-ordinary-least-squares-ols.html", "5.2 Ordinary Least Squares (OLS)", " 5.2 Ordinary Least Squares (OLS) The Ordinary Least Squares (OLS) is one of the most common estimators of the linear regression model. What assumptions do we make with OLS? Why should we care? How well me meet the assumptions of OLS determines (1) the accuracy of OLS coefficients, and (2) the accuracy of our inferences and substantive hypothesis tests. "],["5.3-assumptions-of-ols.html", "5.3 Assumptions of OLS", " 5.3 Assumptions of OLS The assumptions of OLS are as follows: \\(\\mathbb{E}(\\epsilon_{i}) = 0\\) \\(\\mathbb{E}(\\epsilon_{i}^2) = \\sigma^2\\) for all \\(i\\) (homoscedasticity) \\(\\mathbb{E}(\\epsilon_{i}\\epsilon_{j}) = 0\\) for all \\(i \\neq j\\) No perfect collinearity among \\(x\\) variables \\(\\mathbb{C}(\\epsilon_{i},x_{qi}) = 0\\) for all \\(i\\) and \\(k\\) Let’s discuss each assumption in more detail. 5.3.1 Assumption 1. \\(\\mathbb{E}(\\epsilon_{i}) = 0\\) Note that \\(\\mathbb{E}()\\) is the expectation operator. The expected value is an “average” of whatever is inside the parentheses. This assumption states that, on average, the error for the \\(ith\\) observation is zero. Here “for all \\(i\\)” means the same is true for all cases. 5.3.2 Assumption 2. Homoscedasticity In statistics, a vector of random variables is heteroscedastic if the variability of the random disturbance is different across elements of the vector, here our \\(\\mathbf{X}\\)s. The errors or disturbances in our model are homoskedastic if the variance of \\(\\epsilon _{i}\\) is a constant (e.g. \\(\\sigma ^{2}\\)), otherwise, they are heteroskedastic. Graphical Depiction of Homoskedasticity and Heteroskedasticity https://en.wikipedia.org/wiki/Homoscedasticity 5.3.3 3. \\(\\mathbb{E}(\\epsilon_{i}\\epsilon_{j}) = 0\\) Assumption 3 is sometimes referred to as the autocorrelation assumption. This assumption states that the error terms of different observations should not be correlated with each other. For example, when we have time series data and use lagged variables we may want to examine residuals for the possibility of autocorrelation. Graphical Depiction of Positive and Negative Autocorrelation 5.3.4 4. No Perfect Collinearity Perfect collinearity occurs when one variable is a perfect linear function of any other explanatory variable. If perfect collinearity is found among the \\(\\mathbf{X}\\)s then \\(\\mathbf{(X&#39;X)}\\) has no inverse and OLS estimation fails. Perfect collinearity is unlikely except for programming mistakes such as dummy coding all the values in a nominal variable. 5.3.5 5. \\(\\mathbb{C}(\\epsilon_{i},x_{ki}) = 0\\) Note that \\(\\mathbb{C}()\\) is the covariance operator. Assumption five states that that the error of our equation is uncorrelated with all the \\(\\mathbf{X}\\)s. This is often referred to as an endogeneity assumption. This can be a confusing assumption because by definition the residuals \\(\\hat{e_i}\\) are uncorrelated with the \\(\\mathbf{X}\\)s. Here, however, we are concerned with the true errors \\(\\epsilon_i\\). Unfortunately, there are a variety of conditions that lead to \\(\\mathbb{C}(\\epsilon_{i},x_{qi}) \\neq 0\\) in applied contexts. Graphical Depiction of Sources of Endogeneity Figure 2 from Bollen (2012) If we meet these assumptions what large sample properties can we expect? "],["5.4-properties-of-the-ols-estimator.html", "5.4 Properties of the OLS Estimator", " 5.4 Properties of the OLS Estimator If assumptions (1) to (5) hold, then the OLS estimator \\(\\boldsymbol{\\hat{\\beta}}\\) is: A consistent estimator of \\(\\boldsymbol{{\\beta}}\\) Asymptotically normally distributed Having a variance of \\(\\mathbb{V}(\\boldsymbol{\\hat{\\beta}}) = \\sigma^2(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\) Notice that we did not assume normality of \\(\\epsilon_{i},y_{i}\\) or \\(x_{i}\\). Let’s discuss each of these properties in a little bit more detail. 5.4.1 1. Consistentcy of \\(\\boldsymbol{{\\beta}}\\) \\(\\boldsymbol{\\hat{\\beta}}\\) is the OLS estimator of \\(\\boldsymbol{{\\beta}}\\). A consistent estimator is one for which, as the sample size (\\(n\\)) increases, the estimate converges in probability to the value that the estimator is designed to estimate. This is often stated as \\(plim(\\boldsymbol{\\hat{\\beta}})=\\boldsymbol{{\\beta}}\\). Stated differently, as the sample size grows, the OLS coefficients converge to the true coefficients. 5.4.2 2. Asymptotic Normality Asymptotic normality is another property of the OLS estimator when all assumptions are met. “Asymptotic” refers to how an estimator behaves as the sample size tends to infinity. “Normality” refers to the normal distribution, so an estimator that is asymptotically normal will have an approximately normal distribution as the sample size gets larger. 5.4.3 Variance of \\(\\hat{\\beta}\\) Having a variance of \\(\\mathbb{V}(\\boldsymbol{\\hat{\\beta}}) = \\sigma^2(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\) is another property of the OLS estimator when the previously stated assumptions are met. This means, for example, we can estimate the standard errors from the main diagonal of \\(\\sigma^2(\\mathbf{X}&#39;\\mathbf{X})^{-1}\\) and perform significance testing based on this variance. "],["5.5-failure-to-meet-assumptions.html", "5.5 Failure to Meet Assumptions", " 5.5 Failure to Meet Assumptions It is worth thinking about the consequences of not meeting these assumptions. 5.5.1 Failure of Assumption 1. Assumption 1 states \\(\\mathbb{E}(\\epsilon_{i}) = 0\\). This assumption states that, on average, the error for the \\(ith\\) observation is zero. If instead, \\(\\mathbb{E}(\\epsilon_{i}) = c\\) and \\(c \\neq 0\\), and all other assumptions hold, then only the intercept term is biased. Other coefficients OK. 5.5.2 Failure of Assumption 2 or 3. Assumptions 2 and 3 are the homoskedasticity and no autocorrelation assumption, respectively. If we violate (2) or (3), but all other assumptions hold, (1) variance of \\(\\hat{\\beta}\\) is no longer dependable, (2) SEs possibly inaccurate, and (3) significance tests are possibly inaccurate. However, importantly, \\(\\hat{\\beta}\\) is still an unbiased and consistent estimator. 5.5.3 Failure of Assumption 5. Assumption (5) states that that the error of our equation is uncorrelated with all the \\(\\mathbf{X}\\)s. If this assumption fails, while others hold, OLS is no longer a consistent estimator. "],["5.6-regression-and-matrix-notation.html", "5.6 Regression and Matrix Notation", " 5.6 Regression and Matrix Notation Now that we have reviewed the assumptions of OLS, let’s return to the linear regression model and translate it into a matrix form. 5.6.1 An Intercept-Only Model First, let’s take a simpler form of the model, an intercept-only model where \\[ y_i = \\beta_0 1_{i} + \\epsilon_i.\\] Note that we have made the “silent” 1 explicit. This will become important later (e.g., when fitting growth models). It is worthwhile to look at regression model without predictors to understand what it can tell us about the nature of the intercept (or constant). So here we have no predictors, what is \\(\\beta_0\\)? Here, \\(\\beta_0\\) is the mean of the response variable, and we can show this with some algebra, \\[\\mathbb{E}(y_i)=\\mathbb{E}(\\beta_0 1_{i} + \\epsilon_i)=\\beta_0 1_{i} +\\mathbb{E}( \\epsilon_i)=\\beta_0\\] where \\(\\mathbb{E}( \\epsilon_i)=0\\) (Assumption 1). 5.6.2 Intercept-Only Model in Matrix Form Translating into matrix form, \\(y_i\\) can be written as an \\(N\\) x 1 matrix (a column vector). More specifically, for \\(i = 1\\) to \\(N\\) individuals, \\[ y_i = \\left[ \\begin{array}{c} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N \\end{array} \\right] = \\boldsymbol{Y}\\]. (Remember, matrices are often designated as bold capital letters) Doing the same for all the other parts of the model, we get \\[ \\left[ \\begin{array}{c} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N \\end{array} \\right] = \\left[ \\begin{array}{c} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{array} \\right] [\\beta_0] + \\left[ \\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_N \\end{array} \\right]\\] Note that we have taken care that each matrix is of an order that will allow for matrix multiplication. 5.6.3 Simple Regression in Matrix Form Now, let’s expand our regression model by adding a predictor \\(x_{1i}\\). Our model becomes \\[ y_i = \\beta_0 + \\beta_1x_{1i} + \\epsilon_i \\] Written out explicitly in matrix form, the model is \\[ \\left[ \\begin{array}{c} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N \\end{array} \\right] = \\left[ \\begin{array}{cc} 1, x_{11}\\\\ 1, x_{12}\\\\ \\vdots \\\\ 1, x_{1N}\\end{array} \\right] \\left[ \\begin{array}{c}\\beta_0\\\\ \\beta_1\\end{array}\\right] + \\left[ \\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_N \\end{array} \\right]\\] 5.6.4 Multiple Regression in Matrix Form Finally, extending the model to the general case with \\(q\\) predictor variables, we have \\[ y_i = \\beta_0 + \\beta_1x_{1i} + \\beta_2x_{2i} + ... + \\beta_qx_{qi} + \\epsilon_i \\] which is written out in matrix form as \\[ \\underbrace{\\left[ \\begin{array}{c} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N \\end{array} \\right]}_{N \\times 1} = \\underbrace{\\left[ \\begin{array}{cccc} 1, x_{11}, \\ldots, x_{q1}\\\\ 1, x_{12}, \\ldots, x_{q2}\\\\ \\vdots \\\\ 1, x_{1N}, \\ldots, x_{qN}\\end{array} \\right]}_{N \\times (q + 1)} \\underbrace{\\left[ \\begin{array}{c}\\beta_0\\\\ \\beta_1\\\\ \\vdots\\\\ \\beta_q\\end{array}\\right]}_{(q+1) \\times 1} + \\underbrace{\\left[ \\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_N \\end{array} \\right]}_{N \\times 1}\\] Where we have the following elements: \\[ \\boldsymbol{Y} = \\left[ \\begin{array}{c} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N \\end{array} \\right] ;\\boldsymbol{X} = \\left[ \\begin{array}{cccc} 1, x_{11}, \\ldots, x_{q1}\\\\ 1, x_{12}, \\ldots, x_{q2}\\\\ \\vdots \\\\ 1, x_{1N}, \\ldots, x_{qN}\\end{array} \\right]; \\boldsymbol{\\beta} = \\left[ \\begin{array}{c}\\beta_0\\\\ \\beta_1\\\\ \\vdots\\\\ \\beta_q\\end{array}\\right]; \\boldsymbol{\\epsilon} = \\left[ \\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_N \\end{array} \\right] \\] Observe the order of the matrices/vectors. On the right hand side you are matrix multiplying a \\(N \\times (q+1)\\) matrix with a \\((q+1) \\times 1\\) vector. This yields an \\(N \\times 1\\) vector, to which another \\(N \\times 1\\) vector \\(\\boldsymbol{\\epsilon}\\) is added, and this is equal to our outcome vector \\(\\boldsymbol{Y}\\) which is also \\(N \\times 1\\). When we implement this model in R, it will be important to know the portions of the model that are in our data frame, \\(y_i\\) and \\({x_{1}, ..., x_{q}}\\), and to have them structured properly. This will become clear in the examples below. Now that we have the model written out explicitly as matrices, we can easily simplify the notation. In compact matrix notation, the regression model then can be written as \\[ \\boldsymbol{Y} = \\boldsymbol{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon} \\] "],["5.7-solving-the-regression-equation.html", "5.7 Solving the Regression Equation", " 5.7 Solving the Regression Equation In practice, we would like to know the contents of (i.e., solve for) \\(\\boldsymbol{\\beta}\\). Assuming the model is correct, the expected value of \\(\\boldsymbol{\\epsilon}\\) is 0, therefore, \\[ \\boldsymbol{Y} = \\boldsymbol{X}\\boldsymbol{\\beta}\\] Then we just need to solve for \\(\\boldsymbol{\\beta}\\). We can think back about some of the matrix operations we discussed earlier. 5.7.1 Matrix Multiplication and Transpose Our goal is to isolate \\(\\boldsymbol{\\beta}\\). One initial idea might be to multiple each side of the equation by \\(\\mathbf{X}^{-1}\\) in an attempt to remove \\(\\mathbf{X}\\) from the right hand side, and isolate \\(\\boldsymbol{\\beta}\\). Why won’t this work? Instead, let’s pre-multiply each side of the equation by \\(\\boldsymbol{X&#39;}\\). This would give us \\[ \\boldsymbol{X&#39;}\\boldsymbol{Y} = \\boldsymbol{X&#39;}\\boldsymbol{X}\\boldsymbol{\\beta} \\] This gets us a quantity, \\(\\left(\\boldsymbol{X&#39;}\\boldsymbol{X}\\right)\\), a square matrix containing information about the relations among the \\(\\mathbf{x}\\)s. 5.7.2 Matrix Inverse Now, since \\(\\boldsymbol{X&#39;}\\boldsymbol{X}\\) is a square matrix and presumabely has an inverse (e.g. no perfect collinearity), we can premultiply both sides by \\(\\left(\\boldsymbol{X&#39;}\\boldsymbol{X}\\right)^{-1}\\), to obtain \\[ \\left(\\boldsymbol{X&#39;}\\boldsymbol{X}\\right)^{-1} \\left( \\boldsymbol{X&#39;}\\boldsymbol{Y}\\right) = \\left(\\boldsymbol{X&#39;}\\boldsymbol{X}\\right)^{-1} (\\boldsymbol{X&#39;}\\boldsymbol{X}) \\boldsymbol{\\beta} \\] Remembering our assumptions that a matrix multiplied by its inverse equals the identity matrix, \\((\\boldsymbol{X&#39;}\\boldsymbol{X})^{-1} (\\boldsymbol{X&#39;}\\boldsymbol{X})=\\mathbf{I}\\) the equation simplifies to \\[ \\left(\\boldsymbol{X&#39;}\\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X&#39;}\\boldsymbol{Y}\\right) = \\boldsymbol{I}\\boldsymbol{\\beta} \\] or more succinctly \\[ \\left(\\boldsymbol{X&#39;}\\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X&#39;}\\boldsymbol{Y}\\right) = \\boldsymbol{\\beta} \\] We’ve now isolated the unknowns, \\(\\boldsymbol{\\beta}\\) onto one side of the equation and figured out how to use matrix algebra to obtain the regression coefficients. Quite literally, this algebra is what allows for estimation of the parameters when fitting a regression model to data. We will now work through some practical examples - staying aware that this kind of matrix algebra is being done in the background. "],["5.8-the-linear-probability-model.html", "5.8 The Linear Probability Model", " 5.8 The Linear Probability Model While we are discussing the assumptions of OLS it is worth pausing to consider a model for dichotomous outcomes: the linear probability model (LPM). In the LPM we don’t do anything fancy with a binary outcome variable. Instead, we simply apply OLS as we would with a continuous out come variable. Since we aren’t considering the normality of our outcome you might be curious how our assumptions would hold. Remembering the assumptions of OLS: \\(\\mathbb{E}(\\epsilon_{i}) = 0\\) \\(\\mathbb{E}(\\epsilon_{i}^2) = \\sigma^2\\) for all \\(i\\) (homoscedasticity) \\(\\mathbb{E}(\\epsilon_{i}\\epsilon_{j}) = 0\\) for all \\(i \\neq j\\) No perfect collinearity among \\(x\\) variables \\(\\mathbb{C}(\\epsilon_{i},x_{qi}) = 0\\) for all \\(i\\) and \\(k\\) Which assumptions are needed for consistency and asymptotic unbiasedness? No perfect collinearity among \\(x\\) variables Errors uncorrelated with all \\(x\\) variables. In regard to (4) having a dependent variable valued at \\(0,1\\) does not cause any problems. In regard to (5), again no, nothing about a dichotomous outcome violates this assumption. Therefore, in this model \\(\\mathbf{\\hat\\beta}\\) is still consistent and asymptotically unbiased. What about the remaining assumptions? Homoscedasticity Here, a dichotomous outcome does inherently violate the assumption of homoskedasticity. Why is this case? It can be shown that \\(\\mathbb{V}(\\epsilon_{i})\\) now directly depends on the value of \\(x\\) that is taken. If the \\(ith\\) person has one set of values for the \\(x\\) variables, and another individual \\(j\\) has another set, the estimates of \\(\\mathbb{V}(\\epsilon_{i})\\) and \\(\\mathbb{V}(\\epsilon_{j})\\) will differ. This can be seem by looking at the variance of a Bernoulli random variable. This means estimate of variance of \\(\\mathbf{\\hat\\beta}\\) is no longer reliable, SEs and significance tests possibly inaccurate. However, this could be addressed using robust standard errors. It is common to look at plots of predicted values vs residuals to diagnose heteroskedasticity. Generally one would like to see a random blob of points without any discernible pattern. Here is an example of what that plot might look like for an LPM model. Each line represents a different outcome, \\(y=0\\), or \\(y=1\\). 5.8.1 Advantages of the LPM Simplicity. Regression coefficients give impact of \\(x\\) on \\(P(y_{i}=1|x)\\). Effect same regardless of value of \\(x\\) or values of other \\(x\\)s. Can extend with traditional methods easily (interactions, quadratic terms). 5.8.2 Disadvantages of the LPM Functional form unlikely accurate at extreme lows &amp; highs Binary dependent variable creates heteroscedasticity Some regression diagnostics assume homoscedastic error Predicted probabilities not restricted to 0 to 1 range "],["6-chapter-6.html", "Chapter 6 Statistical Control", " Chapter 6 Statistical Control In Chapter 6 we will discuss statistical control. Introduction to DAGs Statistical Control Done Right Statistical Control Gone Wrong Much of this presentation was drawn directly from Rohrer (2018) (https://journals.sagepub.com/doi/full/10.1177/2515245917745629). "],["6.1-statistical-control.html", "6.1 Statistical Control", " 6.1 Statistical Control Randomized experiments often considered the gold standard in scientific research.However, it is often infeasible, difficult or impossible to manipulate the putative effect of interest: cannot randomly resettle individuals into different strata of society cannot assign couples to stay married or get a divorce cannot randomize children to different levels of adversity Researchers have employed a number of different strategies in response to the limitations of observational data: surrogate interventions item the real-life cause of interest cannot be manipulated, often a proxy can be randomized in the lab. avoiding causal language item write about associations and relationships despite substantive question about causal effects statistical control include control variables in analysis but choice of variables often difficult and statistical control itself insufficient "],["6.2-directed-acyclic-graphs-dags.html", "6.2 Directed Acyclic Graphs (DAGs)", " 6.2 Directed Acyclic Graphs (DAGs) Directed acyclic graphs (DAGs) are another approach that can be used to examine causal inferences from observational data. They were developed primarily by the computer scientist Judea Pearl DAGs provide a visual representation of causal assumptions. Some overlap with structural equation models (SEMs). Importantly, DAGs can provide insights on What variables should be controlled for? What variables should not be controlled for? In what situations might control worsen causal inference? 6.2.1 Introduction to DAGs Below is a simple DAG depicting a model in which the relationship between maltreatment externalizing is confounded by a common cause, income. DAGs consist of nodes (variables) and arrows (edges) between these nodes, which reflect causal relationships. It is assumed that manipulation of a variable at which an arrow begins (e.g., a manipulation of child maltreatment with income held constant) would change the variable at the end of the arrow (e.g., externalizing). 6.2.2 Introduction to DAGs: Paths From these two simple building blocks—nodes and arrows—one can visualize more complex situations and trace paths from variable to variable: Paths A simple path leads just from one node to another (income → stress). Paths can also contain multiple nodes: income → stress → child maltreatment Paths can also travel against the direction indicated by arrows child maltreatment ← stress ← income → externalizing 6.2.3 Introduction to DAGs: Chains Chains Chains have the structure A → B → C. Chains can transmit an association between the beginning and end nodes. If income causally affects child maltreatment, and child maltreatment causally affects externalizing, then income and externalizing can be correlated. 6.2.4 Introduction to DAGs: Descendants and Ancestors Chains: Descendants and Ancestors Along a chain, variables that are directly or indirectly causally affected by a certain variable are called its descendants externalizing is a descendant of child maltreatment Variables that directly or indirectly affect a certain variable are considered its ancestors. income is an ancestor of support, child maltreatment and internalizing 6.2.5 Introduction to DAGs: Forks Forks Forks have the structure A ← B → C. A fork can transmit an association, but it is not causal. In isolation, this fork indicates that child maltreatment and externalizing may be correlated because they share a common cause, income. Forks are the causal structure most relevant for the phenomenon of confounding. 6.2.6 Introduction to DAGs: Inverted Forks Inverted Forks Inverted forks have the structure A → B ← C. An inverted fork does not transmit an association. In isolation, If child maltreatment and income both affect externalizing, this does not imply that they are in any way correlated. Inverted forks are relevant to the problem of collider bias. 6.2.7 Introduction to DAGs: Acyclicity Acyclicity DAGs are acyclic because they do not allow for cyclic paths in which variables become their own ancestors. a variable cannot causally affect itself Developmental systems often contain feedback loops and reciprocal relationships. Often feedback loops can be modeled in a DAG by taking the temporal order into account and adding nodes for repeated measures. "],["6.3-statistical-control-done-right.html", "6.3 Statistical Control Done Right", " 6.3 Statistical Control Done Right The central problem of observational data is confounding: the presence of a common cause that lurks behind the potential cause of interest and the outcome of interest. A confounding influence can introduce what is often called a spurious correlation, which ought not to be confused with a causal effect. The extraordinarily influence of randomized experiments in testing causal inferences is based on the simple fact that if the independent variable is randomly assigned—for example, by the flip of a coin—by design it cannot share a common cause with the outcome. How can a DAG be used to figure out how to remove all such noncausal associations so that only the true causal effect remains? 6.3.1 Building a DAG To derive a valid causal conclusion, one must ensure the DAG includes everything that is relevant to the causal effect of interest. What is missing If we want to derive a valid causal conclusion, we need to build a causal DAG that is complete because it includes all common causes of all pairs of variables that are already included in the DAG (Spirtes, Glymour, &amp; Scheines, 2000). That is, any additional variable that either directly or indirectly causally affects at least two variables already included in the DAG should be included. 6.3.2 Building a DAG: Back-Door Paths After a DAG is built, back-door paths can be discerned. Back-door paths are all paths that start with an arrow pointing to the independent variable and end with an arrow pointing to the dependent variable. If we plan to investigate the causal relationship between child maltreatment and internalizing what are the back-door paths in our example DAG? Back-Door Paths child maltreatment ← support ← income → externalizing child maltreatment ← income → externalizing Back-Door Problems Back-door paths are problematic whenever they transmit an association. In this case, both backdoor paths consist of only chains and forks, thus, these two back-door paths are open, and they can transmit a spurious association. Back-Door Solutions The zero-order correlation between child maltreatment and externalizing is a mix of the true causal effect (child maltreatment → externalizing) of interest plus any noncausal association transmitted by the two back-door paths. To remove the undesirable noncausal association, we must block the two back-door paths. Blocking Back-Door Paths The purpose of third-variable control is to block open back-door paths. If all back-door paths between the independent and dependent variables can be blocked, then the causal effect connecting the independent and dependent variables can be identified. Such a causal effect would be considered identifiable, always under the assumption that the DAG captures the true underlying causal web. A back-door path can be blocked by “cutting” the transmission of association at any point in the path by statistically controlling a node. What variables would we want to control for to identify the causal effect of child maltreatment on externalizing? "],["6.4-statistical-control-gone-wrong.html", "6.4 Statistical Control Gone Wrong", " 6.4 Statistical Control Gone Wrong In certain fields, it has become common practice to include as many covariates as possible. Unfortunately, it is not true that simply adding more covariates will improve the estimate of a causal effect. There are two types of variables that researchers should not control for without taking into account potential negative side effects: colliders and mediators. Whereas confounders causally affect the independent variable of interest, colliders and mediators are causally affected by the independent variable. 6.4.1 Collider Bias A collider for a certain pair of variables is any variable that is causally influenced by both of them. Controlling for, or conditioning analysis on, such a variable (or any of its descendants) can introduce a spurious (i.e., noncausal) association between its causes. In DAG terminology, a collider is the variable in the middle of an inverted fork, for example, variable B in A → B ← C. The collider variable normally blocks the path, but when one controls for it, a spurious association between A and C can arise. 6.4.2 Conditioning on a Collider Thought Experiment Imagine we are interested in the effect of child maltreatment on personality features, such as extroversion. For this thought experiment, let us assume that there is actually no causal effect of child maltreatment on extroversion. To investigate the association, we look at all individuals with substantiated claims of maltreatment via CPS. We find a sizable negative association: those who experienced more maltreatment show less extroversion and vice versa. Suppose we then realize bias might be an issue and conduct a follow-up study on individuals who self-report experiencing maltreatment but do not have CPS involvement. Again we find a sizable negative association. By assessing substantiated and unsubstantiated cases separately, we have stratified, or conditioned, our analyses by CPS involvement. However, let’s assume that exposure to child maltreatment and extroversion are likely to be associated with CPS contact. In the simplest case, both have a positive effect: With increasing child maltreatment, the likelihood of CPS involvement increases. With increasing extroversion, the likelihood of CPS involvement increases. In our thought experiment, there is no association between child maltreatment and extroversion if all individuals—with and without CPS involvement—are considered simultaneously without statistical control (of the collider or any descendants) Collider Bias The spurious negative correlation emerges only when the joint outcome of the two variables of interest is controlled for. This observation generalizes to similar situations in which selection into a group is based on multiple desirable features: Group membership is a collider variable, and conditioning analysis on it will introduce or exaggerate trade-offs between desirable features. 6.4.3 Avoiding Collider Bias Avoiding collider bias requires two steps. One must be aware of the collider variable, and this may entail using a DAG to identify colliders that exist between and independent variable and outcome. One must be able to run analyses that are not conditional on the collider. This entails not controlling for the collider when examining the main effect of interest. Thought Experiment In our thought experiment, we must include individuals involved and not involved with CPS Outside of thought experiments, one might often be unaware of collider variables or collect data in such a way that collider bias is built in. 6.4.4 Variations on Collider Bias: Nonresponse Bias Nonresponse bias occurs if, for example, a researcher analyzes only completed questionnaires, and the variables of interest are associated with questionnaire completion. Assume that we are interested in the association between grit and intelligence, and our assessment ends up being very burdensome. Both grit and intelligence make it easier for respondents to push through and complete the assessment. Questionnaire completion is thus a collider between grit and intelligence. Although there might be no association between grit and intelligence in the population, we might find a spurious negative association if we analyze only completed questionnaires. completers low on intelligence and high levels of grit completers low on grit and high on intellgence noncompleter low on both variables less likely to finish 6.4.5 Controlling for Mediators Overcontrol bias is another example of statistical control hurting instead of helping: If mediating variables are controlled for, the very processes of interest are controlled away. Consider our previous example, now slightly modified: "],["7-chapter-7.html", "Chapter 7 Linear Regression", " Chapter 7 Linear Regression In Chapter 7 we will demonstrate how to estimate the linear regression model in R with an eye towards the longitudinal modeling to follow. "],["7.1-example-data-2.html", "7.1 Example Data", " 7.1 Example Data Chapter 6 make use of the same WISC data used in Chapter 3. Here we again read in, subset, and provide descriptives for the WISC data. We will also add a simulated variable childgrad indicating whether the student graduated highschool. filepath &lt;- &quot;https://quantdev.ssri.psu.edu/sites/qdev/files/wisc3raw.csv&quot; wisc3raw &lt;- read.csv(file=url(filepath),header=TRUE) colnames(wisc3raw) &lt;- c( &quot;id&quot;, &quot;verb1&quot;, &quot;verb2&quot;, &quot;verb4&quot;, &quot;verb6&quot;, &quot;perfo1&quot;, &quot;perfo2&quot;, &quot;perfo4&quot;, &quot;perfo6&quot;, &quot;info1&quot;, &quot;comp1&quot;, &quot;simu1&quot;, &quot;voca1&quot;, &quot;info6&quot;, &quot;comp6&quot;, &quot;simu6&quot;, &quot;voca6&quot;, &quot;daded&quot;, &quot;grad&quot;, &quot;constant&quot; ) var_names_sub &lt;- c( &quot;id&quot;, &quot;verb1&quot;, &quot;verb2&quot;, &quot;verb4&quot;, &quot;verb6&quot;, &quot;perfo1&quot;, &quot;perfo2&quot;, &quot;perfo4&quot;, &quot;perfo6&quot;, &quot;daded&quot;, &quot;grad&quot; ) wiscsub &lt;- wisc3raw[,var_names_sub] set.seed(1234) wiscsub$childgrad &lt;- sample(c(0,1), replace=TRUE, size=nrow(wiscsub)) psych::describe(wiscsub) ## vars n mean sd median trimmed mad min max range skew ## id 1 204 102.50 59.03 102.50 102.50 75.61 1.00 204.00 203.00 0.00 ## verb1 2 204 19.59 5.81 19.34 19.50 5.41 3.33 35.15 31.82 0.13 ## verb2 3 204 25.42 6.11 25.98 25.40 6.57 5.95 39.85 33.90 -0.06 ## verb4 4 204 32.61 7.32 32.82 32.42 7.18 12.60 52.84 40.24 0.23 ## verb6 5 204 43.75 10.67 42.55 43.46 11.30 17.35 72.59 55.24 0.24 ## perfo1 6 204 17.98 8.35 17.66 17.69 8.30 0.00 46.58 46.58 0.35 ## perfo2 7 204 27.69 9.99 26.57 27.34 10.51 7.83 59.58 51.75 0.39 ## perfo4 8 204 39.36 10.27 39.09 39.28 10.04 7.81 75.61 67.80 0.15 ## perfo6 9 204 50.93 12.48 51.76 51.07 13.27 10.26 89.01 78.75 -0.06 ## daded 10 204 10.81 2.70 11.50 11.00 2.97 5.50 18.00 12.50 -0.36 ## grad 11 204 0.23 0.42 0.00 0.16 0.00 0.00 1.00 1.00 1.30 ## childgrad 12 204 0.55 0.50 1.00 0.56 0.00 0.00 1.00 1.00 -0.20 ## kurtosis se ## id -1.22 4.13 ## verb1 -0.05 0.41 ## verb2 -0.34 0.43 ## verb4 -0.08 0.51 ## verb6 -0.36 0.75 ## perfo1 -0.11 0.58 ## perfo2 -0.21 0.70 ## perfo4 0.59 0.72 ## perfo6 0.18 0.87 ## daded 0.01 0.19 ## grad -0.30 0.03 ## childgrad -1.97 0.03 "],["7.2-intercept-only-model.html", "7.2 Intercept-Only Model", " 7.2 Intercept-Only Model For our first example, we focus on verbal ability at Grade 2 as an outcome (verb2 in the data frame wiscsub). Examining the distribution for ‘verb2’. library(&quot;ggplot2&quot;) psych::describe(wiscsub$verb2) ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 204 25.42 6.11 25.98 25.4 6.57 5.95 39.85 33.9 -0.06 -0.34 0.43 ggplot(data=wiscsub, aes(x=verb2)) + geom_histogram(binwidth=2.5, fill=&quot;white&quot;, color=&quot;black&quot;, boundary=0) + xlab(&quot;Verbal Ability Grade 2&quot;) + ylab(&quot;Count&quot;) + xlim(0,50) + theme_classic() 7.2.1 Intercept-Only Equation The simplest model is an intercept only model. In this case, we would fit the model \\[ verb_{2i} = b_0 + \\epsilon_{i}\\] Written out explicitly with the “silent” 1 in it, we get \\[ verb_{2i} = b_01_i + \\epsilon_{i}\\] This is helpful for explicit translation into the R code, specifically the formula within the lm() function. 7.2.2 Intercept-Only Model in R We fit the model using the following code. Note that the code has the ‘1’ predictor variable stated explicitly. model1 &lt;- lm(formula = verb2 ~ 1, data = wiscsub, na.action = na.exclude) summary(model1) ## ## Call: ## lm(formula = verb2 ~ 1, data = wiscsub, na.action = na.exclude) ## ## Residuals: ## Min 1Q Median 3Q Max ## -19.4653 -4.6403 0.5647 4.2822 14.4347 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 25.4153 0.4275 59.45 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.106 on 203 degrees of freedom Note that we used na.exclude instead of na.omit (default); practically speaking na.omit deletes missing data entries while na.exclude just excludes from the analysis.Therefore with na.exclude, in the residuals and fitted values, NA will show up where there were missing values. The output indicates that \\(b_0\\) = 25.4153, and its standard error = 0.4275. The intercept reflects the expected value of the outcome variable when all of the predictor variables (i.e. \\(\\left\\{ x_{1i}, ..., x_{qi}\\right\\}\\)) = 0. So, in the absence of any additional information other than the descriptive statistics of \\(verb_{2i}\\), what is our best guess for a person’s \\(verb_{2i}\\) score? It is the mean of \\(verb_{2i}\\). The regression above confirms this notion; regressing the outcome on a vector of 1s allows us to ‘recover’ the mean. 7.2.3 Intercept as Mean of Outcome Notice we can confirm this finding using matrix algebra, as well. \\[ \\mathbb{E}(verb_{2i}) = \\mathbb{E}(b_01_i + \\epsilon_{i}) \\] From the properties of expectation, we have \\(\\mathbb{E}(X+Y)=\\mathbb{E}(X) + \\mathbb{E}(Y)\\). \\[ \\mathbb{E}(verb_{2i}) = \\mathbb{E}(b_01_i) + \\mathbb{E}(\\epsilon_{i}) \\] Another property of expectation relates to taking the expectation of a constant, \\(\\mathbb{E}(c)=c\\), thus \\(\\mathbb{E}(b_01_i)=b_0\\), implying \\[ \\mathbb{E}(verb_{2i}) = b_01_i + \\mathbb{E}(\\epsilon_{i}). \\] Remembering Assumption 1, \\(\\mathbb{E}(\\epsilon_{i})=0\\), we have \\[ \\mathbb{E}(verb_{2i}) = b_0 \\] We can confirm this by looking at Verbal Scores at Wave 2. mean(wiscsub$verb2) ## [1] 25.41534 7.2.4 Intercept-Only Model \\(R^2\\) Yes - we recovered the mean, but we did not attempt to explain any of the variance. Let’s take a look at the variance explained for Model 1. summary(model1)$r.squared ## [1] 0 It thus makes sense that we get 0 as the R-square. From the properties of variance, we know that \\(\\mathbb{V}(c)=0\\). There is no variability due to the regression model because there are no predictors, only a constant. "],["7.3-simple-linear-regression.html", "7.3 Simple Linear Regression", " 7.3 Simple Linear Regression Let’s build up the model further. For example, we could attempt to explain some of the between-person variance in the Grade 2 verbal score from the Grade 1 verbal scores. But, before we do, let’s examine the distribution of the between-person differences in the Grade 1 verbal scores. ggplot(wiscsub, aes(x=verb1)) + geom_histogram(binwidth=2.5, fill=&quot;white&quot;, color=&quot;black&quot;, boundary=0) + xlab(&quot;Verbal Ability Grade 1&quot;) + ylab(&quot;Count&quot;) + xlim(0,50) + theme_classic() And the relation between the Grade 2 and Grade 1 verbal ability scores. ggplot(wiscsub, aes(x=verb1, y = verb2)) + geom_point() + stat_ellipse(color=&quot;blue&quot;, alpha=.7) + xlab(&quot;Verbal Ability Grade 1&quot;) + ylab(&quot;Verbal Ability Grade 2&quot;) + ylim(0,45) + xlim(0,45) + theme_classic() 7.3.1 Regression Equation and Model Fitting Our regression model becomes \\[ verb_{2i} = b_01_i + b_1verb_{1i} + \\epsilon_{i}\\] model2 &lt;- lm(verb2 ~ 1 + verb1, data = wiscsub, na.action = na.exclude) summary(model2) ## ## Call: ## lm(formula = verb2 ~ 1 + verb1, data = wiscsub, na.action = na.exclude) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.5305 -3.0362 0.2526 2.7147 12.5020 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.62965 1.05164 10.11 &lt;2e-16 *** ## verb1 0.75495 0.05149 14.66 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.261 on 202 degrees of freedom ## Multiple R-squared: 0.5156, Adjusted R-squared: 0.5132 ## F-statistic: 215 on 1 and 202 DF, p-value: &lt; 2.2e-16 7.3.2 Path Diagram We might also be interested in a graphical depiction of our model. This can be accomplished with the semPaths package. semPlot::semPaths(model2, what = &quot;paths&quot;) 7.3.3 Interpreting Model Parameters How do we interpret the parameters here? The intercept, \\(b_0\\), is the expected value for the outcome variable when all of the predictor variables equal zero. So, we would expect a child to have a Grade 2 verbal score of 10.62965 if they have a Grade 1 verbal score of 0. The slope, \\(b_1\\) is the expected difference in the outcome variable for each 1-unit difference in the predictor variable. So, across children, for each 1-point difference in a child’s Grade 1 verbal score, we would expect a 0.75 point difference in the Grade 2 verbal score. 7.3.4 Plotting Regression Line We can plot the relation between ‘verb1’ and ‘verb2’, and include the predicted line from the analysis. ggplot(data=wiscsub, aes(x=verb1,y=verb2)) + geom_point(size = 2, shape=19) + geom_smooth(method=lm,se=TRUE,fullrange=TRUE,colour=&quot;red&quot;, size=2) + labs(x= &quot;Verbal Ability Grade 1&quot;, y= &quot;Verbal Ability Grade 2&quot;) + xlim(0,50) + ylim(0,50) + theme_bw() + theme( plot.background = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.border = element_blank() ) + #draws x and y axis line theme(axis.line = element_line(color = &#39;black&#39;)) + #set size of axis labels and titles theme(axis.text = element_text(size=12), axis.title = element_text(size=14)) ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. ## `geom_smooth()` using formula = &#39;y ~ x&#39; "],["7.4-mean-centering-predictors.html", "7.4 Mean Centering Predictors", " 7.4 Mean Centering Predictors In this case, and in many other cases, the intercept does not have a ‘useful’ interpretation for the empirical example. This is because no students had a Grade 1 verbal score equal to 0. Therefore, if we want to make the intercept more meaningful, we need to make a Grade 1 verbal score with a more meaningful 0 point. Typically we center the predictor variables in regression analysis. For example, we create a centered variable, \\(x^{*}_{1i}\\) by subtracting the sample mean, \\(\\bar{x_1}\\) from each observation, \\[ x^{*}_{1i} = x_{1i} - \\bar{x_1} \\] Our model becomes \\[ y_i = b_0(1_i) + b_1(x^{*}_{1i}) + \\epsilon_i \\] We can sample-mean center \\(verb_{1i}\\) in R as follows #calculate the mean centered variable wiscsub$verb1_star &lt;- wiscsub$verb1 - mean(wiscsub$verb1, na.rm = TRUE) Then we can fit a new model using \\(verb^{*}_{1i}\\), such that \\[ verb_{2i} = b_0(1_i) + b_1(verb^{*}_{1i}) + \\epsilon_i \\] model3 &lt;- lm(verb2 ~ 1 + verb1_star, data = wiscsub, na.action = na.exclude) summary(model3) ## ## Call: ## lm(formula = verb2 ~ 1 + verb1_star, data = wiscsub, na.action = na.exclude) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.5305 -3.0362 0.2526 2.7147 12.5020 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 25.41534 0.29831 85.20 &lt;2e-16 *** ## verb1_star 0.75495 0.05149 14.66 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.261 on 202 degrees of freedom ## Multiple R-squared: 0.5156, Adjusted R-squared: 0.5132 ## F-statistic: 215 on 1 and 202 DF, p-value: &lt; 2.2e-16 Note: Mean centering should be used to aid interpretation. Historically, it has been suggested that mean centering will reduce multicollinearity, however this is not the case. See for more information. 7.4.1 Interpreting Model Parameters Note that the estimate for the slope \\(b_1\\) stays the same, but the estimate for the intercept is different. This is because the variable ‘verb1_star’ equals 0 when a child has an average 1st grade verbal score. Therefore the expected value for the 2nd grade verbal score, for a child with an average 1st grade verbal score, is 25.41534. 7.4.2 Plotting Regression Line ggplot(data=wiscsub, aes(x=verb1_star,y=verb2)) + geom_point(size = 2, shape=19) + geom_smooth(method=lm,se=TRUE,fullrange=TRUE,colour=&quot;red&quot;, size=2) + labs(x= &quot;Sample-Centered Verbal Ability Grade 1&quot;, y= &quot;Verbal Ability Grade 2&quot;) + xlim(-20,20) + ylim(0,50) + #theme with white background theme_bw() + #eliminate background, gridlines, and chart border theme( plot.background = element_blank() ,panel.grid.major = element_blank() ,panel.grid.minor = element_blank() ,panel.border = element_blank() ) + #draws x and y axis line theme(axis.line = element_line(color = &#39;black&#39;)) + #set size of axis labels and titles theme(axis.text = element_text(size=12), axis.title = element_text(size=14)) ## `geom_smooth()` using formula = &#39;y ~ x&#39; Note the change of scale on the x-axis. "],["7.5-multiple-linear-regression.html", "7.5 Multiple Linear Regression", " 7.5 Multiple Linear Regression Now, let’s include a second predictor. We have information on the number of years of education for the children’s fathers, variable daded. The values in daded indicate the number of years of education each father completed. First, let’s take a look at the distribution of this new predictor variable. psych::describe(wiscsub$daded) ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 204 10.81 2.7 11.5 11 2.97 5.5 18 12.5 -0.36 0.01 0.19 ggplot(data=wiscsub, aes(x=daded)) + geom_histogram(binwidth=2.5, fill=&quot;white&quot;, color=&quot;black&quot;) + xlim(0,20) + xlab(&quot;Number of Years of Father&#39;s Education&quot;) + ylab(&quot;Count&quot;) + theme_classic() ## Warning: Removed 2 rows containing missing values or values outside the scale range ## (`geom_bar()`). And the relation between Grade 2 verbal scores and daded. ggplot(data=wiscsub, aes(x=daded, y = verb2)) + geom_point() + xlim(0,20) + ylim(0,50) + xlab(&quot;Father&#39;s Education (Years)&quot;) + ylab(&quot;Verbal Ability Grade 2&quot;) + theme_classic() 7.5.1 Regression Equation Our model now becomes \\[ verb_{2i} = b_01_{i} + b_1verb^{*}_{1i} + b_2daded^{*}_{i} + \\epsilon_{i}\\] where \\(verb^{*}_{1i}\\) is the sample-centered version of \\(verb_{1i}\\), and \\(daded^{*}_{i}\\) is the sample-centered version of \\(daded_{i}\\). The slope, \\(b_2\\) is the expected difference in grade 2 verbal score for each 1 year difference in father’s education. We can also center the daded variable. # Calculate mean-centered version of father&#39;s education variable wiscsub$daded_star &lt;- wiscsub$daded - mean(wiscsub$daded) 7.5.2 Fit Model in R model4 &lt;- lm(verb2 ~ 1 + verb1_star + daded_star, data = wiscsub, na.action = na.exclude) summary(model4) ## ## Call: ## lm(formula = verb2 ~ 1 + verb1_star + daded_star, data = wiscsub, ## na.action = na.exclude) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.4354 -2.9189 -0.1542 2.3746 11.1678 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 25.41534 0.29069 87.430 &lt; 2e-16 *** ## verb1_star 0.66786 0.05626 11.872 &lt; 2e-16 *** ## daded_star 0.41454 0.12108 3.424 0.000749 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.152 on 201 degrees of freedom ## Multiple R-squared: 0.5422, Adjusted R-squared: 0.5377 ## F-statistic: 119.1 on 2 and 201 DF, p-value: &lt; 2.2e-16 Now we have an intercept and two slopes. 7.5.3 Path Diagram semPlot::semPaths(model4, what = &quot;paths&quot;) 7.5.3.1 Interpreting Model Parameters \\(b_0\\) is the expected value of the outcome variable when all other variables are 0. Therefore, in this case, \\(b_0\\) is the expected Grade 2 verbal score for a child with an average Grade 1 verbal score (i.e. \\(verb^{*}_{1i}\\) = 0) and whose father had an average education (i.e. \\(daded^{*}_{i}\\) = 0, \\(\\bar{daded_{i}}\\) = 10.81 years of education. \\(b_1\\) is the expected difference in the outcome for a 1-unit difference in \\(x_{1i}\\). In this example (i.e. ‘model4’), \\(b_1\\) is the expected difference in Grade 2 verbal score (outcome variable, \\(y_i\\) = \\(verb_{2i}\\)) for a 1 point difference in the Grade 1 verbal score (\\(x_{1i}\\) = \\(verb^{*}_{1i}\\)), holding constant the level of father’s education. \\(b_2\\) is the expected difference in the outcome for a 1-unit difference in \\(x_{2i}\\). For this example (i.e. ‘model4’), \\(b_2\\) is the expected difference in Grade 2 verbal score (outcome variable, \\(y_i\\) = \\(verb_{2i}\\)) for each year difference in father’s education (\\(x_{2i}\\) = \\(daded^{*}_i\\)), holding constant in Grade 1 verbal score. 7.5.4 A Note on Interpretation The distinguishing feature for interpretation in linear models without interactions or higher-order terms is that the effect of a given change in an independent variable is the same regardless of the value of that variable (at the start of its change) and regardless of the level of the other variables in the model. Interpretation only needs to specify which variable is changing, by how much, and that other variables are being held constant. In regard to our last example, years of father’s education does impact the relationship between Grade 1 and 2 verbal scores. Likewise, the effect of father’s education on Grade 1 verbal scores does not depend on Grade 1 verbal scores. Said in a different way, no matter what a child’s Grade 1 verbal score was, the effect of father’s education on Grade 2 verbal scores is the same. Consider another model with categorical and continuous predictors, grad and verb1_star, respectively. library(&quot;ggiraphExtra&quot;) model4b &lt;- lm(verb2 ~ 1 + verb1_star + grad, data = wiscsub, na.action = na.exclude) ggPredict(model4b,se=TRUE,interactive=FALSE) Notice in the plot, whether a student’s father graduated HS does not impact the relationship between Grade 1 and 2 verbal scores. Differences in Grade 1 and 2 verbal scores are not dependent on whether or not the father graduated HS. Most importantly, using the coefficients themselves we can easily interpret the model parameters. "],["7.6-categorical-variable-interaction.html", "7.6 Categorical Variable Interaction", " 7.6 Categorical Variable Interaction Ok, let’s move on to the topic of an interaction which uses the product of two predictor variables as a new predictor. Working up a slightly different example with the ‘grad’ variable (whether dad graduated high school), \\[ verb_{2i} = b_01_i + b_1verb^{*}_{1i} + b_2grad_{i} + b_3(verb^{*}_{1i}grad_{i}) + \\epsilon_{i}\\] Where \\(verb^{*}_{1i}\\) is the mean-centered version of \\(verb_{1i}\\), and \\(grad_i\\) is a dummy coded variable that equals 0 if the child’s father did not graduate high school, and equals 1 if the child’s father did graduate high school. We did not sample-mean center \\(grad_i\\) in this example because a value of 0 already has substantive meaning for the current example (i.e. when \\(grad_i\\) equals 0, the father did not graduate high school). 7.6.1 Interaction as Moderation Often, we describe phenomena in terms of moderation; or that the relation between two variables (i.e. \\(y_i\\) and \\(x_{1i}\\)) is moderated by a third variable (i.e. \\(x_{2i}\\)). For example, the relation between Grade 1 and Grade 2 verbal scores may be moderated by father’s graduation status. More specifically, the relation between 1st and 2nd grade verbal score may be different for children whose fathers’ did not or did graduate from high school. The inclusion of product terms (i.e. interactions) allows for a direct investigation of a moderation hypothesis. 7.6.1.1 Choosing a Moderator When we use a product term, we should define one of the variables as the moderator and one of the variables as the predictor of interest. Let’s call \\(verb^{*}_{1i}\\) the predictor of interest, and \\(grad_{i}\\) the moderator. 7.6.2 Moderation by Categorical Variable When the moderator is a dummy variable then the form of the moderation becomes fairly simple; we will have one equation for \\(grad_{i} = 0\\), and a second equation for \\(grad_i = 1\\). 7.6.2.1 Rewriting Equation To illustrate the notion of two equations, let’s rewrite the regression equation \\[ verb_{2i} = b_01_i + b_1verb^{*}_{1i} + b_2grad_{i} + b_3(verb^{*}_{1i}grad_{i}) + \\epsilon_{i}\\] as two separate regression equations, one for fathers who graduated from highschool and one for fathers that did not. We can accomplish this by plugging in \\(0\\) and \\(1\\) into the regression equation and rearranging some of the terms. Doing so we get Equation for Students whose father Graduated Highschool \\[ verb_{2i} = (b_0 + b_2) + (b_1 + b_3)verb^{*}_{1i} + \\epsilon_{i}\\] Equation for Students whose father Did Not Graduate from Highschool \\[ verb_{2i} = b_0 + b_1verb^{*}_{1i} + \\epsilon_{i}\\] 7.6.3 Interpretation Without an interaction, our linear regression model assumes that the only difference between the regression line for each group (graduate HS vs not) is the intercept. That is, it assumes that the relationship between verbal scores at Grades 1 and 2 is the same for both groups. Children Whose father’s Did Not Graduate HS The expected Grade 2 verbal score for a child whose father did not graduate high school and who had an average Grade 1 verbal score is \\(b_0\\). Also, for a child whose father did not graduate high school, \\(b_1\\) is the expected difference in their Grade 2 verbal score for a one-point difference in their Grade 1 verbal score. Children Whose father’s Did Not Graduate HS The parameter estimates \\(b_0\\) and \\(b_1\\) maintain their interpretation from before. But now each of them is moderated (i.e. shifted or altered) by \\(b_2\\) or \\(b_3\\). Specifically, the expected Grade 2 verbal score for a child whose father did graduate high school and who earned an average Grade 1 verbal score is \\(b_0 + b_2\\). And, for a child whose father did graduate high school, \\(b_1 + b_3\\) is the expected difference in their Grade 2 verbal score for a one-point change in their Grade 1 verbal score. 7.6.4 Fit Regression Model in R OK - let’s fit the model! Note that within this model we use the code I(verb1_star * grad). This produces the interaction term within the model. The wrapper function I() indicates to R to perform this data computation as-is, otherwise we would need to perform this computation (i.e. the multiplication of verb1_star by grad) outside of the function lm(). model5 &lt;- lm(verb2 ~ 1 + verb1_star + grad + I(verb1_star*grad), data = wiscsub, na.action = na.exclude) summary(model5) ## ## Call: ## lm(formula = verb2 ~ 1 + verb1_star + grad + I(verb1_star * grad), ## data = wiscsub, na.action = na.exclude) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.3433 -3.0761 -0.0825 2.5689 10.7289 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 25.2663 0.3416 73.956 &lt;2e-16 *** ## verb1_star 0.7861 0.0604 13.015 &lt;2e-16 *** ## grad 1.4632 0.8107 1.805 0.0726 . ## I(verb1_star * grad) -0.2430 0.1324 -1.836 0.0678 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.232 on 200 degrees of freedom ## Multiple R-squared: 0.5268, Adjusted R-squared: 0.5197 ## F-statistic: 74.22 on 3 and 200 DF, p-value: &lt; 2.2e-16 7.6.5 Path Diagram semPlot::semPaths(model5, what = &quot;paths&quot;) The parameter estimates from this model indicate that, for children whose father did not graduate high school, the expected Grade 2 verbal score for a child that earned an average 1st grade verbal score equals 25.2663 (\\(b_0\\)). Also, for children whose father did not graduate high school, a 1-point difference in their Grade 1 verbal score is expected to correspond with a 0.7861 (\\(b_1\\)) point difference in the Grade 2 verbal score. Moreover, the parameter estimates indicate that, for children whose father did graduate high school, the expected Grade 2 verbal score for a child that earned an average Grade 1 verbal score is 25.2663 + 1.4632 = 26.7295 (\\(b_0 + b_2\\)). Also, for children whose father graduated high school, a 1-point difference in their Grade 1 verbal score is expected to correspond with a (\\(b_1 + b_3\\)) = 0.7861 - 0.2430 = 0.5431 point difference in the Grade 2 verbal score. Even though the interaction is not significant, we can plot it for illustrating the moderation effect: #plot of moderation ggplot(data=wiscsub, aes(y=verb2,x=verb1_star, color = factor(grad))) + geom_jitter() + stat_smooth(method=&#39;lm&#39;, se=TRUE, fullrange=TRUE) + xlab(&quot;1st Grade Verbal Score&quot;) + ylab(&quot;2nd Grade Verbal Score&quot;) + guides(color=guide_legend(title=&quot;HS Grad&quot;)) + theme_bw() ## `geom_smooth()` using formula = &#39;y ~ x&#39; The example from ‘model5’ contained an interaction using a dummy variable (i.e., \\(grad_i\\)). Interactions may also occur between two continuous variables (i.e., \\(verb^{*}_{1i}\\) and \\(daded^{*}_{i}\\)). We will not cover here, but note that it is still very useful to consider and communicate those interactions as moderation. There are many resources on interactions of two (or more) continuous variables. "],["8-chapter-8.html", "Chapter 8 Logistic Regression", " Chapter 8 Logistic Regression In Chapter 8 we will introduce the logistic regression model through the lens of generalized linear models (GLMs). The GLM is an essential tool for modeling discretely distributed dependent variables. "],["8.1-categorical-data-in-the-social-sciences.html", "8.1 Categorical Data in the Social Sciences", " 8.1 Categorical Data in the Social Sciences Linear regression is a workhorse procedure of modern statistics. Our introduction to regression in this class was framed around the idea of a continuous dependent (outcome) variable. However, categorical data is extremely common in many health, behavioral and social science applications. 8.1.1 Examples of Categorical Data Binary Variables have two categories and are often used to indicate that an event has occurred or a characteristic is present. Are you sick? Did you vote in the last election? Are you married? Ordinal variables have categories that can be ranked. Surveys often ask respondents to indicate their agreement to a statement, how frequently then engage in a behavior, or even educational attainment. Nominal variables occur when there are multiple outcomes that cannot be ordered. For example, left or right handedness or occupation. Censored variables occur when the value of a variable is unknown over some range of the variable. For example, measuring hourly wages might be restricted on the lower end by minimum wage laws. Counts indicate the number of times that some event has occurred. How many drinks last week? How many people living in a house? How many years of education? Censored and count variables are often lumped in with more traditional categorical variables under the umbrella of limited dependent variables. "],["8.2-introduction-to-glms.html", "8.2 Introduction to GLMs", " 8.2 Introduction to GLMs Earlier we noted linear regression is typically applied to continuous variables. The ubiquity of categorical data leads us to a modeling framework better suited to handling a wide range of categorical outcomes: the Generalized Linear Model (GLM). In the GLM, the response variable \\(y_{i}\\) is assumed to follow an exponential distribution with mean \\(\\mu_{i}\\), which itself is a nonlinear function of \\(x^{&#39;}_{i}\\beta\\). We can think about \\(\\mu\\) as the mean of a conditional response distribution at a given point in the covariate space. There are three important components to the GLM: A random component: The random component of the GLM contains the response variable \\(\\mathbf{Y}\\) and its probability distribution (e.g. the binomial distribution of \\(\\mathbf{Y}\\) in the binary regression model). A Linear Predictor: The linear predictor typically takes the form of \\(\\mathbf{X}\\boldsymbol{\\beta}\\) where \\(\\mathbf{X}\\) is an \\(n \\times q\\) matrix of observations and \\(\\boldsymbol{\\beta}\\) is an \\(q \\times 1\\) column vector. Link Function: The link function, typically specified as \\(g()\\), is used to relate each component of \\(\\mathbb{E}(\\mathbf{Y})\\) to the linear predictor, \\(g[\\mathbb{E}(\\mathbf{Y})]=\\mathbf{X}\\boldsymbol{\\beta}\\). 8.2.1 Linear Regression as GLM Linear regression can be formulated in the GLM framework as follows: \\[ \\mu_{i} = \\beta_{0} + \\beta_{1}x_{1i} \\] A random component: We can make specify \\(\\mathbf{Y} \\sim \\mathcal{N}(\\mu, \\sigma^2)\\). A Linear Predictor: \\(\\mathbf{X}\\) are the continuous or discrete explanatory variables. The way we think about the structural component here doesn’t really differ from how we think about it with standard linear models; in fact, that’s one of the nice advantages of the GLM. Link Function: For linear regression we use the identity link (e.g. \\(\\eta=g[\\mathbb{E}(\\mathbf{Y})]=\\mathbb{E}(\\mathbf{Y})\\)). 8.2.2 Logistic Regression as GLM Let’s also take a look at binary logistic regression formulated as GLM. \\[ \\mathrm{logit}(\\pi_i) = log(\\frac{\\pi_{i}}{1-\\pi_i})=\\beta_{0} + \\beta_{1}x_{1i} \\] A random component: The distribution of \\(\\mathbf{Y}\\) is assumed to be binomial with success probability \\(\\mathbb{E}(\\mathbf{Y})=\\pi\\). A Linear Predictor: \\(\\mathbf{X}\\) are the continuous or discrete explanatory variables. Link Function: For logistic regression we use the log-odds (or logit) link (e.g. \\(\\eta=g(\\pi)=log(\\frac{\\pi_{i}}{1-\\pi_i})\\)), where \\(\\eta\\) is the transformed outcome. 8.2.3 Poisson Regression as GLM Poisson regression can also be formulated as a GLM: \\[ \\mathrm{log}(\\lambda_i) = \\beta_{0} + \\beta_{1}x_{1i} \\] A random component: The distribution of \\(\\mathbf{Y}\\) is assumed to be Poisson with mean \\(\\lambda\\) \\(\\mathbb{E}(\\mathbf{Y})=\\pi\\). A Linear Predictor: \\(\\mathbf{X}\\) are the continuous or discrete explanatory variables. Link Function: For Poisson regression the log link is used. 8.2.4 Additional Remarks When the outcome data \\(\\mathbf{Y}\\) are not normally distributed, we can always do transformation to change its scale. These are typically done via link functions denoted as \\(g(\\cdot)\\) - so we get \\(g(\\mathbf{Y})\\). If we denote the transformed outcome as \\(\\boldsymbol{\\eta}\\), then we can denote it as: \\[g(\\mathbf{Y})=\\boldsymbol{\\eta}\\] From a conceptual point of view, the link function \\(g(\\cdot)\\) transforms \\(\\mathbf{Y}\\) into a normal outcome. Note that we are simplifying notation somewhat: while we are modeling some expectation of \\(\\mathbf{Y}\\), not exactly \\(\\mathbf{Y}\\), we will keep on using \\(\\mathbf{Y}\\). This is to say the link is applied to the parameter governing the response distribution, not the actual response data. We use link functions to formalize that the conditional expectation for \\(\\mathbf{Y}\\) (conditional because it is the expected value of \\(Y\\) depending on the level of the predictors and the chosen link). Each link function also has an inverse, \\(h(\\cdot)=g^{-1}(\\cdot)\\), which allows us to define \\[\\mathbf{y}=g^{-1}(\\boldsymbol{\\eta})=h(\\boldsymbol{\\eta})\\] The inverse of a link function back-converts the linear combination of predictors into the original outcome. "],["8.3-binary-logistic-regression.html", "8.3 Binary Logistic Regression", " 8.3 Binary Logistic Regression 8.3.1 Overcoming LPM To avoid the problems of the LPM we’d like a model where \\[P(\\text{Event Occurs}|x_{1},\\dots,x_{q})\\] is forced to be within the range of \\(0\\) to \\(1\\). One way to do this is to transform the probability above into the odds metric, \\[ \\mathrm{Odds}(\\mathbf{Y})= \\frac{P(\\text{Outcome = 1}|x_{1},\\dots,x_{q})}{P(\\text{Outcome = 0}|x_{1},\\dots,x_{q})} = \\frac{P(\\text{Outcome = 1}|x_{1},\\dots,x_{q})}{P(1- \\text{Outcome = 1}|x_{1},\\dots,x_{q})} \\] which has a range of \\(0\\) to \\(\\infty\\). So, we are halfway there. Indeed, by taking the log of the odds (or logit) we extend the support of \\(\\mathbf{Y}\\) to have a range of \\(-\\infty\\) to \\(\\infty\\). This maps probability ranging between \\(0\\) and \\(1\\) to log odds ranging from negative infinity to positive infinity. This is one example of why the logit link is used for logistic regression. Nowe we can seamlessly model the probability of an event occurring, giving the explanatory variables, \\(x_{1},\\dots,x_{q}\\). We denote this probability as \\(\\pi(x_{1},\\dots,x_{q})\\) , or equivalently, \\(P(\\text{Event Occurs}|x_{1},\\dots,x_{q})\\). Often times you will simply see \\(\\pi\\) for convenience, but it is important to remember this probability is conditional on the explanatory variables in the model. 8.3.2 Model The binary logistic regression model is expressed as \\[ log(\\frac{\\pi_{i}}{1-\\pi_i})=\\beta_{0} + \\beta_{1}x_{1i}. \\] Where \\(\\left(\\frac{(\\pi_{i})}{1-(\\pi_{i})}\\right)\\) is the odds of an event occurring and \\(log\\) is the natural logarithm. Therefore, the parameter estimates from a generalized linear regression using the logistic link function are scaled in log-odds or logit units. We can also rewrite the model above, solving for \\(\\pi_{i}\\), as \\[ \\pi_{i}=\\frac{\\mathrm{exp}(\\beta_{1}x_{1i})}{1+\\mathrm{exp}(\\beta_{1}x_{1i})} \\] This is also called the inverse function for the logit link function, or the logistic link, \\(h(\\cdot) = \\frac{e^{(\\cdot)}}{1+e^{(\\cdot)}}\\). In practice, this transformation is what is used for solving the regression equation, and it is called logistic regression: "],["8.4-example-data-3.html", "8.4 Example Data", " 8.4 Example Data Chapter 7 In Chapter 7 we use data from Dunn, Aknin, and Norton (2007), who examined the relationship between spending habits and happiness using OLS regression. To capture spending habits self-reported monthly spending was categorized as being either personal or prosocial, and then summed to create a category-specific total. Measures of happiness were obtained using a 5-item ordinal scale. The dependent variable in this analysis (GeneralHappiness) was self-reported general happiness. Participants rated their general happiness by answering the question “Do you feel happy, in general?” by selecting from five possible response options (no, rarely, sometimes, most of the time, and yes), which were then scored from 1-5. Here, higher numbers were indicative of greater happiness. For the purpose of our analysis we will dichotomize GeneralHappiness depending on whether a person was not happy vs happy. The sample was selected to be a nationally representative sample of 632 Americans (287 males and 345 females). Participants responded as part of a larger, online survey, in return for points that could be redeemed for prizes (Dunn, 2008). No further details on sampling were available. 8.4.1 Variables Happy: Dichotomous variable indicating whether the subject responded “yes” when asked if they felt happy, in general. PersonalSpending: Self-reported dollars spent per month on (a) bills and expenses, and (b) gifts for themselves. ProsocialSpending: Self-reported dollars spent per month on (a) gifts for others, and (b) donations to charity. PersonalIncome: Participants selected their personal income category from 6 options: less than $20,000, $20,000-$35,000, $35001-$50,000, $50,001-$65000, $65,001-$80,000, $80,001+. library(&quot;ggplot2&quot;) dunn2008 &lt;- read.csv(&quot;data/DUNN2008.csv&quot;) dunn2008$Happy &lt;- ifelse(dunn2008$GeneralHappiness == &quot;yes&quot;, 1, 0) dunn2008$PersonalSpending &lt;- dunn2008$PersonalSpending/100 dunn2008$ProsocialSpending &lt;- dunn2008$ProsocialSpending/100 dunn2008$PersonalSpending_star &lt;- as.numeric(scale(dunn2008$PersonalSpending, scale = FALSE)) dunn2008$ProsocialSpending_star &lt;- as.numeric(scale(dunn2008$ProsocialSpending, scale = FALSE)) dunn2008$Income &lt;- dplyr::recode(dunn2008$PersonalIncome, &quot;20001-35000&quot; = &quot;20-35K&quot;, &quot;35001-50000&quot; = &quot;35-50K&quot;, &quot;50001-65000&quot; = &quot;50-65K&quot;, &quot;65001-80000&quot; = &quot;65-80K&quot;, &quot;80001andup&quot; = &quot;&gt; 80K&quot;, &quot;less20000&quot; = &quot;&lt; 20K&quot;) inc_lev_order &lt;- c(&quot;&lt; 20K&quot;,&quot;20-35K&quot;, &quot;35-50K&quot;,&quot;50-65K&quot;,&quot;65-80K&quot;,&quot;&gt; 80K&quot;) dunn2008$Income &lt;- factor(dunn2008$Income, levels=inc_lev_order) "],["8.5-intercept-only-model-1.html", "8.5 Intercept-Only Model", " 8.5 Intercept-Only Model In logistic regression, we are interested in how various predictors are related to the probability of a specific outcome \\(P(Y_i = 1) = \\pi_i\\). In this example we are interested in the probability an individual reports being happy, in a general sense. Making use of the logit link function, the general equation for logistic regression is \\[\\mathrm{logit}(\\pi_i) = \\beta_{0} + \\beta_{1}x_1 + ... + \\beta_{q}x_q\\] Which after back transformation gives us … \\[ P(Y_i = 1) = \\pi_i = \\frac{e^{\\beta_{0} + \\beta_{1}x_1 + ... + \\beta_{q}x_q}}{1+e^{\\beta_{0} + \\beta_{1}x_1 + ... + \\beta_{q}x_q}} = \\frac{\\mathrm{exp}(\\beta_{0} + \\beta_{1}x_1 + ... + \\beta_{q}x_q)}{1+\\mathrm{exp}(\\beta_{0} + \\beta_{1}x_1 + ... + \\beta_{q}x_q)} \\] 8.5.1 Intercept-Only Model in R In our example the variable Happy indicates whether a subject reporting being happy. Let’s start with the simplest model for predicting Happy, the intercept-only model. More specifically, we have $ logit(_i) = b_0(1_i)$where \\(\\pi_i = P(grad_i = 1)\\). We can use the glm() function to fit the model to the data model9 &lt;- glm(Happy ~ 1, family = &quot;binomial&quot;, data = dunn2008, na.action = na.exclude) summary(model9) ## ## Call: ## glm(formula = Happy ~ 1, family = &quot;binomial&quot;, data = dunn2008, ## na.action = na.exclude) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.69077 0.08435 -8.19 2.62e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 805.02 on 631 degrees of freedom ## Residual deviance: 805.02 on 631 degrees of freedom ## AIC: 807.02 ## ## Number of Fisher Scoring iterations: 4 Without wanting to get to detailed we don’t need to specify the logit link here because it is the canonical link function for the binomial distribution. This essentially means there is a direct correspondence between the predicted mean and the distribution’s canonical location parameter. 8.5.2 Interpretation 8.5.2.1 Intercept Parameter In the intercept-only model, the intercept, \\(b_0\\), reflects The expected log-odds (\\(-0.69077\\)) of an individual reporting they were happy. The odds of someone reporting being happy \\(\\mathrm{exp}(b_0)=0.5\\). exp(-0.69077) ## [1] 0.50119 The expected probability (\\(0.33\\)) of the a subject reported being happy in general. \\[ P(Happy_i = 1) = \\pi_i = \\frac{e^{b_0}}{1+e^{b_0}} \\] or, equivalently, in R exp(-0.69077)/(1 + exp(-0.69077)) ## [1] 0.3338618 We can also confirm that the backward transformed parameter from this intercept-only logistic regression matches the expectation we get from the descriptives of the raw data. mean(dunn2008$Happy) ## [1] 0.3338608 Note: If \\(\\beta_j &gt; 0\\) then \\(\\mathrm{exp}(b_j) &gt; 1\\), indicating a positive relationship between \\(X_{j}\\) and the probability of the event occurring. If \\(\\beta_j &lt; 0\\), the opposite relationship holds. "],["8.6-single-predictor-model.html", "8.6 Single Predictor Model", " 8.6 Single Predictor Model OK, let’s include a predictor in our logistic regression model. Let’s start with PersonalSpending such that \\[ logit(\\pi_i) = b_0 + b_1PersonalSpending^{*}_{1i} + \\epsilon_i \\] where \\(\\pi_i = P(Happy_i = 1)\\). Here, \\(PersonalSpending^{*}\\) is the mean-centered amount of money one spends on themselves in a month (in units of \\(100\\) dollars). Let’s fit the model in R. model10 &lt;- glm(Happy ~ 1 + PersonalSpending_star, family = &quot;binomial&quot;, data = dunn2008, na.action = na.exclude) summary(model10) ## ## Call: ## glm(formula = Happy ~ 1 + PersonalSpending_star, family = &quot;binomial&quot;, ## data = dunn2008, na.action = na.exclude) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.690890 0.084358 -8.190 2.61e-16 *** ## PersonalSpending_star -0.001355 0.004605 -0.294 0.769 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 805.02 on 631 degrees of freedom ## Residual deviance: 804.93 on 630 degrees of freedom ## AIC: 808.93 ## ## Number of Fisher Scoring iterations: 4 8.6.1 Overdispersion A quick digression. In the binary logistic regression model overdispersion occurs when the observed variance is larger than what the binomial distribution would predict. For example, if \\(Y \\sim \\mathrm{Binomial}(n_{i},\\pi_{i})\\), the mean is \\(\\u_{i}=n_{i}\\pi_{i}\\) and the variance is \\(n_{i}\\pi_{i}(1-\\pi_{i})\\). Since both of these moments rely on \\(\\pi_{i}\\), it can be overly restrictive, and if overdispersion is present inferences can become distorted. We will talk about this more later. 8.6.2 Coefficients Again, There are essentially three ways to interpret coefficients from a logistic regression model: The log-odds (or logit) The Odds Probabilities 8.6.2.1 Log-Odds The parameter estimate \\(b_0\\) reflects the expected log-odds (\\(-0.69\\)) of being happy for an individual with an average amount of personal spending. The estimate for \\(b_1\\) indicates the expected difference of the log-odds of being happy for a \\(100\\) dollar difference in personal spending. Therefore, we expect a \\(-0.001\\) difference in the log-odds of being happy for a \\(100\\) dollar difference in personal spending. 8.6.2.2 Odds Parameter estimates from a logistic regression are often reported in terms of odds rather than log-odds. To obtain parameters in odds units, we simply exponentiate the coefficients. Note that this is just one of the steps of the inverse link function (which would take us all the way to probability units). exp(cbind(OR = coef(model10), confint(model10))) ## Waiting for profiling to be done... ## OR 2.5 % 97.5 % ## (Intercept) 0.5011299 0.4240750 0.5903901 ## PersonalSpending_star 0.9986459 0.9886908 1.0072747 In other words, the odds of being happy when personal spending is at average levels is \\(exp(-0.690890) = 0.5\\). In regard to the slope coefficient, for a \\(100\\) dollar difference in monthly personal spending, we expect to see about \\(.1\\%\\) decrease in the odds of being happy. This decrease does not depend on the value that personal spending is held at. Note this is not significant and we would not report this interpretation in practice. Essentially, if the odds ratio is equal to one, the predictor did not have an impact on the outcome. 8.6.2.3 Probability Remember, probabilities range from \\([0,1]\\), whereas log-odds (the output from the raw logistic regression equation) can range from \\((-\\infty,\\infty)\\), and odds and odds ratios can range from \\((0,\\infty)\\). Due to the bounded range of probabilities, probabilities are non-linear, but log-odds can be linear. For example, as personal spending goes up by constant increments, the probability of happiness will increase (decrease) by varying amounts, but the log-odds will increase (decrease) by a constant amount, and the odds will increase (decrease) by a constant multiplicative factor. For this reason it is not so simple to interpret probabilities in logistic regression from the coefficient directly. Often it is much simpler to plot the probabilities across a range of the predictor variables. ggplot(data=dunn2008, aes(x=PersonalSpending_star,y=Happy)) + geom_point(alpha = .08, size = 10) + xlab(&quot;Personal Spending&quot;) + ylab(&quot;Happy&quot;) + theme_bw() + stat_smooth(method = &#39;glm&#39;, method.args = list(family = &quot;binomial&quot;), se = TRUE) ## `geom_smooth()` using formula = &#39;y ~ x&#39; Notice how the density of the observations is visualized by manipulating the transparency (alpha) level of the data points. The predicted curve based on our model has of course a non-linear shape (however, if we were to plot the relationship between the variables with using the logit link, it would be a straight line). "],["8.7-marginal-effects.html", "8.7 Marginal Effects", " 8.7 Marginal Effects So far we have considered two possibilities for interpreting logistic regression results: Interpreting the log-odds directly Transforming the log-odds into odds A probability metric (for a single explanatory variable) However, as we include more covariates in our model, interpretation becomes more difficult. We can only think about “holding other variable constant” in the log-odds and odds scale. For nonlinear model marginal effects provide us with an intuitive and easy to interpret method for understanding and communicating results. 8.7.1 A Definition of Marginal Effects Marginal effects are partial derivatives of the regression equation with respect to each variable in the model for each unit in the data. Put differently, the marginal effect measures the association between a change in an explanatory variable and a change in the response. The marginal effect is the slope of the prediction function, measured at a specific value of the explanatory variable. In linear models the effect of a given change in an independent variable is the same regardless of (1) the value of that variable at the start of its change, and (2) the level of the other variables in the model. In nonlinear models the effect of a given change in an independent variable (1) depends on the values of other variables in the model, and (2) is no longer equal to the parameter itself. Consider a linear and nonlinear model for happiness as a function of personal spending and a dummy variable indicating whether someone is rich. 8.7.2 A Few Observations For the linear model: - Whether one is rich or poor does no impact the relationship between happiness and personal spending. - Differences in happiness levels between rich and poor are not dependent on the amount of money one spends. From the nonlinear model: - Whether one is rich or poor does impact the relationship between happiness and personal spending. - Differences in happiness levels between rich and poor are dependent on the amount of money one spends. 8.7.2.1 Another Nonlinear Example A helpful example is provided in the marginaleffects vignette. Consider a simple quadratic \\[ y = -x^2 \\\\ \\] with partial derivative of \\(y\\) with respect to \\(x\\) \\[ \\frac{\\partial y}{\\partial x} = -2x. \\] https://vincentarelbundock.github.io/marginaleffects/articles/mfx.html 8.7.3 Types of Marginal Effects There are generally three types of marginal effects people consider: Marginal Effects at the Means (MEM) Average Marginal Effects (AME) Marginal Effects at Representative Values (MEM) We will focus on marginal effects at representative values as this is the most powerful option. 8.7.4 Example Model Let’s fit a more complicated model. To look at marginal effects we will use the marginaleffects package. library(&quot;marginaleffects&quot;) model11 &lt;- glm(Happy ~ 1 + PersonalSpending + ProsocialSpending + Income, family = &quot;binomial&quot;, data = dunn2008, na.action = na.exclude) summary(model11) ## ## Call: ## glm(formula = Happy ~ 1 + PersonalSpending + ProsocialSpending + ## Income, family = &quot;binomial&quot;, data = dunn2008, na.action = na.exclude) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.636366 0.223151 -2.852 0.00435 ** ## PersonalSpending -0.011635 0.006643 -1.752 0.07985 . ## ProsocialSpending 0.083741 0.032975 2.540 0.01110 * ## Income20-35K -0.331327 0.288006 -1.150 0.24997 ## Income35-50K -0.057632 0.297299 -0.194 0.84629 ## Income50-65K 0.170117 0.326317 0.521 0.60214 ## Income65-80K 0.221637 0.323873 0.684 0.49376 ## Income&gt; 80K 0.259641 0.307140 0.845 0.39791 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 805.02 on 631 degrees of freedom ## Residual deviance: 791.06 on 624 degrees of freedom ## AIC: 807.06 ## ## Number of Fisher Scoring iterations: 4 8.7.5 Marginal Effects at Representative Values (MER) For example, let’s look at the impact of Income on the probability of being happy. marginaleffects::plot_cap(model11, condition = c(&quot;Income&quot;), conf.int = TRUE) ## Warning: This function has been renamed to `plot_predictions()`. The `plot_cap()` ## alias will be removed in the near future. What if we were interested in the relationship between Income and PersonalSpending on the probability of being happy. marginaleffects::plot_cap(model11, condition = c(&quot;PersonalSpending&quot;,&quot;Income&quot;)) ## Warning: This function has been renamed to `plot_predictions()`. The `plot_cap()` ## alias will be removed in the near future. What if we were interested in the relationship between Income and ProsocialSpending on the probability of being happy. marginaleffects::plot_cap(model11, condition = c(&quot;ProsocialSpending&quot;,&quot;Income&quot;)) ## Warning: This function has been renamed to `plot_predictions()`. The `plot_cap()` ## alias will be removed in the near future. In nonlinear, the marginal effect of one variable is conditional on the value of the other variable. This function draws a plot of the marginal effect of the effect variable for different values of the condition variable. Let’s look at the effect of PersonalSpending on the relationship between ProsocialSpending and Happy. marginaleffects::plot_cme(model11, variables = &quot;ProsocialSpending&quot;, condition = &quot;PersonalSpending&quot;) ## Warning: This function has been renamed to `plot_slopes()`. The `plot_cme()` ## alias will be removed in the near future. In addition, we can look at the effect of PersonalSpending on the relationship between PersonalSpending and Happy. marginaleffects::plot_cme(model11, variables = &quot;PersonalSpending&quot;, condition = &quot;ProsocialSpending&quot;) ## Warning: This function has been renamed to `plot_slopes()`. The `plot_cme()` ## alias will be removed in the near future. "],["9-chapter-9.html", "Chapter 9 Poisson Regression", " Chapter 9 Poisson Regression In Chapter 9 we will round out our discussion of the GLM with Poisson regression. Poisson regression can be a useful modeling approach for handling count dependent variables Counts typically describe nonnegative (or only positive) integer. Examples of counts are number of drinks per day, children per household, etc. In certain contexts, the Poisson distribution describes the number of events that occur in a given time period \\(\\mu\\) typically represents the mean number of events per period In the Poisson distribution, the mean is also equal to the variance. One important consideration when fitting Poisson regression models is overdispersion We will look at how one might assess overdispersion in Poisson regression and suggest some alternative procedures. A "],["9.1-poisson-regression.html", "9.1 Poisson Regression", " 9.1 Poisson Regression 9.1.1 Review of GLM To review, there are three important components to the GLM: A random component: The random component of the GLM contains the response variable \\(\\mathbf{Y}\\) and its probability distribution (e.g. the binomial distribution of \\(\\mathbf{Y}\\) in the binary regression model). A Linear Predictor: The linear predictor typically takes the form of \\(\\mathbf{X}\\boldsymbol{\\beta}\\) where \\(\\mathbf{X}\\) is an \\(n \\times q\\) matrix of observations and \\(\\boldsymbol{\\beta}\\) is an \\(q \\times 1\\) column vector. Link Function: The link function, typically specified as \\(g()\\), is used to relate each component of \\(\\mathbb{E}(\\mathbf{Y})\\) to the linear predictor, \\(g[\\mathbb{E}(\\mathbf{Y})]=\\mathbf{X}\\boldsymbol{\\beta}\\). 9.1.2 Poisson Regression as GLM Poisson regression can also be formulated as a GLM: \\[ \\mathrm{log}(\\mu) = \\beta_{0} + \\beta_{1}x_{1} \\] or equivalently, \\[ \\mu = \\mathrm{exp}(\\beta_{0} + \\beta_{1}x_{1}) = \\mathrm{exp}(\\beta_{0})\\mathrm{exp}(\\beta_{1}x_{1}). \\] A random component: The distribution of \\(\\mathbf{Y}\\) is assumed to be Poisson, \\(Y_{i} \\sim \\mathrm{Poisson}(\\mu_{i})\\). A Linear Predictor: The systematic component takes the form of \\(\\mathbf{X}\\boldsymbol{\\beta}\\). Link Function: For Poisson regression the log link is used. "],["9.2-poisson-distribution.html", "9.2 Poisson Distribution", " 9.2 Poisson Distribution To gain some intuition about the Poisson regression model consider the Poisson distribution \\[ P(\\mathrm{y}|\\mu) = \\frac{e^{-\\mu}\\mu^{\\mathrm{y}}}{\\mathrm{y}!} \\] where \\(y\\) is a random count variable \\(\\mu\\) is the expected number of times an event ocurrs \\(y! = y \\times (y-1) \\times ... \\times 1\\) is the factorical operator The Poisson distribution relies on a single parameter, \\(\\mu\\). Importantly, \\(\\mu\\) represents both the mean and the variance of the Poisson distribution (e.g. when \\(\\mu\\) is large both the mean and variance are large). https://en.wikipedia.org/wiki/Poisson_distribution What Does This Mean In Practice: As \\(\\mu\\) grows the center of the distribution shifts to right Departure of real count data from predictions from Poisson distribution Variance frequently greater than mean (overdisperion) Frequency of 0 counts exceed number predicted by Poisson "],["9.3-notes-on-interpretation.html", "9.3 Notes on Interpretation", " 9.3 Notes on Interpretation 9.3.1 One Predictor Model Consider a one-predictor Poisson regression, \\[ \\mathrm{log}(\\mu_{i}) = \\beta_{0} + \\beta_{1}x_{1i}\\\\ \\] where \\[ \\mu = \\mathrm{exp}(\\beta_{0} + \\beta_{1}x_{1}) \\] 9.3.2 Similarity to Logistic Regression Interpretation of the Poisson regression coefficients is similar to logistic regression. For example, \\(\\mathrm{exp}(\\beta_{0})\\) is the effect on the mean of \\(Y\\) when \\(\\mathbf{x}=0\\) \\(\\mathrm{exp}(\\beta_{1})\\) is the multiplicative effect on the mean of \\(Y\\) for each 1-unit difference in \\(\\mathbf{x_{1}}\\) 9.3.3 Percent Change We can also talk about these regression coefficients in terms of percent change as follows, If \\(\\beta_{1}\\) is negative: All else being equal, we might expect to see a \\((1-\\mathrm{exp}(\\beta_{1})) \\times 100\\) percent decrease in the expected count of \\(Y\\), with each additional unit increase in \\(x_{1}\\), holding constant all other variables in the model. If \\(\\beta_{1}\\) is positive: All else being equal, we might expect to see a \\((\\mathrm{exp}(\\beta_{1})-1) \\times 100\\) percent increase in the expected count of \\(Y\\), with each additional unit increase in \\(x_{1}\\), holding constant all other variables in the model. The following relationships are helpful to keep in mind, If \\(\\beta_{1}=0\\) then \\(\\mathrm{exp}(\\beta_{1}) = 1\\) the expected count, \\(\\mu=\\mathbb{E}(Y)=\\mathrm{exp}(\\beta_{1})\\) \\(Y\\) and \\(x_{1}\\) are unrelated. If \\(\\beta_{1}&gt;0\\) then \\(\\mathrm{exp}(\\beta_{1}) &gt; 1\\) the expected count, \\(\\mu=\\mathbb{E}(Y)\\), is \\(\\mathrm{exp}(\\beta_{1})\\) times larger then when \\(x_{1}=0\\). If \\(\\beta_{1}&lt;0\\) then \\(\\mathrm{exp}(\\beta_{1}) &lt; 1\\) the expected count, \\(\\mu=\\mathbb{E}(Y)\\), is \\(\\mathrm{exp}(\\beta_{1})\\) times smaller then when \\(x_{1}=0\\). Note that the parameter estimates in their original metric will describe the outcome variable in terms of log units. If we prefer to describe the phenomena in terms of the original count units we will need to use the inverse link function. "],["9.4-example-data-4.html", "9.4 Example Data", " 9.4 Example Data Increasingly researchers are taking a life-course perspective to understanding how different life stages shape a variety of later in life outcomes. In this case study, Ferraro, Schafer, and Wilkinson (2016) examine the relationship between physical health in adulthood and multiple domains of childhood disadvantage using a count regression model. Data are drawn from the National Survey of Midlife Development in the United States (MIDUS). MIDUS contains a battery of retrospective questions concerning childhood disadvantage, as well as extensive measures of adult risks and resources. Although the authors use data from both waves of MIDUS in the paper, here we focus only on their first model of adult health outcomes, which takes into account both childhood disadvantage and the mediating effects of later life resources and risk behaviors. 9.4.1 Dependent variable The dependent variable for this analysis is health problems at Wave 1: morbidityw1. Here, adult health problems are measured by the self-reported occurrence of 31 diseases or health conditions. For 29 of these items respondents were asked “In the past 12 month have you experienced or been treated for any of the following?” For the remaining 2, cancer and heart disease, respondents were asked if they had ever been diagnosed with the disease. Finally, morbidityw1 is the sum of these 31 items, where each is coded 1 for yes, and 0 for no. 9.4.2 Explanatory Variables 9.4.2.1 Early Life Disadvantage ses: Childhood SES is a sum score based on standardized measures of (1) the education for the head of household, (2) financial strain and (3) receipt of welfare. family: Family composition is a sum score based on (1) no presence of a male in the household, (2) parental divorce, and (3) death of parent prior to age 16, abuse_rare: Physical or Emotional Child abuse by parents is categorized by frequency of abuse. abuse_rare indicates respondent rarely experience one or both types of abuse. The reference category is never having experienced emotional or physical abuse. abuse_freq1: abuse_frequency1 indicates respondents frequently (sometimes or often) experienced one type of abuse during childhood. The reference category is never having experienced emotional or physical abuse. abuse_freq2: abuse_frequency2 indicates respondents frequently experienced both types of abuse during childhood. The reference category is never having experienced emotional or physical abuse. health: Adolescent health problems are measured by self-rated physical and mental health at age 16. 9.4.2.2 Adult Characteristics age: Age at time of Wave 1 interview. nonwhite: Race (white or nonwhite). female: Gender (female or male). educate: Number of years of completed education. catincome: Household income adjusted by household size and recoded into five percentile categories (&lt; 21st *percentile, 21st to 40th percentile41st to 60th percentile, 61st to 80th percentile, and &gt; 80th percentile.). a1sj6: Financial strain during adulthood; responses range from 1 (no difficulty paying monthly bills) to 3 (very difficult to pay monthly bills). smoke_dose: Lifetime smoking is calculated from information reported by respondents: age when started smoking, year stopped (for former smokers), and average number of cigarettes smoked daily. Using a yearly metric, lifetime smoking is the product of years smoked and annual number of cigarettes, divided by 10,000 (see Footnote 11, p. 130). heavydr2: The measurement of heavy drinking is sex differentiated and tapped respondents’ period of greatest lifetime consumption: five or more drinks per day for men and four or more drinks for women. obese: Obesity, dummy variable coded 1 if body mass index [kg/m2] &gt; 30. fampos: Family support as measured by four items reflecting the presence of positive relationship characteristics. friendpos: Friend support as measured by four items reflecting the presence of positive relationship characteristics. famneg: Family strain as measured by four items reflecting the presence of difficult relationship characteristics. friendneg: Friend strain as measured by four items reflecting the presence of difficult relationship characteristics. integration: Social integration as measured by three 7-item Likert–type questions. ever_divor: Ever divorced, a dummy variable coded 1 if the respondent reported having been divorced. controlw1: Average score for a 12-item index of the respondent’s feelings of personal control. References "],["9.5-single-predictor-model-1.html", "9.5 Single Predictor Model", " 9.5 Single Predictor Model 9.5.1 Read in Data We can read in the data and create a mean-centered version of income. ferraro2016 &lt;- read.csv(&quot;data/ferraro2016.csv&quot;) ferraro2016$income_star &lt;- as.numeric(scale(ferraro2016$catincome, scale = FALSE)) 9.5.2 Single Predictor Model in GLM Let’s fit a single predictor Poisson regression model for morbidity at Wave 1 using income bracket as our predictor. For now let’s treat income bracket as a continuous predictor. Note for the Poisson regression we use family = poisson(link=log) argument. model1 &lt;- glm( formula = morbidityw1 ~ 1 + income_star, family = poisson(link=log), data = ferraro2016, na.action = na.exclude ) summary(model1) ## ## Call: ## glm(formula = morbidityw1 ~ 1 + income_star, family = poisson(link = log), ## data = ferraro2016, na.action = na.exclude) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.007472 0.011047 91.195 &lt; 2e-16 *** ## income_star -0.032445 0.007838 -4.139 3.48e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 8166.6 on 2994 degrees of freedom ## Residual deviance: 8149.5 on 2993 degrees of freedom ## (27 observations deleted due to missingness) ## AIC: 14986 ## ## Number of Fisher Scoring iterations: 5 9.5.3 Deviance and Goodness of Fit summary(model1) ## ## Call: ## glm(formula = morbidityw1 ~ 1 + income_star, family = poisson(link = log), ## data = ferraro2016, na.action = na.exclude) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.007472 0.011047 91.195 &lt; 2e-16 *** ## income_star -0.032445 0.007838 -4.139 3.48e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 8166.6 on 2994 degrees of freedom ## Residual deviance: 8149.5 on 2993 degrees of freedom ## (27 observations deleted due to missingness) ## AIC: 14986 ## ## Number of Fisher Scoring iterations: 5 We can think about the deviance as a measure of how well the model fits the data. If the model fits well, the observed values \\(Y_{i}\\) will be close to their predicted means \\(\\mu_{i}\\), causing the deviance to be small. If this value greatly exceeds one, it may be indicative of overdispersion. The rationale for this heuristic is based on the fact that the residual deviance is \\(\\chi^2_k\\) distributed with mean equal to the degrees of freedom (n-p). Instead of using this rule of thumb it is just as simple to formulate a goodness-of-fit test for our model as follows 1 - pchisq(summary(model1)$deviance, summary(model1)$df.residual ) ## [1] 0 The test of the model’s deviance is a test of the model against the null model. The null hypothesis is the fitted model fits the data as well as the saturated model. In this case, rejecting the null hypothesis (e.g. \\(p &gt; 0.05\\)), indicates the Poisson model does not fit well. For more information see this wonderful post on stackexchange: https://stats.stackexchange.com/a/531484. 9.5.4 Overdispersion? The deviance statistic suggests there may be a problem with overdispersion? Overdispersion indicates there is greater variability in the data than would be expected based on the model. Overdispersion is often encountered when fitting simple Poisson regression models. The Poisson distribution has one free parameter and does not allow for the variance to be adjusted independently of the mean. If overdispersion is present the resultant model may yield biased parameter estimates and underestimated standard errors, possibly leading to invalid conclusions. 9.5.5 Interpretation of Single Predictor Model How do we interpret the coefficients from our single predictor poisson regression? In the classic linear regression model we might interpret a slope coefficient as characterizing the change in mean number of health problems for a 1-unit increase in income. In our Poisson regression we are modeling the log of the mean number of health problems, so we must convert back to original units. Intercept in Log Units This is the Poisson regression estimate when all variables in the model are evaluated at zero. For our model, we have centered the income variable. This means for an individual with an average income level the log of the expected count for health problems is \\(1.007\\) units. Intercept in Count Units We can also exponentiate the intercept, \\(exp(1.007) = 2.7\\) indicating that at Wave 1 follow-up, an individual with an average income level is expected to have approximately \\(2.7\\) health problems. 9.5.5.1 Slope Coefficient Slope Coefficient in Log Units Within our single predictor model, \\(b_1\\) is the difference in log number of health problems for a 1-level difference in income bracket. For example, consider a comparison of two models—one for a given income bracket \\((x)\\), and one income bracket higher \\((x+1)\\): \\[\\begin{equation} \\begin{split} log(\\mu_X) &amp;= \\beta_0 + \\beta_1X \\\\ log(\\mu_{X+1}) &amp;= \\beta_0 + \\beta_1(X+1) \\\\ log(\\mu_{X+1})-log(\\mu_X) &amp;= \\beta_1 \\\\ log \\left(\\frac{\\mu_{X+1}}{\\mu_X}\\right) &amp;= \\beta_1\\\\ \\frac{\\mu_{X+1}}{\\mu_X} &amp;= e^{\\beta_1} \\end{split} \\tag{4.1} \\end{equation}\\] These results suggest that by exponentiating the coefficient of income, we obtain the multiplicative factor by which the mean count of health problems change. In this case, the mean number of health problems changes by a factor of \\(e^{(-0.032445)}=0.968\\). We might also think about percent change as in Section 9.3.3. Here we would \\(1-e^{(-0.032445)}*100=3.19\\) percent decrease in the number of health problems for a 1-unit increase in income band. To calculate the percentage change you can use the formulas in Section 9.3.3 or use the catregs package on github. # manual #(1-exp(coef(model1)[2]))*100 # if slope is negative, % decrease #(exp(coef(model1)[1])-1)*100 # if slope is positive, % increase # using catregs #devtools::install_github(&quot;dmmelamed/catregs&quot;) catregs::list.coef(model1) ## $out ## variables b SE z ll ul p.val exp.b ll.exp.b ul.exp.b ## 1 (Intercept) 1.007 0.011 91.195 0.986 1.029 0 2.739 2.680 2.799 ## 2 income_star -0.032 0.008 -4.139 -0.048 -0.017 0 0.968 0.953 0.983 ## percent CI ## 1 173.867 95 % ## 2 -3.192 95 % Marginal Effects Let’s turn to the marginaleffects (Arel-Bundock 2022) package to look at the marginal effects of income on health. marginaleffects::plot_cap(model1, condition = c(&quot;income_star&quot;), conf.int = TRUE) ## Warning: This function has been renamed to `plot_predictions()`. The `plot_cap()` ## alias will be removed in the near future. References "],["9.6-multiple-predictor-model.html", "9.6 Multiple Predictor Model", " 9.6 Multiple Predictor Model Let’s add another variable into the model. Specifically, the variable abuse_rare, which equals \\(1\\) if the child was rarely abused during early development, and \\(0\\) if the child experienced abuse. ferraro2016$abuse_rare &lt;- factor(ferraro2016$abuse_rare) model2 &lt;- glm( formula = morbidityw1 ~ 1 + abuse_rare + income_star + abuse_rare:income_star, family = poisson(link=log), data = ferraro2016, na.action = na.exclude ) summary(model2) ## ## Call: ## glm(formula = morbidityw1 ~ 1 + abuse_rare + income_star + abuse_rare:income_star, ## family = poisson(link = log), data = ferraro2016, na.action = na.exclude) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.0766511 0.0129647 83.045 &lt; 2e-16 *** ## abuse_rare1 -0.2382614 0.0251265 -9.482 &lt; 2e-16 *** ## income_star -0.0311089 0.0091684 -3.393 0.000691 *** ## abuse_rare1:income_star -0.0000822 0.0179350 -0.005 0.996343 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 8068.4 on 2966 degrees of freedom ## Residual deviance: 7956.7 on 2963 degrees of freedom ## (55 observations deleted due to missingness) ## AIC: 14737 ## ## Number of Fisher Scoring iterations: 5 As the model becomes more complicated it can be helpful to write out the equation, \\[ \\mathrm{log}(morbidity_i) = b_0 + b_1(abuse\\_rare_{i})+ b_2(income^{*}_{1i})+ b_3(income^{*}_{1i})(abuse\\_rare_{i}) \\] Again, when we have a dummy variable interaction we have essentially set up two different equations: Equation for Children Experiencing Maltreatment Coefficients \\(b_0\\) and \\(b_2\\) describe the relation between \\(income^{*}_{1i}\\) and \\(morbidity_i\\) for those who experienced childhood abuse \\((abuse\\_rare_{i}=0)\\). \\[ \\mathrm{log}(morbidity_i) = b_0 + b_2(income^{*}_{1i}) \\] Equation for Children Rarely Experiencing Maltreatment \\[ \\mathrm{log}(morbidity_i) = (b_0 + b_1) + (b_2 + b_3)income^{*}_{1i} \\] While coefficients \\(b_0 + b_1\\) and \\(b_2 + b_3\\) describe the relation for those who rarely experienced childhood abuse \\((abuse\\_rare_{i}=1)\\). Note that \\(b_3\\) is not significantly different from zero, so we would not interpret the interaction between income and abuse directly. Marginal Effects Let’s again turn to the marginaleffects (Arel-Bundock 2022) package to look at the marginal effects of income and childhood abuse on health. marginaleffects::plot_predictions(model2, condition = c(&quot;income_star&quot;,&quot;abuse_rare&quot;), conf.int = TRUE) References "],["9.7-revisisting-overdispersion.html", "9.7 Revisisting Overdispersion", " 9.7 Revisisting Overdispersion Let’s run our model fit test based on the deviance for model2. 1 - pchisq(summary(model2)$deviance, summary(model2)$df.residual ) ## [1] 0 Again, we reject the hypothesis of a close fit between model and data. To gain a little more insight we can plot estimates of the variance against the expected value, alongside a line with an intercept of zero and a slope of 1. We expect the data points to fall somewhat evenly along that line. Here, it appears our variance is consistently larger than our mean, indicating the possibility of overdispersion. plot( log(fitted(model2)), log((ferraro2016$morbidityw1-fitted(model2))^2), xlab=expression(hat(mu)), ylab=expression((y-hat(mu))^2), pch=20,col=&quot;blue&quot; ) abline(0,1) ## &#39;varianc = mean&#39; line We can also run some formal tests for overdispersion. For example, using the AER package we can estimate an overdispersion parameter and test whether or not it equals zero. AER::dispersiontest(model2,trafo=1) ## Warning in y - yhat: longer object length is not a multiple of shorter object ## length ## Warning in (y - yhat)^2 - y: longer object length is not a multiple of shorter ## object length ## ## Overdispersion test ## ## data: model2 ## z = 13.22, p-value &lt; 2.2e-16 ## alternative hypothesis: true alpha is greater than 0 ## sample estimates: ## alpha ## 1.986701 Here we clearly see that there is evidence of overdispersion (\\(\\alpha\\) is estimated to be 1.986) which speaks quite strongly against the assumption of equidispersion (i.e. \\(\\alpha=0\\)). One way to handle this is to fit a estimate with overdispersion parameter directly in the model using a quassipoison approach. 9.7.1 Quassi-Poisson Family If we want to test and adjust for overdispersion we can add a scale parameter with the family=quasipoisson option. The estimated scale parameter will be labeled as Overdispersion parameter in the output. model3 &lt;- glm( formula = morbidityw1 ~ 1 + abuse_rare + income_star + abuse_rare:income_star, family = quasipoisson(link=log), data = ferraro2016, na.action = na.exclude ) summary(model3) ## ## Call: ## glm(formula = morbidityw1 ~ 1 + abuse_rare + income_star + abuse_rare:income_star, ## family = quasipoisson(link = log), data = ferraro2016, na.action = na.exclude) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.0766511 0.0217425 49.518 &lt; 2e-16 *** ## abuse_rare1 -0.2382614 0.0421387 -5.654 1.71e-08 *** ## income_star -0.0311089 0.0153759 -2.023 0.0431 * ## abuse_rare1:income_star -0.0000822 0.0300781 -0.003 0.9978 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for quasipoisson family taken to be 2.812535) ## ## Null deviance: 8068.4 on 2966 degrees of freedom ## Residual deviance: 7956.7 on 2963 degrees of freedom ## (55 observations deleted due to missingness) ## AIC: NA ## ## Number of Fisher Scoring iterations: 5 The new standard errors (in comparison to the model without the overdispersion parameter), are larger, Thus, the Wald statistics will be smaller and less likely to be significant. We could also use a likelihood-ratio test to test whether a quasipoisson GLM with overdispersion is significantly better than a regular poisson GLM without overdispersion : pchisq(summary(model3)$dispersion * model2$df.residual, model2$df.residual, lower = FALSE) ## [1] 0 This results suggest we would reject the null hypothesis of the Poisson regression being a superior fit to the quasipoisson approach, providing evidence for the quasipoisson model. 9.7.2 Marginal Effects Examples Fit our main model. model4 &lt;- glm( formula = morbidityw1 ~ 1 + health + abuse_rare + income_star + abuse_rare:income_star, family = quasipoisson(link=log), data = ferraro2016, na.action = na.exclude ) summary(model4) ## ## Call: ## glm(formula = morbidityw1 ~ 1 + health + abuse_rare + income_star + ## abuse_rare:income_star, family = quasipoisson(link = log), ## data = ferraro2016, na.action = na.exclude) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.959e-01 2.883e-02 34.541 &lt; 2e-16 *** ## health 1.855e-01 4.209e-02 4.406 1.09e-05 *** ## abuse_rare1 -2.215e-01 4.201e-02 -5.273 1.43e-07 *** ## income_star -2.637e-02 1.529e-02 -1.724 0.0848 . ## abuse_rare1:income_star 7.307e-05 2.984e-02 0.002 0.9980 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for quasipoisson family taken to be 2.769737) ## ## Null deviance: 8068.4 on 2966 degrees of freedom ## Residual deviance: 7904.2 on 2962 degrees of freedom ## (55 observations deleted due to missingness) ## AIC: NA ## ## Number of Fisher Scoring iterations: 5 In GLM models, most quantities of interest are conditional, in the sense that they will typically depend on the values of all the predictors in the model. Therefore, we need to decide where in the predictor space we want to evaluate the quantity of interest described above. 9.7.3 References "],["10-chapter-10.html", "Chapter 10 Marginal Effects Hello ", " Chapter 10 Marginal Effects In Chapter 10 we will dig a little deeper into the marginaleffects (Arel-Bundock 2022) R package. References "],["10.1-reintroducing-marginal-effects.html", "10.1 Reintroducing Marginal Effects", " 10.1 Reintroducing Marginal Effects Marginal effects are partial derivatives of the regression equation with respect to each variable in the model for each unit in the data. Said in a different way, a marginal effect captures the association between a change in an explanatory variable and a change in a response. In fact, the marginal effect is the slope of this prediction function, measured at a specific value of the explanatory variable. Big picture: Marginal effects use model predictions to aid us in our interpretation of a fitted model, while also providing a framework to visualize and test our results. Linear Models In linear models (without interactions, quadratic terms, etc.) the effect of a given change in an independent variable is the same regardless of (1) the value of that variable at the start of its change, and (2) the level of the other variables in the model. Nonlinear Models In nonlinear models the effect of a given change in an independent variable (1) depends on the values of other variables in the model, and (2) is no longer equal to the parameter itself. Contrast Linear and Nonlinear Models Consider a linear and nonlinear model for happiness as a function of personal spending and a dummy variable indicating whether someone is rich. A Few Observations On the Image Above For the linear model: - Whether one is rich or poor does no impact the relationship between happiness and personal spending. - Differences in happiness levels between rich and poor are not dependent on the amount of money one spends. From the nonlinear model: - Whether one is rich or poor does impact the relationship between happiness and personal spending. - Differences in happiness levels between rich and poor are dependent on the amount of money one spends. Continuous and Categorical Predictors Figure from https://www.andrewheiss.com/blog/2022/05/20/marginalia/ Continuous Variable: the statistical effect for continuous explanatory variables; the partial derivative of a variable in a regression model; the effect of a single slider Categorical Variable: the statistical effect for categorical explanatory variables; the difference in means when a condition is on vs. when it is off; the effect of a single switch Types of Marginal Effects There are generally three types of marginal effects people consider: Average Marginal Effects (AME) Marginal Effects at the Means (MEM) Marginal Effects at Representative Values (MEM) We won’t dive into the nuances of these differences here, but interested parties should take a look at the amazing Marginalia Website from Andrew Heiss. The three features of marginal effects we will explore in class: Slopes Predictions Hypothesis Tests "],["10.2-data-from-ferraro-et-al.-2016.html", "10.2 Data from Ferraro et al. (2016)", " 10.2 Data from Ferraro et al. (2016) Let’s take another look at the data from Ferraro, Schafer, and Wilkinson (2016) examining physical health in adulthood using a count regression model. Data are drawn from the National Survey of Midlife Development in the United States (MIDUS). MIDUS contains a battery of retrospective questions concerning childhood disadvantage, as well as extensive measures of adult risks and resources. Although the authors use data from both waves of MIDUS in the paper, here we focus only on their first model of adult health outcomes, which takes into account both childhood disadvantage and the mediating effects of later life resources and risk behaviors. Outcome Variable morbidityw1: Self-reported occurrence of 31 diseases or health conditions Explanatory Variables health: Adolescent health problems are measured by self-rated physical and mental health at age 16. age: Age at time of Wave 1 interview. female: Logical indicating if the subject identified as female. smoke_dose: Lifetime smoking is calculated from information reported by respondents: age when started smoking, year stopped (for former smokers), and average number of cigarettes smoked daily. Using a yearly metric, lifetime smoking is the product of years smoked and annual number of cigarettes, divided by 10,000 (see Footnote 11, p. 130). heavydr2: The measurement of heavy drinking is sex differentiated and tapped respondents’ period of greatest lifetime consumption: five or more drinks per day for men and four or more drinks for women. obese: Obesity, dummy variable coded 1 if body mass index [kg/m2] &gt; 30. fampos: Family support as measured by four items reflecting the presence of positive relationship characteristics. friendpos: Friend support as measured by four items reflecting the presence of positive relationship characteristics. abuse_rare: Physical or Emotional Child abuse by parents is categorized by frequency of abuse. abuse_rare indicates respondent rarely experience one or both types of abuse. The reference category is never having experienced emotional or physical abuse. abuse_freq1: abuse_frequency1 indicates respondents frequently (sometimes or often) experienced one type of abuse during childhood. The reference category is never having experienced emotional or physical abuse. abuse_freq2: abuse_frequency2 indicates respondents frequently experienced both types of abuse during childhood. The reference category is never having experienced emotional or physical abuse. References "],["10.3-fitting-a-quasi-poisson-model.html", "10.3 Fitting a Quasi-Poisson Model", " 10.3 Fitting a Quasi-Poisson Model We will now fit a Quasi-Poisson regression model to the Ferraro et al. (2016) data. As described previously the Quasi-Poisson model is a generalization of the Poisson regression and is used when modeling an overdispersed count variable. ferraro2016 &lt;- read.csv(&quot;data/ferraro2016.csv&quot;) ferraro2016$female &lt;- as.factor(ferraro2016$female) ferraro2016$obese &lt;- as.factor(ferraro2016$obese) ferraro2016$abuse_rare &lt;- as.factor(ferraro2016$abuse_rare) ferraro2016$abuse_freq1 &lt;- as.factor(ferraro2016$abuse_freq1) ferraro2016$abuse_freq2 &lt;- as.factor(ferraro2016$abuse_freq2) model4 &lt;- glm( formula = morbidityw1 ~ 1 + female + health + age + smoke_dose + heavydr2 + obese + fampos + friendpos + abuse_rare + abuse_freq1 + abuse_freq2, family = quasipoisson(link=log), data = ferraro2016, na.action = na.exclude ) summary(model4) ## ## Call: ## glm(formula = morbidityw1 ~ 1 + female + health + age + smoke_dose + ## heavydr2 + obese + fampos + friendpos + abuse_rare + abuse_freq1 + ## abuse_freq2, family = quasipoisson(link = log), data = ferraro2016, ## na.action = na.exclude) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.2792560 0.1439042 1.941 0.05241 . ## female1 0.3284639 0.0384021 8.553 &lt; 2e-16 *** ## health 0.1665501 0.0419736 3.968 7.43e-05 *** ## age 0.0153301 0.0015396 9.957 &lt; 2e-16 *** ## smoke_dose 0.0055588 0.0008907 6.241 5.02e-10 *** ## heavydr2 0.1136408 0.0440990 2.577 0.01002 * ## obese1 0.2639706 0.0394117 6.698 2.56e-11 *** ## fampos -0.0885551 0.0314942 -2.812 0.00496 ** ## friendpos -0.0711315 0.0282620 -2.517 0.01190 * ## abuse_rare1 -0.0118142 0.0503028 -0.235 0.81433 ## abuse_freq11 0.0903667 0.0557820 1.620 0.10535 ## abuse_freq21 0.2731073 0.0519243 5.260 1.55e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for quasipoisson family taken to be 2.443076) ## ## Null deviance: 7403.7 on 2754 degrees of freedom ## Residual deviance: 6414.4 on 2743 degrees of freedom ## (267 observations deleted due to missingness) ## AIC: NA ## ## Number of Fisher Scoring iterations: 5 Remember, we can interpret these coefficients just as we would regression coefficients, however, we would be speaking in terms of the log of the mean count. Broadly, we may be interested in (1) predictions, (2) slopes and (3) hypothesis tests. Let’s briefly review functionality for each of these areas. "],["10.4-marginaleffects-slopes.html", "10.4 marginaleffects: Slopes", " 10.4 marginaleffects: Slopes Slopes are defined as the partial derivatives of the regression equation with respect to an explanatory variable of interest (i.e. a marginal effect or trend). In the remainder of this section we use the terms slope and marginal effect interchangeably. The marginal effect is a unit- (or person-level) measure of association between changes in a regressor and changes in the response. In a simple linear model the marginal effects will be the same from individual to individual. In anything more complex the marginal effect will vary by individual, because it depends on the values of the other covariates for each individual. The slopes() function produces unique estimates of the marginal effect for each row of the data used to fit the model. The output of slopes() is a simple data.frame. library(marginaleffects) head(slopes(model4)) ## ## Term Contrast Estimate Std. Error z Pr(&gt;|z|) S 2.5 % 97.5 % ## abuse_freq1 1 - 0 0.590 0.382 1.55 0.122 3.0 -0.1576 1.338 ## abuse_freq1 1 - 0 0.406 0.264 1.54 0.124 3.0 -0.1113 0.923 ## abuse_freq1 1 - 0 0.746 0.482 1.55 0.122 3.0 -0.1984 1.690 ## abuse_freq1 1 - 0 0.625 0.402 1.56 0.119 3.1 -0.1617 1.413 ## abuse_freq1 1 - 0 0.410 0.267 1.54 0.124 3.0 -0.1122 0.933 ## abuse_freq1 1 - 0 0.420 0.263 1.60 0.110 3.2 -0.0955 0.936 ## ## Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, morbidityw1, female, health, age, smoke_dose, heavydr2, obese, fampos, friendpos, abuse_rare, abuse_freq1, abuse_freq2 ## Type: response Average marginal effects (AME) To obtain the AMEs we generate predictions for each row of the original data, then collapse to averages. avg_slopes(model4) ## ## Term Contrast Estimate Std. Error z Pr(&gt;|z|) S 2.5 % 97.5 % ## abuse_freq1 1 - 0 0.2521 0.16023 1.573 0.11566 3.1 -0.0620 0.5661 ## abuse_freq2 1 - 0 0.7936 0.16233 4.889 &lt; 0.001 19.9 0.4755 1.1118 ## abuse_rare 1 - 0 -0.0319 0.13557 -0.235 0.81383 0.3 -0.2976 0.2338 ## age dY/dX 0.0415 0.00424 9.799 &lt; 0.001 72.9 0.0332 0.0498 ## fampos dY/dX -0.2399 0.08544 -2.808 0.00498 7.6 -0.4074 -0.0725 ## female 1 - 0 0.8839 0.10324 8.562 &lt; 0.001 56.3 0.6816 1.0863 ## friendpos dY/dX -0.1927 0.07665 -2.514 0.01193 6.4 -0.3429 -0.0425 ## health dY/dX 0.4512 0.11400 3.958 &lt; 0.001 13.7 0.2278 0.6747 ## heavydr2 1 - 0 0.3169 0.12657 2.503 0.01230 6.3 0.0688 0.5649 ## obese 1 - 0 0.7599 0.12066 6.298 &lt; 0.001 31.6 0.5234 0.9964 ## smoke_dose dY/dX 0.0151 0.00243 6.202 &lt; 0.001 30.7 0.0103 0.0198 ## ## Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high ## Type: response Note that since marginal effects are derivatives, they are only properly defined for continuous numeric variables. When the model also includes categorical regressors, the summary function will try to display relevant (regression-adjusted) contrasts between different categories, as shown above. Group-Average Marginal Effect (G-AME) We can also use the by argument the average marginal effects within different subgroups of the observed data, based on values of the regressors. For example, to compute the average marginal effects of age on the number of health problems for those who experienced no childhood maltreatment and those who experienced both physical and emotional abuse: avg_slopes( model4, by = &quot;abuse_freq2&quot;, variables = &quot;age&quot; ) ## ## Term Contrast abuse_freq2 Estimate Std. Error z Pr(&gt;|z|) S 2.5 % ## age mean(dY/dX) 0 0.0384 0.00394 9.74 &lt;0.001 72.1 0.0307 ## age mean(dY/dX) 1 0.0524 0.00556 9.43 &lt;0.001 67.7 0.0415 ## 97.5 % ## 0.0461 ## 0.0633 ## ## Columns: term, contrast, abuse_freq2, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted ## Type: response We can also plot these slopes: plot_slopes( model4, condition = &quot;abuse_freq2&quot;, variables = &quot;age&quot; ) Marginal Effect at User-Specified Values Sometimes, we are only interested in looking at the estimated marginal effects for certain types of individuals, or for user-specified values of the regressors. The datagrid function from marginaleffects allows us to build a data grid containing values that are of interest to us. For example, let’s look at the effect of experiencing both childhood emotional and physical abuse on the number of adult health problems for a 30-year old male who is obese. avg_slopes( model4, variables = &quot;abuse_freq2&quot;, newdata = datagrid( age = 30, obese = 1, female = 0 ) ) ## ## Term Contrast Estimate Std. Error z Pr(&gt;|z|) S 2.5 % 97.5 % ## abuse_freq2 1 - 0 0.59 0.114 5.16 &lt;0.001 21.9 0.366 0.814 ## ## Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted ## Type: response Hypothesis Tests We also might be interested in hypothesis tests based on our model. For example, compared to those who did not experience childhood trauma is the effect of experiencing emotional and physical abuse during childhood on adult health problems different depending on whether an individual experienced both, or just one of these type of abuse. hypotheses(model4, hypothesis = &quot;abuse_freq11 = abuse_freq21&quot; ) ## ## Term Estimate Std. Error z Pr(&gt;|z|) S 2.5 % ## abuse_freq11 = abuse_freq21 -0.183 0.0544 -3.36 &lt;0.001 10.3 -0.289 ## 97.5 % ## -0.076 ## ## Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high "],["10.5-marginaleffects-predictions.html", "10.5 marginaleffects: Predictions", " 10.5 marginaleffects: Predictions Predictions In the context of these examples we will consider predictions to be the predicted value of an outcome of interest produced from a fitted model for a given scale and combination of values of the predictor variables. For example, let’s take a look at the predicted number of health problems for the first six individuals in the sample. First, let’s get the predicted number of health problems on the type = \"link\" scale: library(marginaleffects) pred &lt;- predictions(model4, type = &quot;link&quot;) head(pred) ## ## Estimate Std. Error z Pr(&gt;|z|) S 2.5 % 97.5 % ## 1.83 0.0857 21.4 &lt;0.001 334.4 1.66 2.00 ## 1.46 0.0664 21.9 &lt;0.001 351.7 1.33 1.59 ## 2.07 0.0722 28.6 &lt;0.001 595.9 1.92 2.21 ## 1.89 0.0993 19.0 &lt;0.001 265.5 1.69 2.08 ## 1.47 0.0818 17.9 &lt;0.001 236.8 1.31 1.63 ## 1.49 0.0659 22.6 &lt;0.001 373.8 1.36 1.62 ## ## Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, morbidityw1, female, health, age, smoke_dose, heavydr2, obese, fampos, friendpos, abuse_rare, abuse_freq1, abuse_freq2 ## Type: link Or we can look at the type = \"response\" scale (e.g. raw count): pred &lt;- predictions(model4, type = &quot;response&quot;) head(pred) ## ## Estimate Std. Error z Pr(&gt;|z|) S 2.5 % 97.5 % ## 6.24 0.535 11.7 &lt;0.001 102.2 5.19 7.29 ## 4.29 0.285 15.0 &lt;0.001 167.6 3.73 4.85 ## 7.89 0.569 13.9 &lt;0.001 142.6 6.77 9.00 ## 6.61 0.657 10.1 &lt;0.001 76.8 5.33 7.90 ## 4.34 0.355 12.2 &lt;0.001 111.7 3.64 5.04 ## 4.44 0.293 15.2 &lt;0.001 170.2 3.87 5.02 ## ## Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, morbidityw1, female, health, age, smoke_dose, heavydr2, obese, fampos, friendpos, abuse_rare, abuse_freq1, abuse_freq2 ## Type: response We may also be interested in looking at what is often called the Adjusted Prediction at the Mean (APM). The APM is calculated with all continuous regressors held at their mean, and all categorical variables held at their mode. To do this we can use the datagrid function in marginaleffects as in predictions(model4, newdata = &quot;mean&quot;, type = &quot;response&quot;) ## ## Estimate Std. Error z Pr(&gt;|z|) S 2.5 % 97.5 % female health age ## 2.6 0.106 24.4 &lt;0.001 436.1 2.39 2.81 1 0.387 47 ## smoke_dose heavydr2 obese fampos friendpos abuse_rare abuse_freq1 abuse_freq2 ## 11.9 0.247 0 3.42 3.22 0 0 0 ## ## Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, morbidityw1, female, health, age, smoke_dose, heavydr2, obese, fampos, friendpos, abuse_rare, abuse_freq1, abuse_freq2 ## Type: response Beyond Individual and Aggregate Predictions It can often be more interested to look Average Adjusted Predictions (AAP) for different subgroupings of our data. The marginaleffects package calculates AAPs in the following manner: Create a new dataset with each of the original regressor values, but fixing some regressors to values of interest. Take the average of the predicted values in this new dataset. We can obtain AAPs by applying the avg_predictions() functions or by argument: avg_predictions(model4, , type = &quot;response&quot;) ## ## Estimate Std. Error z Pr(&gt;|z|) S 2.5 % 97.5 % ## 2.71 0.049 55.3 &lt;0.001 Inf 2.61 2.81 ## ## Columns: estimate, std.error, statistic, p.value, s.value, conf.low, conf.high ## Type: response We can compute average adjusted predictions for different subsets of the data with the by argument. avg_predictions(model4, by = c(&quot;abuse_freq2&quot;, &quot;obese&quot;, &quot;female&quot;), type = &quot;response&quot; ) ## ## abuse_freq2 obese female Estimate Std. Error z Pr(&gt;|z|) S 2.5 % 97.5 % ## 0 0 0 2.00 0.0646 31.0 &lt;0.001 698.5 1.88 2.13 ## 0 0 1 2.59 0.0750 34.5 &lt;0.001 863.9 2.44 2.73 ## 0 1 0 2.71 0.1104 24.5 &lt;0.001 438.5 2.49 2.92 ## 0 1 1 3.57 0.1353 26.4 &lt;0.001 507.9 3.31 3.84 ## 1 0 0 2.70 0.1133 23.9 &lt;0.001 415.7 2.48 2.93 ## 1 0 1 3.54 0.1408 25.1 &lt;0.001 460.8 3.26 3.81 ## 1 1 0 3.89 0.1887 20.6 &lt;0.001 310.9 3.52 4.26 ## 1 1 1 4.56 0.2112 21.6 &lt;0.001 340.7 4.14 4.97 ## ## Columns: abuse_freq2, obese, female, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high ## Type: response Counterfactuals In the next example, we create a counterfactual data grid where each observation of the dataset is repeated twice, with different values of the abuse_freq2 variable, and all other variables held at the observed values. predictions( model4, type = &quot;response&quot;, by = &quot;abuse_freq2&quot;, newdata = datagrid(abuse_freq2 = 0:1, grid_type = &quot;counterfactual&quot;) ) ## ## abuse_freq2 Estimate Std. Error z Pr(&gt;|z|) S 2.5 % 97.5 % ## 0 2.53 0.0572 44.2 &lt;0.001 Inf 2.42 2.64 ## 1 3.32 0.1410 23.5 &lt;0.001 404.9 3.04 3.60 ## ## Columns: abuse_freq2, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high ## Type: response Hypothesis Tests Importantly, we can also conduct hypothesis tests using these same ideas. For example, are the mean number of adult health problems equal for those who experienced no childhood abuse, compared to those who experienced both emotional and physical abuse? avg_predictions(model4, by = &quot;abuse_freq2&quot;, hypothesis = &quot;b1 = b2&quot;, type = &quot;response&quot; ) ## ## Term Estimate Std. Error z Pr(&gt;|z|) S 2.5 % 97.5 % ## b1=b2 -0.913 0.128 -7.11 &lt;0.001 39.7 -1.16 -0.661 ## ## Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high ## Type: response Thinking back on our previous grid of predicted values: avg_predictions(model4, by = c(&quot;abuse_freq2&quot;, &quot;obese&quot;, &quot;female&quot;), type = &quot;response&quot; ) ## ## abuse_freq2 obese female Estimate Std. Error z Pr(&gt;|z|) S 2.5 % 97.5 % ## 0 0 0 2.00 0.0646 31.0 &lt;0.001 698.5 1.88 2.13 ## 0 0 1 2.59 0.0750 34.5 &lt;0.001 863.9 2.44 2.73 ## 0 1 0 2.71 0.1104 24.5 &lt;0.001 438.5 2.49 2.92 ## 0 1 1 3.57 0.1353 26.4 &lt;0.001 507.9 3.31 3.84 ## 1 0 0 2.70 0.1133 23.9 &lt;0.001 415.7 2.48 2.93 ## 1 0 1 3.54 0.1408 25.1 &lt;0.001 460.8 3.26 3.81 ## 1 1 0 3.89 0.1887 20.6 &lt;0.001 310.9 3.52 4.26 ## 1 1 1 4.56 0.2112 21.6 &lt;0.001 340.7 4.14 4.97 ## ## Columns: abuse_freq2, obese, female, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high ## Type: response We can test for the equivalence of any two rows of this grid using the \"b1=b2\" notation where the numbers match the rows used for our desired comparison. For example, let’s say we want to test whether there is a difference in average number of health problems between obese and non-obese females who experienced child maltreatment (e.g.\"b6=b8\") avg_predictions(model4, by = c(&quot;abuse_freq2&quot;, &quot;obese&quot;, &quot;female&quot;), type = &quot;response&quot;, hypothesis = &quot;b6 = b8&quot; ) ## ## Term Estimate Std. Error z Pr(&gt;|z|) S 2.5 % 97.5 % ## b6=b8 -1.02 0.172 -5.92 &lt;0.001 28.2 -1.36 -0.682 ## ## Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high ## Type: response Plotting Predictions It is often easier to visualize quantities of interest. To visualize predicted values we can use the plot_predictions() function from marginaleffects. For example, lets visualize some of the predictions we analyzed previously. plot_predictions( model4, condition = c(&quot;age&quot;, &quot;obese&quot;,&quot;female&quot;,&quot;abuse_freq2&quot;), type = &quot;response&quot; ) We can also use some built-in arguments to look at the predicted number of health problems by respondent age and whether they were at mean, 1 SD below, or 1 SD above in terms of cigarette exposure. Notice, the plot_predictions() function is a ggplot2 object. This means we can modify the plot as needed using ggplot2 syntax. library(ggplot2) plot_predictions( model4, condition = list(&quot;age&quot;,&quot;smoke_dose&quot;=&quot;threenum&quot;), type = &quot;response&quot; ) + theme_classic() + ggtitle(&quot;Adult Health Problems by Age and Smoking Status&quot;) + xlab(&quot;Age&quot;) + ylab(&quot;Number of Health Problems&quot;) "],["10.6-references-3.html", "10.6 References", " 10.6 References "],["11-chapter-11.html", "Chapter 11 Two-Occassion Change", " Chapter 11 Two-Occassion Change In this section we will introduce longitudinal models in their most basic form: repeated measures collected at two occasions. Two-occasions data are a natural starting point for studying change: all longitudinal data collection begins with two occasions. Given any two-occasion repeated measures data, popular analytic approaches include the ANCOVA (autoregressive) and difference score models. This script works through some basic representations of change, more specifically (1) autoregressive models of change, and (2) difference-score models of change. These two models consider and answer different kinds of research questions: questions about change in interindividual differences and questions about intraindividual change. "],["11.1-introduction.html", "11.1 Introduction", " 11.1 Introduction 11.1.1 A Thought Experiment Now that we are considering repeated measures data it is helpful to thinking about what repeated measures buy us. Here are a few thoughts: With cross-sectional data: No sense of passage of time Cannot control for what happened at earlier timepoints Causality (e.g. cause precedes the effect, cause related to effect, plausible alternatives) Depending on true change process, cross-sectional slice may mislead Must rely more heavily on theory (e.g. equivalent models (MacCallum et al. 1993)) Equivalent Models and Time Dependence In addition to allowing us to model processes unfolding across time, repeated measures also allow us to address an important problem in applied modeling: the existence of equivalent models. Equivalent Models: An Example Excerpt from MacCallum et al. (1993) Examples of Equivalent Models Figure 3 from MacCallum et al. (1993) Now image these variables were collected at four different waves. This would eliminate the possibility of equivalent models under this scenario. References "],["11.2-example-data-i.html", "11.2 Example Data I", " 11.2 Example Data I For our first set of examples we will use the WISC data. Here we again read in, subset, and provide basic descriptives. filepath &lt;- &quot;https://quantdev.ssri.psu.edu/sites/qdev/files/wisc3raw.csv&quot; wisc3raw &lt;- read.csv(file=url(filepath),header=TRUE) var_names_sub &lt;- c( &quot;id&quot;, &quot;verb1&quot;, &quot;verb2&quot;, &quot;verb4&quot;, &quot;verb6&quot;, &quot;perfo1&quot;, &quot;perfo2&quot;, &quot;perfo4&quot;, &quot;perfo6&quot;, &quot;momed&quot;, &quot;grad&quot; ) wiscsub &lt;- wisc3raw[,var_names_sub] psych::describe(wiscsub) ## vars n mean sd median trimmed mad min max range skew ## id 1 204 102.50 59.03 102.50 102.50 75.61 1.00 204.00 203.00 0.00 ## verb1 2 204 19.59 5.81 19.34 19.50 5.41 3.33 35.15 31.82 0.13 ## verb2 3 204 25.42 6.11 25.98 25.40 6.57 5.95 39.85 33.90 -0.06 ## verb4 4 204 32.61 7.32 32.82 32.42 7.18 12.60 52.84 40.24 0.23 ## verb6 5 204 43.75 10.67 42.55 43.46 11.30 17.35 72.59 55.24 0.24 ## perfo1 6 204 17.98 8.35 17.66 17.69 8.30 0.00 46.58 46.58 0.35 ## perfo2 7 204 27.69 9.99 26.57 27.34 10.51 7.83 59.58 51.75 0.39 ## perfo4 8 204 39.36 10.27 39.09 39.28 10.04 7.81 75.61 67.80 0.15 ## perfo6 9 204 50.93 12.48 51.76 51.07 13.27 10.26 89.01 78.75 -0.06 ## momed 10 204 10.81 2.70 11.50 11.00 2.97 5.50 18.00 12.50 -0.36 ## grad 11 204 0.23 0.42 0.00 0.16 0.00 0.00 1.00 1.00 1.30 ## kurtosis se ## id -1.22 4.13 ## verb1 -0.05 0.41 ## verb2 -0.34 0.43 ## verb4 -0.08 0.51 ## verb6 -0.36 0.75 ## perfo1 -0.11 0.58 ## perfo2 -0.21 0.70 ## perfo4 0.59 0.72 ## perfo6 0.18 0.87 ## momed 0.01 0.19 ## grad -0.30 0.03 And some bivariate plots of the two-occasion relations. psych::pairs.panels(wiscsub[,c(&quot;verb1&quot;,&quot;verb6&quot;)]) "],["11.3-two-occassions-of-change.html", "11.3 Two-Occassions of Change", " 11.3 Two-Occassions of Change Although we may prefer to have more timepoints, two waves of data is often what is available or affordable, often directly related to a change of interest, e.g., pretest-posttest an improvement over cross-sectional data for many questions of interest Notions of Stability Stability in the Absolute or Exact Sense A variable may be considered stable to the extent that mean values for \\(y_{1}\\) and \\(y_{2}\\) are equal over time (\\(\\bar{y}_{2}-\\bar{y}_{1}=0\\), \\(y_{2i}-y_{1i}=0\\)) individual values of \\(y_{1i}\\) and \\(y_{2i}\\) are equal over time (\\(y_{2i}-y_{1i}=0\\)) Stability in the Relative Sense A variable may be considered stable to the extent that \\(y_{1i}\\) and \\(y_{2i}\\) are correlated (\\(r_{12}=1\\)) Note that mean or individual values may increase or decrease over time, but correlation will be largely unchanged unless relative position changes. Notions of Change Raw Change In thinking about difference scores (change scores, gain scores) we are often interested in the absolute change in the value of \\(y_{t}\\). Difference scores captures increase or decrease in the mean values or individual values. Correlation between \\(y_{1}\\) and \\(y_{2}\\) can be anywhere between \\(0\\) or \\(1\\) and difference scores may be small or large. For example. if \\(r_{12} = 1.0\\) and we add \\(5\\) points to \\(y_2\\), \\(r_{12}\\) will still be equl to \\(1.0\\) but the mean difference will have changed. y1 &lt;- c(1,1,1,2,2,2) y2 &lt;- c(3,3,3,4,4,4) # cor(y1,y2) = 1 # mean(y2-y1) = 2 y2 &lt;- y2 + 5 # cor(y1,y2) = 1 # mean(y2-y1) = 7 Relative Change When we think about relative change, a lower correlation indicates greater change, but change is relative to other cases in the data and not absolute. For example, if \\(r_{12}=0.45\\), and we add \\(5\\) points to all scores, \\(r_{12}\\) will still be \\(0.45\\). Even though the scores themselves will all change, the relative degree of change is unaffected. Compare Absolute and Relative Change Image from Newsom (2011) "],["11.4-autoregressive-model.html", "11.4 Autoregressive Model", " 11.4 Autoregressive Model Before introducing the model behind each approach is is helpful to consider the data characteristics underlying each notion of change. Residualized Change The first approach uses the measure at the second time point as a dependent variable regressed on the the measure at the first time point. The term residualized change comes from the fact that an outcome, such as verbal scores at grade 6, is regressed on itself at a prior occasion, verbal scores at grade 1. Any variability in the outcome that is explained by the lagged regressor will be set aside and is threfore not explainable by a key predictor. Thus, the autoregressive effect residualizes the outcome leaving only variability that is unexplained by the lagged variable, which can be construed as the variability due to change. In this way it is helpful to view the data as a scatter plot, like we would in a regression model. library(&quot;ggplot2&quot;) ggplot(data = wiscsub, aes(x = verb1, y = verb6)) + geom_point() + xlab(&quot;Verbal Scores at Grade 1&quot;) + ylab(&quot;Verbal Scores at Grade 6&quot;) + theme_bw() Autoregressive Model When researchers refer to the autoregressive (residualized change) model for two occasion data they are referring to the following multiple regression model: \\[ y_{2i} = \\beta_0 + \\beta_1y_{1i} + e_{i} \\] where \\(y_{1i}\\) is the value of the outcome variable for individual \\(i\\) at time \\(1\\) \\(y_{2i}\\) is the value of the outcome variable for individual \\(i\\) at time \\(2\\) \\(\\beta_0\\) is an intercept parameter, the expected value of \\(y_{2i}\\) when \\(y_{1i}=0\\) \\(\\beta_1\\) is a regression parameter indicating the difference in the predicted score of \\(y_{2i}\\) based on a 1-unit difference in \\(y_{1i}\\) \\(e_{i}\\) is the model error for individual \\(i\\) Note, the term residualized change comes from the fact that the autoregressive effect residualizes the outcome. This leaves only the variability that is unexplained by the previous timepoint, or the variability due to change. Autoregressive Residuals With the autoregressive model it is helpful to think more about the residual term. Let’s ignore the scaling constant for now, If we subtract \\(\\beta_1y_{1i}\\) from both sides of the AR equation we isolate the residuals: \\[ e_{i} = y_{2i} -\\beta_1y_{1i} \\] Here, the residualized change is the function of a weighted combination of our time 1 scores. Instead of talking about raw change we are instead asking “Where would we predict you to be at time 2 given your standing relative to the mean at time 1?”** Consider the following scenarios: \\(e_{i}\\) is positive: you changed more in a positive direction than would have been expected. \\(e_{i}\\) is negative: you changed more in a negative direction than would have been expected. Autoregressive Model in R As we said previously, the autoregrssive model is useful for examining questions about change in interindividual differences. The model for verbal scores at grade 6 can be written as \\[ verb6_{i} = \\beta_{0} + \\beta_{1}verb1_{i} + e_{i}\\] We note that this is a model of relations among between-person differences. This model is similar to, but is not a single-subject time-series model (which are also called autoregressive models, but are fit to a different kind of data). Translating the between-person autoregressive model into code and fitting it to the two-occasion WISC data we have ARfit &lt;- lm( formula = verb6 ~ 1 + verb1, data=wiscsub, na.action=na.exclude ) summary(ARfit) ## ## Call: ## lm(formula = verb6 ~ 1 + verb1, data = wiscsub, na.action = na.exclude) ## ## Residuals: ## Min 1Q Median 3Q Max ## -20.2459 -5.8651 0.1781 4.9048 27.9976 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 20.22485 1.99608 10.13 &lt;2e-16 *** ## verb1 1.20117 0.09773 12.29 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.087 on 202 degrees of freedom ## Multiple R-squared: 0.4279, Adjusted R-squared: 0.425 ## F-statistic: 151.1 on 1 and 202 DF, p-value: &lt; 2.2e-16 The intercept term, \\(\\beta_{0}\\) = 20.22 is the expected value of Verbal Ability at the 2nd occasion, for an individual with a Verbal Ability score = 0 at the 1st occasion. The slope term, \\(\\beta_{1}\\) = 1.20 indicates that for every 1-point difference in Verbal Ability at the 1st occasion, we expect a 1.2 point difference at the 2nd occasion. We can plot the autoregressive model prediction with confidence intervals (CI). The function termplot takes the fitted lm object. The CI bounds are plotted with the se option and residuals with partial.resid option. termplot(ARfit,se=TRUE,partial.resid=TRUE, main=&quot;Autoregressive Model&quot;, xlab=&quot;Verbal Score at Grade 1&quot;, ylab=&quot;Verbal Score at Grade 6&quot;) Note that this code makes use of the lm() model object. We can also do something similar with the raw data using ggplot. ggplot(data = wiscsub, aes(x = verb1, y = verb6)) + geom_point() + geom_smooth(method=&quot;lm&quot;, formula= y ~ 1 + x, se=TRUE, fullrange=TRUE, color=&quot;red&quot;, size=2) + xlab(&quot;Verbal Score at Grade 1&quot;) + ylab(&quot;Verbal Score at Grade 6&quot;) + ggtitle(&quot;Autoregressive Model&quot;) + theme_classic() Note that this code embeds an lm() model within the ggplot function. "],["11.5-difference-score-model.html", "11.5 Difference Score Model", " 11.5 Difference Score Model Again, let’s first take a look at the data characteristics. Raw Change The second approach involves computing a change score by subtracting the measure at time 1 from the measure at time 2 (e.g. \\(y_{t}-y_{t-1}\\)) This raw change score is then typically used as the dependent variable in a regression equation. Here we are speaking in terms of difference scores and raw change. We can plot intraindividual change, by putting time along the x-axis. This requires reshaping the data from wide format to long format. To recap, our wide data looks like this: head(round(wiscsub,2)) ## id verb1 verb2 verb4 verb6 perfo1 perfo2 perfo4 perfo6 momed grad ## 1 1 24.42 26.98 39.61 55.64 19.84 22.97 43.90 44.19 9.5 0 ## 2 2 12.44 14.38 21.92 37.81 5.90 13.44 18.29 40.38 5.5 0 ## 3 3 32.43 33.51 34.30 50.18 27.64 45.02 46.99 77.72 14.0 1 ## 4 4 22.69 28.39 42.16 44.72 33.16 29.68 45.97 61.66 14.0 1 ## 5 5 28.23 37.81 41.06 70.95 27.64 44.42 65.48 64.22 11.5 0 ## 6 6 16.06 20.12 38.02 39.94 8.45 15.78 26.99 39.08 14.0 1 We can reshape our data to a long format using the reshape() function as follows wiscsublong &lt;- reshape( data = wiscsub[c(&quot;id&quot;,&quot;verb1&quot;,&quot;verb6&quot;)], varying = c(&quot;verb1&quot;,&quot;verb6&quot;), timevar = &quot;grade&quot;, idvar = &quot;id&quot;, direction = &quot;long&quot;, sep = &quot;&quot; ) wiscsublong &lt;- wiscsublong[order(wiscsublong$id,wiscsublong$grade),] head(round(wiscsublong,2)) ## id grade verb ## 1.1 1 1 24.42 ## 1.6 1 6 55.64 ## 2.1 2 1 12.44 ## 2.6 2 6 37.81 ## 3.1 3 1 32.43 ## 3.6 3 6 50.18 Now, the long data is structured in a manner amenable to plotting. Notice here that each line indicates how an individual’s Grade 6 score differs from their Grade 1 score: intraindividual change. library(&quot;ggplot2&quot;) ggplot(data = wiscsublong, aes(x = grade, y = verb, group = id)) + geom_point() + geom_line() + xlab(&quot;Grade&quot;) + ylab(&quot;WISC Verbal Score&quot;) + ylim(0,100) + scale_x_continuous(breaks=seq(1,6,by=1)) + theme_bw() Calculating Difference Scores Using the same repeated measures notation as above we can think about difference scores in the following way \\[ y_{2i} = y_{1i} + \\Delta_{i} \\] where \\(y_{1i}\\) is the value of the outcome variable for individual \\(i\\) at time \\(1\\) \\(y_{2i}\\) is the value of the outcome variable for individual \\(i\\) at time \\(2\\) \\(\\Delta_i\\) is the difference score for individual \\(i\\) We can calculate the difference score as \\[ \\Delta_{i} = y_{2i} - y_{1i}\\] where \\(\\Delta_{i}\\) is a score just like other scores (we can calculate its mean and covariance with other variables, etc.). Comparison to Residualized Change Remember when we talked about the autoregressive change model we showed the residual was equal to \\[ e_{i} = y_{2i} -\\beta_1y_{1i} \\] For the autoregressive model, change is the function of a weighted combination of the scores. In the difference score approach, we defined the difference scores as \\[ \\Delta_{i} = y_{2i} - y_{1i}\\] What we see from this relationship is that raw change is residualized change when \\(\\beta_1 = 1\\). We can see these concepts are intimately linked. Difference Scores in WISC Data For our empirical example we can write the difference score, or raw change in verbal ability, between Grades 1 and 6, as \\[ verbD_{i} = verb2_{i} - verb1_{i} \\] Furthermore, we can calculate the difference score in R as follows #calculating difference score wiscsub$verbD &lt;- wiscsub$verb6-wiscsub$verb1 head(round(wiscsub,2)) ## id verb1 verb2 verb4 verb6 perfo1 perfo2 perfo4 perfo6 momed grad verbD ## 1 1 24.42 26.98 39.61 55.64 19.84 22.97 43.90 44.19 9.5 0 31.22 ## 2 2 12.44 14.38 21.92 37.81 5.90 13.44 18.29 40.38 5.5 0 25.37 ## 3 3 32.43 33.51 34.30 50.18 27.64 45.02 46.99 77.72 14.0 1 17.75 ## 4 4 22.69 28.39 42.16 44.72 33.16 29.68 45.97 61.66 14.0 1 22.03 ## 5 5 28.23 37.81 41.06 70.95 27.64 44.42 65.48 64.22 11.5 0 42.72 ## 6 6 16.06 20.12 38.02 39.94 8.45 15.78 26.99 39.08 14.0 1 23.88 Difference Score Descriptives Look at the descriptives with the difference score. psych::describe(wiscsub[,c(&quot;verb1&quot;,&quot;verb6&quot;,&quot;verbD&quot;)]) ## vars n mean sd median trimmed mad min max range skew kurtosis ## verb1 1 204 19.59 5.81 19.34 19.50 5.41 3.33 35.15 31.82 0.13 -0.05 ## verb6 2 204 43.75 10.67 42.55 43.46 11.30 17.35 72.59 55.24 0.24 -0.36 ## verbD 3 204 24.16 8.15 23.91 23.85 8.09 4.62 50.88 46.26 0.38 0.14 ## se ## verb1 0.41 ## verb6 0.75 ## verbD 0.57 psych::corr.test(wiscsub[,c(&quot;verb1&quot;,&quot;verb6&quot;,&quot;verbD&quot;)]) ## Call:psych::corr.test(x = wiscsub[, c(&quot;verb1&quot;, &quot;verb6&quot;, &quot;verbD&quot;)]) ## Correlation matrix ## verb1 verb6 verbD ## verb1 1.00 0.65 0.14 ## verb6 0.65 1.00 0.84 ## verbD 0.14 0.84 1.00 ## Sample Size ## [1] 204 ## Probability values (Entries above the diagonal are adjusted for multiple tests.) ## verb1 verb6 verbD ## verb1 0.00 0 0.04 ## verb6 0.00 0 0.00 ## verbD 0.04 0 0.00 ## ## To see confidence intervals of the correlations, print with the short=FALSE option Of particular interest in questions about intraindividual change is the relation between the pre-test score and the amount of intraindividual change. We can look at the bivariate association. psych::pairs.panels(wiscsub[,c(&quot;verb1&quot;,&quot;verbD&quot;)]) A note on computing difference scores: always use raw scores when computing difference scores, Pre-standardizing variables discards important variance information. A Difference Score Regression Model For the purpose of comparison consider a linear model is expressed for \\(i = 1\\) to \\(N\\) as \\[ \\Delta y_{i} = \\beta_{0} + \\beta_{1}y_{1i} + e_{i} \\] where we are looking at change in verbal test scores while controlling for grade 1 scores, \\(\\beta_0\\) is an intercept parameter, the predicted score of \\(\\Delta y\\) when \\(y_{1i}=0\\) \\(\\beta_1\\) is a slope parameter indicating the difference in the predicted score of \\(\\Delta y\\) based on a 1-unit difference in \\(y_{1i}\\) \\(e_{i}\\) is the residual score for individual \\(i\\) #Difference score model DIFfit &lt;- lm(formula = verbD ~ 1 + verb1, data=wiscsub, na.action=na.exclude) summary(DIFfit) ## ## Call: ## lm(formula = verbD ~ 1 + verb1, data = wiscsub, na.action = na.exclude) ## ## Residuals: ## Min 1Q Median 3Q Max ## -20.2459 -5.8651 0.1781 4.9048 27.9976 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 20.22485 1.99608 10.132 &lt;2e-16 *** ## verb1 0.20117 0.09773 2.058 0.0408 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.087 on 202 degrees of freedom ## Multiple R-squared: 0.02054, Adjusted R-squared: 0.0157 ## F-statistic: 4.237 on 1 and 202 DF, p-value: 0.04083 The intercept term, \\(\\beta_{0} = 20.22\\) is the expected value of the difference score (raw change in verbal ability), for an individual with a verbal ability score = 0 at the first occasion. The slope term, \\(\\beta_{1} = 0.20\\) indicates that for every 1-point difference in verbal ability at the first occasion, we expect a \\(0.2\\) point difference in the amount of intraindividual change. The same methods as above can be used to plot the results of the difference score model. termplot(DIFfit,se=TRUE,partial.resid=TRUE, main=&quot;Difference-score Model&quot;, xlab=&quot;Verbal Score at Time 1&quot;, ylab=&quot;Difference in G1 and G6 Verbal Scores&quot;) We can also do something similar using ggplot. #making interindividual regression plot ggplot(data = wiscsub, aes(x = verb1, y = verbD)) + geom_point() + geom_smooth(method=&quot;lm&quot;, formula= y ~ 1 + x, se=TRUE, fullrange=TRUE, color=&quot;red&quot;, size=2) + xlab(&quot;Verbal Score at Grade 1&quot;) + ylab(&quot;Difference Score&quot;) + ggtitle(&quot;Difference Score Model&quot;) + theme_classic() Note that each of these model results plots are regression plots: outcome on the y-axis, predictor on the x-axis. "],["11.6-critiques-and-comparisons.html", "11.6 Critiques and Comparisons", " 11.6 Critiques and Comparisons Critiques of the Autoregressive Model Some interpretational oddities arise from the autoregressive model that are worth considering. Consider the following situation: We are studying a weight loss intervention where we measure weight prior to and after an intervention. The mean weight at time 1 is 250 pounds (\\(\\mu_{t1}=150\\)) and the mean weight at time 2 is 230 pounds (\\(\\mu_{t1}=130\\)). Now consider two people: Individual 1: Weight at time 1 was \\(240lbs\\) and weight at time 2 is \\(240lbs\\). Relative standing has gone down so there is positive residualized change. Individual 2: Weight at time 1 was \\(250lbs\\) and weight at time 2 is \\(230lbs\\). Relative standing is the same so there is no residualized change. Critique of Difference Score Model There have also been some major historical critiques of differences scores (e.g. Cronbach and Furby (1970)). Much of these critiques are based on reliability and the following rationale. Consider a typical model for a set of repeated measures, \\[ y_{1i} = y_{true,i} + e_{1i} \\\\ y_{2i} = y_{true,i} + e_{2i} \\] where \\(y_{true,i}\\) is the unobserved true score at both occasions \\(e_{i}\\) is the unobserved random error that is independent over each occasion Note, in this theoretical model the true score remains the same and all changes are based on random noise. If this model holds then we could write a simple difference score as \\[ D_{i} = y_{2i} - y_{1i} \\\\ \\quad\\quad\\: \\quad\\quad\\:\\quad\\quad\\:\\quad\\quad\\quad= (y_{true,i} + e_{2i}) - (y_{true,i} + e_{1i})\\\\ \\quad\\quad\\: \\quad\\quad\\:\\quad\\quad\\:\\quad\\quad\\quad = (y_{true,i} - y_{true,i}) + (e_{2i} - e_{1i})\\\\ \\quad\\quad = (e_{2i} - e_{1i})\\\\ \\] where Variance of the difference score is entirely based on the variance of the differences in random error scores the reliability of the difference scores is zero Alternative Interpretation This has led many to many historical critiques of difference scores. However, other researchers have pointed out this conclusion is based on how one envisions change. If, for example, we have the following theoretical model for change, \\[ y_{1i} = y_{true,i} + e_{1i}\\\\ y_{2i} = (y_{1i} + \\Delta y_{true,i}) + e_{2i} \\] where \\(y_{true}\\) is the unobserved true score at both occasions \\(\\Delta y_{true}\\) is the unobserved true change score between occasions \\(e_{i}\\) is the unobserved random error that is independent over each occasion If this model holds, as opposed to the alternative model, then the difference scores \\[ D_{i} = y_{2i} - y_{1i} \\\\ \\quad\\quad\\: \\quad\\quad\\:\\quad\\quad\\:\\quad\\quad\\quad= (y_{true,i} + e_{2i}) - (y_{true,i} + e_{1i})\\\\ \\quad\\quad\\: \\quad\\quad\\:\\quad\\quad\\:\\quad\\quad\\quad = (y_{true,i} - y_{true,i}) + (e_{2i} - e_{1i})\\\\ \\quad\\quad \\quad\\quad\\quad= \\Delta y_{1} + (e_{2i} - e_{1i})\\\\ \\] where now the variance of the difference score is based on the variance of the differences in the random error scores and the gain in the true scores the relative size of the true score gain determines variance and reliability of the difference scores this implies difference scores may be an entirely appropriate means for measuring change References "],["11.7-adding-explanatory-variables.html", "11.7 Adding Explanatory Variables", " 11.7 Adding Explanatory Variables Assessing the effects of an intervention, grouping variable or construct on change in another construct is a central goal in virtually all areas of the social, health and behavioral sciences. developmental researchers interested in how the occurrence of particular life events (e.g., job loss, marriage) affects changes in well-being or personality. educational psychology researchers investigate how teacher characteristics (e.g.,competence, motivation) predict changes in students’ achievement. Most researchers rely on one of two basic strategies when analyzing the prospective effects of one construct on another construct in the context of two-wave data: (1) the ANCOVA model or (2) the difference score model. For example, in this situation the autoregressive model becomes the traditional ANCOVA model \\[ y_{2i} = \\beta_{0} + \\beta_{1} x_{i} + \\beta_{2} y_{1i} + \\epsilon_{i} \\] and we have the following difference score model \\[ \\Delta_{i} = \\beta_{0} + \\beta_{1}x_{i} + \\epsilon_{i} \\] where \\(x_{i}\\) is either zero or one for the \\(i^{th}\\) individual in the sample. Although not immediately obvious, the difference between these two approaches is that \\(\\beta{2}\\) is estimated in the ANCOVA model, and fixed to \\(1\\) in the difference score approach. As pointed out by many different researchers over the year, it can be challenging to decide which of the two approaches is more appropriate. However, the decision is often critical as the two approaches can yield results that substantially differ concerning the magnitude, sign, and statistical significance of the estimated treatment effect. "],["11.8-lords-paradox.html", "11.8 Lord’s Paradox", " 11.8 Lord’s Paradox Consider the summary of Lord’s Paradox from Wikipedia The most famous formulation of Lord’s paradox comes from his 1967 paper: “A large university is interested in investigating the effects on the students of the diet provided in the university dining halls and any sex differences in these effects. &gt; Various types of data are gathered. In particular, the weight of each student at the time of his arrival in September and his weight the following June are recorded.” (Lord &gt; 1967, p. 304) In both September and June, the overall distribution of male weights is the same, although individuals’ weights have changed, and likewise for the distribution of female weights. Lord imagines two statisticians who use different common statistical methods but reach opposite conclusions. One statistician uses difference scores and finds no significant difference between genders: “[A]s far as these data are concerned, there is no evidence of any interesting effect of diet (or of anything else) on student weights. In particular, there is no evidence of any differential effect on the two sexes, since neither group shows any systematic change.” (pg. 305) Visually, the first statistician sees that neither group mean (‘A’ and ‘B’) has changed, and concludes that the new diet had no causal impact. The second statistician uses the ANCOVA (residualized change) model. They find a significant difference between the two sexes. Visually, the second statistician fits a regression model (green dotted lines), finds that the intercept differs for boys vs girls, and concludes that the new diet had a larger impact for males. Lord concluded: “there simply is no logical or statistical procedure that can be counted on to make proper allowance for uncontrolled preexisting differences between groups.” image "],["11.9-example-data-ii.html", "11.9 Example Data II", " 11.9 Example Data II Now, let us compare the common situation where we have a predictor of interest. Castro-Schilo and Grimm (2018) consider a situation common to behavioral science researchers. Sofia, a close relationships’ researcher, is interested in examining change in relationship satisfaction before and after marriage. Specifically, she wants to know whether cohabiting prior to marriage has an effect on changes in relationship satisfaction. As she plans her data analysis strategy, Sofia realizes she has two options: (1) use the postmarriage relationship satisfaction score as her outcome and include the premarriage score in her analysis as a predictor, in addition to whether or not couples cohabited prior to marriage to account for individual differences in baseline relationship satisfaction (known as residualized change approach) or (2) compute the difference between the postmarriage relationship satisfaction score and the premarriage relationship satisfaction score and use it as her outcome to test the effect of cohabiting prior to marriage (known as difference score approach). Which of these two strategies is most appropriate? Should both strategies provide the same results? Sofia has heard difference scores have a bad reputation, why is that? Are there additional strategies Sofia is not aware of that might be preferred over her current options? 11.9.1 Generate Some Data According to (Castro-Schilo and Grimm 2018) Let’s generate some data according to the example from Castro-Schilo and Grimm (2018). Below is one possible data generating model. We will consider two scenarios. In both scenarios the following facts will be true: Both data sets have an equal number of dyads that did and did not cohabit Cohabitation had a null effect on changes in relationship satisfaction. Dyads did not exhibit any changes in relationship satisfaction over time. In this first example dataset, however, there are preexisting groups at the premarriage assessment. That is, because in a real investigation of cohabitation a controlled experiment would not be feasible, we generated one data set with lower relationship satisfaction at baseline for dyads who reported cohabiting. Note that our description of the first dataset corresponds to the assumption made by the difference score model (i.e., no change between groups across time under the assumption of the null hypothesis for the key predictor; there are two populations at baseline). # set a seed so we all have the same data set.seed (1234) # create an id variable (N = 100 in each group) id &lt;- c(1:100, 1:100) # create a time variable (0 = time 1, 1 = time 2) time &lt;- c(rep(0, 100), rep(1, 100)) # create grouping variable with equal number cohabiting (1) and not (0) cohabit &lt;- rep(c(0,1), 100) # put all variables in a dataframe data &lt;- data.frame(id, cohabit, time) # generate dataset A data_A &lt;- data data_A$score &lt;- 4.5 + # intercept -2*data_A$cohabit + # difference in cohabitation 0*data_A$time + # effect of time 0*data_A$cohabit*data_A$time + # interaction rnorm(200, 0, 0.3) # error # Make into wide format for computing difference score data_A &lt;- reshape( data_A, v.names = &#39;score&#39;, timevar = &quot;time&quot;, idvar = &quot;id&quot;, direction= &quot;wide&quot; ) # assign appropriate variable names names(data_A) &lt;- c(&#39;id&#39;, &#39;cohabit&#39;, &#39;score1&#39;, &#39;score2&#39;) # compute difference variable data_A$diff &lt;- data_A$score2 - data_A$score1 # declare cohabiting variable as a factor (categorical variable) data_A$cohabit &lt;- factor(data_A$cohabit) 11.9.2 Plot Data # visualize the data library(ggplot2) ggplot(data_A, aes(x = score1, y = score2, shape= cohabit)) + geom_point(size = 3) + geom_smooth(method = lm, se = F) + xlab(&quot;Relationship Satisfaction Time 1&quot;) + ylab(&quot;Relationship Satisfaction Time 2&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; 11.9.3 Fit Residualized Change Model Recall that the main interest in these analyses is to assess the potential effect of cohabitation on changes in relationship satisfaction. According to the residualized change model we fit to data set A (which is an ANCOVA model in this application because of the categorical predictor), the factors explain 92% of the variance in the outcome and results suggest there is a statistically and practically significant effect of cohabitation in dyad’s relationship satisfaction at Time 2, controlling for baseline relationship satisfaction. # ancova model mylml &lt;- lm(score2 ~ score1 + cohabit, data = data_A) summary (mylml) ## ## Call: ## lm(formula = score2 ~ score1 + cohabit, data = data_A) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.86084 -0.17640 0.00684 0.17019 0.92449 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.66901 0.46562 10.028 &lt; 2e-16 *** ## score1 -0.02875 0.10390 -0.277 0.783 ## cohabit1 -2.11473 0.21860 -9.674 6.78e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3114 on 97 degrees of freedom ## Multiple R-squared: 0.9183, Adjusted R-squared: 0.9167 ## F-statistic: 545.4 on 2 and 97 DF, p-value: &lt; 2.2e-16 11.9.4 Fit Difference Score Model # difference model mylm2 = lm(diff ~ cohabit, data = data_A) summary(mylm2) ## ## Call: ## lm(formula = diff ~ cohabit, data = data_A) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.29920 -0.22982 0.00904 0.28891 1.09081 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.07941 0.06212 1.278 0.204 ## cohabit1 -0.04002 0.08786 -0.455 0.650 ## ## Residual standard error: 0.4393 on 98 degrees of freedom ## Multiple R-squared: 0.002113, Adjusted R-squared: -0.00807 ## F-statistic: 0.2075 on 1 and 98 DF, p-value: 0.6498 In contrast, when we fit the difference score model to the same data, less than 1% of the variance in the outcome is explained by the model, and results suggested that cohabitation did not have an effect on relationship satisfaction change. As expected, the inferences from these models are strikingly different, which is an example of Lord’s paradox and can be attributed to the differences in baseline scores across couples who cohabit and those who do not. References "],["11.10-example-data-iii.html", "11.10 Example Data III", " 11.10 Example Data III Consider a second example dataset, where, again We have an equal number of dyads that did and did not cohabit Cohabitation had a null effect on changes in relationship satisfaction. Dyads did not exhibit any changes in relationship satisfaction over time. Unlike the first dataset, however, this second dataset would have been obtained if Sofia could have randomly assigned dyads to cohabit or live apart. This second data set is in line with the assumption of the residualized change model, where the Time 1 score is uncorrelated with the key predictor, which is equivalent to affirming there are no preexisting group differences at baseline; there is one population of dyads at baseline with respect to relationship satisfaction. 11.10.1 Generate Some Data According to (Castro-Schilo and Grimm 2018) Example B # set a seed so we all have the same data set.seed (1234) # create an id variable (N = 100 in each group) id &lt;- c(1:100, 1:100) # create a time variable (0 = time 1, 1 = time 2) time &lt;- c(rep(0, 100), rep(1, 100)) # create grouping variable with equal number cohabiting (1) and not (0) cohabit &lt;- rep(c(0,1), 100) # put all variables in a dataframe data &lt;- data.frame(id, cohabit, time) # generate dataset A data_B &lt;- data data_B$score &lt;- 4.5 + # intercept 0*data_B$cohabit + # difference in cohabitation 0*data_B$time + # effect of time 0*data_B$cohabit*data_B$time + # interaction rnorm(200, 0, 0.3) # error # Make into wide format for computing difference score data_B &lt;- reshape( data_B, v.names = &#39;score&#39;, timevar = &quot;time&quot;, idvar = &quot;id&quot;, direction= &quot;wide&quot; ) # assign appropriate variable names names(data_B) &lt;- c(&#39;id&#39;, &#39;cohabit&#39;, &#39;score1&#39;, &#39;score2&#39;) # compute difference variable data_B$diff &lt;- data_B$score2 - data_B$score1 # declare cohabiting variable as a factor (categorical variable) data_B$cohabit &lt;- factor(data_B$cohabit) 11.10.2 Plot Data # visualize the data library(ggplot2) ggplot(data_B, aes(x = score1, y = score2, shape= cohabit)) + geom_point(size = 3) + geom_smooth(method = lm, se = F) + xlab(&quot;Relationship Satisfaction Time 1&quot;) + ylab(&quot;Relationship Satisfaction Time 2&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; 11.10.3 Fit Residualized Change Model # ancova model mylml &lt;- lm(score2 ~ score1 + cohabit, data = data_B) summary (mylml) ## ## Call: ## lm(formula = score2 ~ score1 + cohabit, data = data_B) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.86084 -0.17640 0.00684 0.17019 0.92449 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.66901 0.46562 10.028 &lt;2e-16 *** ## score1 -0.02875 0.10390 -0.277 0.783 ## cohabit1 -0.05724 0.06230 -0.919 0.361 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3114 on 97 degrees of freedom ## Multiple R-squared: 0.009265, Adjusted R-squared: -0.01116 ## F-statistic: 0.4535 on 2 and 97 DF, p-value: 0.6367 11.10.4 Fit Difference Score Model # difference model mylm2 = lm(diff ~ cohabit, data = data_B) summary(mylm2) ## ## Call: ## lm(formula = diff ~ cohabit, data = data_B) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.29920 -0.22982 0.00904 0.28891 1.09081 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.07941 0.06212 1.278 0.204 ## cohabit1 -0.04002 0.08786 -0.455 0.650 ## ## Residual standard error: 0.4393 on 98 degrees of freedom ## Multiple R-squared: 0.002113, Adjusted R-squared: -0.00807 ## F-statistic: 0.2075 on 1 and 98 DF, p-value: 0.6498 References "],["11.11-closing-thoughts.html", "11.11 Closing Thoughts", " 11.11 Closing Thoughts Interpretations of change are fundamentally restricted by the choice of model The residualized change and difference score models will often lead to different inferences about the effect of the predictor of interest. The assumptions each of these models make are untestable, and this makes it challenging to argue that either model is more appropriate than the other. Dyads who report cohabiting prior to marriage will never be able to report what their relationship satisfaction after marriage would have been if they had not cohabited. We will never know what level of relationship satisfaction would have been reported by dyads who did not cohabit before marriage if they had in fact chosen to live with their partners prior to getting married. 11.11.0.1 Random Assignment When experimentatal procedures are followed, dyads would be randomly assigned to a grouping variable, and this would bring independence of the grouping variable with the pre-test scores. Thus, proper experimentation prohibits preexisting groups to be formed at baseline. Under such circumstances, the residualized change model and difference score models will arrive at the same inference, but the former has the advantage of having more power because it makes the correct assumption about the distribution of baseline scores. 11.11.0.2 Observational Studies Which should be our choice when experimentation is not possible? If we have different populations with different levels of the outcome at baseline, the residualized change model may not be appropriate. For researchers going this route, a latent change score model may alleviate some concerns about reliability and measurement error. 11.11.0.3 Two-Occassion Change If you found all of this to be inherently confusing it is worth thinking about the following quote: “Two waves of data are better than one, but maybe not much better.” (Rogosa, Brandt, and Zimowski 1982). References "],["11.12-references-4.html", "11.12 References", " 11.12 References "],["12-chapter-12.html", "Chapter 12 Introduction to Growth", " Chapter 12 Introduction to Growth This chapter introduces models for repeated measures data (e.g., RM ANOVA, RM MANOVA) along a continuum. The motivation for this introduction is to present how these models are linked together, making the similarities and differences among them easier to identify and understand. We will tie the models together in a multilevel framework, working from repeated measures ANOVA through repeated measures MANOVA to growth models. "],["12.1-introduction-1.html", "12.1 Introduction", " 12.1 Introduction 12.1.1 What is a multilevel model? Multilevel models (MLM) go by many names: General Linear Mixed Model Random Coefficients Model Hierarchical Linear Model 12.1.2 Two Faces of MLM In traditional MLM we typically have a model for the means, and a model for the variance. 12.1.2.1 A Model for the Means Typicalled known as fixed effects Similar to the quantities tested in single-level models How the expected value of an outcome varies based on the values of predictors in our model 12.1.2.2 A Model for the Variances Typically known as random effects Similar to the assumptions used in single-level models Model for how residuals are distributed and vary across observations (persons, groups, and time) 12.1.3 Two-Level Longitudinal Data Between-Person Variation Level 2 or inter-individual differences Time-invariant More of less than other people Within-Person Variation Level 1 or intra-individual differences Time-varying Can only assess with longitudinal studies More or less than one’s average "],["12.2-example-data-5.html", "12.2 Example Data", " 12.2 Example Data Loading some new libraries used in this chapter. library(psych) # for descriptives etc library(ggplot2) # for plotting library(nlme) # for mixed effects models library(lme4) # for mixed effects models library(lmerTest) # to get significance tests from lmer 12.2.1 Data Preparation and Description For our examples, we use 3-occasion WISC data that are equally spaced. Load the repeated measures data filepath &lt;- &quot;https://quantdev.ssri.psu.edu/sites/qdev/files/wisc3raw.csv&quot; wisc3raw &lt;- read.csv(file=url(filepath),header=TRUE) Next, let’s Subset the variables of interest. For this chapter we will include: 3-occasion equally spaced repeated measures (verb2, verb4, verb6) A person-level grouping variable (grade) An ID variable (id) After subsetting let’s take a look at some basic descriptives. varnames &lt;- c(&quot;id&quot;,&quot;verb2&quot;,&quot;verb4&quot;,&quot;verb6&quot;,&quot;grad&quot;) wiscsub &lt;- wisc3raw[ ,varnames] describe(wiscsub) ## vars n mean sd median trimmed mad min max range skew ## id 1 204 102.50 59.03 102.50 102.50 75.61 1.00 204.00 203.00 0.00 ## verb2 2 204 25.42 6.11 25.98 25.40 6.57 5.95 39.85 33.90 -0.06 ## verb4 3 204 32.61 7.32 32.82 32.42 7.18 12.60 52.84 40.24 0.23 ## verb6 4 204 43.75 10.67 42.55 43.46 11.30 17.35 72.59 55.24 0.24 ## grad 5 204 0.23 0.42 0.00 0.16 0.00 0.00 1.00 1.00 1.30 ## kurtosis se ## id -1.22 4.13 ## verb2 -0.34 0.43 ## verb4 -0.08 0.51 ## verb6 -0.36 0.75 ## grad -0.30 0.03 Multilevel modeling analyses typically require a long data set. So, we also reshape from wide to long in order to have a long data set. verblong &lt;- reshape( data = wiscsub, varying = c(&quot;verb2&quot;,&quot;verb4&quot;,&quot;verb6&quot;), timevar = &quot;grade&quot;, idvar = &quot;id&quot;, direction = &quot;long&quot;, sep = &quot;&quot; ) verblong &lt;- verblong[order(verblong$id,verblong$grade), c(&quot;id&quot;,&quot;grade&quot;,&quot;verb&quot;,&quot;grad&quot;)] head(verblong,12) ## id grade verb grad ## 1.2 1 2 26.98 0 ## 1.4 1 4 39.61 0 ## 1.6 1 6 55.64 0 ## 2.2 2 2 14.38 0 ## 2.4 2 4 21.92 0 ## 2.6 2 6 37.81 0 ## 3.2 3 2 33.51 1 ## 3.4 3 4 34.30 1 ## 3.6 3 6 50.18 1 ## 4.2 4 2 28.39 1 ## 4.4 4 4 42.16 1 ## 4.6 4 6 44.72 1 12.2.2 Sample Moments For clarity, let’s consider the basic information representation of the 3-occasion repeated measures data. In particular, data (even non-repeated measures data) are summarized (at the sample-level) as (1) a vector of means and (2) a variance-covariance matrix. #mean vector (from wide data) meanvector &lt;- sapply(wiscsub[ ,c(&quot;verb2&quot;,&quot;verb4&quot;,&quot;verb6&quot;)], mean, na.rm=TRUE) round(meanvector,2) ## verb2 verb4 verb6 ## 25.42 32.61 43.75 #variance-covariance matrix (from wide data) varcovmatrix &lt;- cov(wiscsub[ ,c(&quot;verb2&quot;,&quot;verb4&quot;,&quot;verb6&quot;)], use=&quot;pairwise.complete.obs&quot;) round(varcovmatrix,2) ## verb2 verb4 verb6 ## verb2 37.29 33.82 47.40 ## verb4 33.82 53.58 62.25 ## verb6 47.40 62.25 113.74 Making visual counterparts can also be extremely useful - especially for facilitating higher-level conversations in a research group. Basic sample-level descriptions in visual form. Note that the time variable has been converted to a factor = categorical ggplot(data=verblong, aes(x=factor(grade), y=verb)) + geom_boxplot(notch = TRUE) + stat_summary(fun.y=&quot;mean&quot;, geom=&quot;point&quot;, shape=23, size=3, fill=&quot;white&quot;) + labs(x = &quot;Grade&quot;, y = &quot;Verbal Ability&quot;) + theme_classic() ## Warning: The `fun.y` argument of `stat_summary()` is deprecated as of ggplot2 3.3.0. ## ℹ Please use the `fun` argument instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. pairs.panels(wiscsub[,c(&quot;verb2&quot;,&quot;verb4&quot;,&quot;verb6&quot;)]) Reminder, we should always be careful about the scaling of the x- and y-axes in these plots. One additional recoding for convenience is to center and scale our time variable. This gives us a specific \\(0\\) point and an intuitive \\(0, 1, 2\\) scale that is useful for our didactic purposes. unique(verblong$grade) ## [1] 2 4 6 verblong$time0 &lt;- (verblong$grade-2)/2 # from 2,4,6 to 0,1,2 unique(verblong$time0) ## [1] 0 1 2 head(verblong,12) ## id grade verb grad time0 ## 1.2 1 2 26.98 0 0 ## 1.4 1 4 39.61 0 1 ## 1.6 1 6 55.64 0 2 ## 2.2 2 2 14.38 0 0 ## 2.4 2 4 21.92 0 1 ## 2.6 2 6 37.81 0 2 ## 3.2 3 2 33.51 1 0 ## 3.4 3 4 34.30 1 1 ## 3.6 3 6 50.18 1 2 ## 4.2 4 2 28.39 1 0 ## 4.4 4 4 42.16 1 1 ## 4.6 4 6 44.72 1 2 Plotting the raw data along this new time variable. #plotting intraindividual change RAW DATA ggplot(data = verblong, aes(x = time0, y = verb, group = id)) + ggtitle(&quot;Raw Data&quot;) + geom_point() + geom_line() + xlab(&quot;Time&quot;) + ylab(&quot;WISC Verbal Score&quot;) + ylim(0,100) + xlim(0,2) + theme_classic() Note that the time variable in this plot has NOT been converted to a factor. It is a continuous variable. "],["12.3-a-general-model.html", "12.3 A General Model", " 12.3 A General Model The presentation of all of these models here is an attempt to integrate traditions that are typically kept separate or set against each other. In reality, they are just a few examples of the many, many possible models that exist. Each model is useful in specific situations. The objective of all of our analyses is to deconstruct the data into meaningful and interpretable pieces. Each model does this in a different way, with different assumptions. We can judge the models based on How well they articulate and test our theory, and How well they recover the data (evaluated as misfit to the impled moments). Recall that the regression model may be compactly written as \\[ \\mathbf{Y} = \\mathbf{X}\\mathbf{\\beta} + \\boldsymbol{e} \\] We make it into a multilevel regression model by further partitioning the predictor space into between-person and within-person components. Note: this is the very same distinction that is made in traditional presentations of ANOVA when examining between-person factors and within-person factors. The general model becomes \\[ \\underbrace{\\boldsymbol{Y}_i}_{\\begin{subarray}{c}\\text{repeated measures}\\\\ \\text{for persion i}\\end{subarray}} = \\underbrace{\\boldsymbol{X}_i}_{\\begin{subarray}{c}\\text{known}\\\\ \\text{covariates}\\end{subarray}}\\underbrace{\\boldsymbol{\\beta}}_{\\begin{subarray}{c}\\text{fixed}\\\\ \\text{effects}\\end{subarray}} + \\underbrace{\\boldsymbol{Z}_i}_{\\begin{subarray}{c}\\text{known}\\\\ \\text{covariates}\\end{subarray}}\\underbrace{\\boldsymbol{u}_i}_{\\begin{subarray}{c}\\text{random}\\\\ \\text{effects}\\end{subarray}} + \\underbrace{\\boldsymbol{e}_i}_{\\begin{subarray}{c}\\text{residuals}\\end{subarray}} \\] which is also called the linear mixed model. "],["12.4-unconditional-means-model.html", "12.4 Unconditional Means Model", " 12.4 Unconditional Means Model It is often recommended to fit the unconditional means model before moving on to more complicated models. This is primarily because the unconditional means model does well at partitioning variance of the outcome across levels. We can use this model to better understand the amount of outcome variation that exists at the within and between levels of our model. If we fail to find sufficient variation at a given level there may be little reason to proceed with attempts to explain variance at that level of analysis. 12.4.1 Level 1 First, let us write out the level-1 (individual) model \\[ y_{it} = 1\\beta_{0i} + e_{it} \\] where \\(y_{it}\\) is the repeated measures score for individual \\(i\\) at time \\(t\\) \\(\\beta_{0i}\\) is the random intercept for individual \\(i\\) (person-specific mean) \\(e_{it}\\) is the time-specific residual score (within-person deviation) \\(e_{it} \\sim \\mathcal{N}(0,\\sigma^{2}_{e})\\) Note, the level 1 model shows us the true individual-level trajectories are completely flat, sitting at \\(\\beta_{0i}\\). 12.4.2 Level 2 The level-2 (sample) equation for the random intercept \\(\\beta_{0i}\\) can be written as \\[ \\beta_{0i} = 1\\gamma_{00} + u_{0i} \\] where \\(\\gamma_{00}\\) is the sample mean for the intercept (grand mean) \\(u_{0i}\\) is individual \\(i\\)’s deviation from the sample mean (between person deviation) \\(u_{0i} \\sim \\mathcal{N}(0,\\psi_{u0})\\) Note, the looking at the level 1 and level 2 model tells us that while these flat trajectories may differ in elevation, across everyone in the population, their average elevation is \\(\\gamma_{00}\\). 12.4.3 Single Equation We can write also both models in a single equation as follows \\[ y_{it} = (1\\gamma_{00} + 1u_{0i}) + e_{it} \\] where \\(y_{it}\\) is the repeated measures score for individual \\(i\\) at time \\(t\\) \\(\\gamma_{00}\\) is the sample mean for the intercept (grand mean) \\(u_{0i}\\) is individual \\(i\\)’s deviation from the sample mean (between person deviation) \\(u_{0i} \\sim \\mathcal{N}(0,\\psi_{u0})\\) \\(e_{it}\\) is the time-specific residual score (within-person deviation) \\(e_{it} \\sim \\mathcal{N}(0,\\sigma^{2}_{e})\\) 12.4.4 Model Elaboration 12.4.4.1 Within-Person Residual Covariance For clarity, Let’s write out the full variance covariance matrix of the within-person residuals (spanning across the \\(T = 3\\) repeated measures). Remember, we wrote \\((e_{it} \\sim \\mathcal{N}(0,\\sigma^{2}_{e}))\\), or in matrix notation, \\[ \\sigma^{2}_{e}\\boldsymbol{I} = \\sigma^{2}_{e} \\left[\\begin{array} {rrr} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{array}\\right] = \\left[\\begin{array} {rrr} \\sigma^{2}_{e} &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma^{2}_{e} &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma^{2}_{e} \\end{array}\\right] = \\boldsymbol{\\Lambda} \\] Note, this is the homoscedasticity of errors assumption. 12.4.4.2 Between-Person Residual Covariance We can now do the same for the full variance covariance matrix of the between-person residuals, \\[ \\left[\\begin{array} {r} \\psi_{u0} \\end{array}\\right] = u_{0i}u_{0i}&#39; \\] Note, in the unconditional means model there is no-growth, each individual has an intercept, but no change in scores is predicted because there are no predictors (e.g. time) in the level-1 equation. 12.4.5 Estimated Quantities Importantly, in the unconditional means model we will be interested in estimating three parameters: The sample-level mean of the random intercept (\\(\\gamma_{00}\\)) or the grand mean across all occasions and individuals. The variance of the random intercept (\\(\\psi_{u0}\\)) Provides information about the magnitude of between person differences in scores at each measurement occasion. The residual variance (\\(\\sigma^{2}_{e}\\)) Provides information about the magnitude of with-person fluctuations in scores over time. 12.4.6 More Notation It is also worth mentioning that through some replacement (e.g., replacing \\(u_{i}\\) = \\(b_{i}\\), the different vectors of 1 with \\(X_{i}\\) and \\(Z_{i}\\) = 1 we can get to … \\[ \\boldsymbol{Y} = \\boldsymbol{X}\\boldsymbol{\\beta} + \\boldsymbol{Z}\\boldsymbol{b} + \\boldsymbol{e} \\] which is just the more general notation often used in the statistics literature. The equation we started with, with another multilevel notation … \\[ \\boldsymbol{Y} = \\boldsymbol{X}\\boldsymbol{\\gamma} + \\boldsymbol{Z}\\boldsymbol{u} + \\boldsymbol{e} \\] 12.4.7 Unconditional Means Model in R We can write the unconditional means model in R as follows. um_fit &lt;- lme( fixed = verb ~ 1, random = ~ 1|id, data = verblong, na.action = na.exclude ) summary(um_fit) ## Linear mixed-effects model fit by REML ## Data: verblong ## AIC BIC logLik ## 4682.66 4695.906 -2338.33 ## ## Random effects: ## Formula: ~1 | id ## (Intercept) Residual ## StdDev: 4.406063 10.277 ## ## Fixed effects: verb ~ 1 ## Value Std.Error DF t-value p-value ## (Intercept) 33.92433 0.5174359 408 65.56238 0 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -1.9626206 -0.6924771 -0.1481960 0.5511044 3.1026127 ## ## Number of Observations: 612 ## Number of Groups: 204 # um_fit2 &lt;- lmer( # verb ~ 1 + (1|id), # data=verblong, # na.action = na.exclude # ) # summary(um_fit2) 12.4.7.1 Interpretation The single fixed effect in our unconditional means model is the grand mean, or \\(\\gamma_{00}\\). Rejection of the null indicates the average verbal score between Grades 2 and 6 is non-zero. Next we look at the random effects. The estimated between-person standard deviation is \\(\\psi_{u0}=4.4\\) and the estimated within-person standard deviation is \\(\\sigma^{2}_{e}=10.28\\). 12.4.8 Intra-Class Correlation The intra-class correlation (ICC) as the ratio of the random intercept variance (between-person) to the total variance, defined as the sum of the random intercept variance and residual variance (between + within). Specifically, \\[ICC_{between} = \\frac{\\sigma^{2}_{u0}}{\\sigma^{2}_{u0} + \\sigma^{2}_{e}}\\] 12.4.8.1 Calculating the ICC The ICC is the ratio of the random intercept variance (between-person var) over the total variance (between + within var): ICC_between &lt;- 4.4^2 / (4.4^2 + 10.28^2) ICC_between ## [1] 0.1548324 # Simple function for computing ICC from lme() output ICClme &lt;- function(out) { varests &lt;- as.numeric(VarCorr(out)[1:2]) return(paste(&quot;ICC =&quot;, varests[1]/sum(varests))) } ICClme(um_fit) ## [1] &quot;ICC = 0.155269768304537&quot; From the unconditional means model, the ICC was calculated, which indicated that of the total variance in verbal scores, approximately 16%, is attributable to between-person variation whereas 84% is attributable to within-person variation. This means there is a good portion of within-person variance sill to be modeled. 12.4.9 Model-Impled Moments What is the implied representation of the basic information? What are the model-implied moments? Let’s remember our original equation. \\[\\left[\\begin{array} {r} Y_{i0} \\\\ Y_{i1} \\\\ Y_{i2} \\end{array}\\right] = \\left[\\begin{array} {r} X_{0} \\\\ X_{1} \\\\ X_{2} \\end{array}\\right] \\left[\\begin{array} {r} \\beta_{0} \\end{array}\\right] + \\left[\\begin{array} {r} Z_{0} \\\\ Z_{1} \\\\ Z_{2} \\end{array}\\right] \\left[\\begin{array} {r} u_{0i} \\end{array}\\right] + \\left[\\begin{array} {r} e_{i0} \\\\ e_{i1} \\\\ e_{i2} \\end{array}\\right]\\] In this unconditional means model the \\(X\\) and \\(Z\\) design matrices are simply vectors of \\(1\\)s, leaving us with \\[\\left[\\begin{array} {r} Y_{i0} \\\\ Y_{i1} \\\\ Y_{i2} \\end{array}\\right] = \\left[\\begin{array} {r} 1 \\\\ 1 \\\\ 1 \\end{array}\\right] \\left[\\begin{array} {r} \\beta_{0} \\end{array}\\right] + \\left[\\begin{array} {r} 1 \\\\ 1 \\\\ 1 \\end{array}\\right] \\left[\\begin{array} {r} u_{0i} \\end{array}\\right] + \\left[\\begin{array} {r} e_{i0} \\\\ e_{i1} \\\\ e_{i2} \\end{array}\\right]\\]. 12.4.9.1 Mean Vector To obtain the model-implied mean vector we want \\(\\mathbb{E}(\\mathbf{Y})\\). Remember, from our covariance algebra \\[ \\mathbb{E}(\\mathbf{A}+\\mathbf{B}+\\mathbf{C}) = \\mathbb{E}(\\mathbf{A}) +\\mathbb{E}(\\mathbf{B})+\\mathbb{E}(\\mathbf{C}) \\] which gives us \\[\\begin{align} \\mathbb{E} \\left( \\left[\\begin{array} {r} Y_{i0} \\\\ Y_{i1} \\\\ Y_{i2} \\end{array}\\right] \\right) = &amp; \\mathbb{E}\\left( \\left[\\begin{array} {r} 1 \\\\ 1 \\\\ 1 \\end{array}\\right] \\left[\\begin{array} {r} \\beta_{0} \\end{array}\\right] \\right) + \\mathbb{E}\\left( \\left[\\begin{array} {r} 1 \\\\ 1 \\\\ 1 \\end{array}\\right] \\left[\\begin{array} {r} u_{0i} \\end{array}\\right] \\right) + \\mathbb{E}\\left( \\left[\\begin{array} {r} e_{i0} \\\\ e_{i1} \\\\ e_{i2} \\end{array}\\right] \\right) \\end{align}\\] or after simplifying \\[\\begin{align} \\mathbb{E} \\left( \\mathbf{Y} \\right) = &amp; \\mathbb{E}\\left( \\beta_{0}\\right) + \\mathbf{0} + \\mathbf{0} \\\\ \\end{align}\\] 12.4.9.2 Mean Vector in R Let’s make the implied mean vector in R. First, extract the fixed effects from the model using fixef(), specifically the contents of the \\(\\beta\\) matrix. fixef(um_fit) ## (Intercept) ## 33.92433 beta &lt;- matrix(fixef(um_fit)[1], nrow = 1, ncol = 1) beta ## [,1] ## [1,] 33.92433 Create the model design matrix for the fixed effects. In this model this is a matrix of order \\(3 \\times 1\\). X &lt;- matrix(c(1,1,1), nrow = 3, ncol = 1) X ## [,1] ## [1,] 1 ## [2,] 1 ## [3,] 1 Creating the model implied mean vector through multiplication \\[ \\mathbf{Y} = \\mathbf{X}\\mathbf{\\beta} + 0 + 0 \\] meanvector_um &lt;- X %*% beta meanvector_um ## [,1] ## [1,] 33.92433 ## [2,] 33.92433 ## [3,] 33.92433 Note this is the overall (grand) mean. 12.4.9.3 Model-Implied Covariance Matrix Now, let’s take a look at the model-implied variance-covariance matrix. Before we start let’s review the model again, \\[ \\boldsymbol{Y}_i = \\boldsymbol{X}_i\\boldsymbol{\\beta} + \\boldsymbol{Z}_i\\boldsymbol{u}_i + \\boldsymbol{e}_i \\] where \\(\\mathbf{Z}_i\\) is the random effects regressor (design) matrix; \\(\\boldsymbol{\\beta}\\) contains the fixed effects; \\(\\boldsymbol{u}_i\\) contains the random effects which are distributed normally with \\(0\\) mean and covariance matrix \\(\\mathbf{\\Psi}\\) and \\(\\boldsymbol{e}_i\\) are errors which are distributed normally with \\(0\\) mean and covariance matrix \\(\\mathbf{\\Lambda_{i}}\\), and our “standard assumption” was that \\(\\mathbf{\\Lambda_{i}} = \\mathbf{\\sigma^2}\\mathbf{I}\\) (homogeneity of errors). We’d like to identify the quantity \\(\\mathbb{C}ov(\\mathbf{Y})\\). We subtract the “means” from both sides … \\[\\left[\\begin{array} {r} Y_{i0} \\\\ Y_{i1} \\\\ Y_{i2} \\end{array}\\right] - \\left[\\begin{array} {r} X_{0} \\\\ X_{1} \\\\ X_{2} \\end{array}\\right] \\left[\\begin{array} {r} \\beta_{0} \\end{array}\\right] = \\left[\\begin{array} {r} Z_{0} \\\\ Z_{1} \\\\ Z_{2} \\end{array}\\right] \\left[\\begin{array} {r} u_{0i} \\end{array}\\right] + \\left[\\begin{array} {r} e_{i0} \\\\ e_{i1} \\\\ e_{i2} \\end{array}\\right]\\] So on the left side we now have de-meaned scores and on the right we have a between-portion part and a within-person part. Note that \\(\\mathbf{Y}^{*}\\) is now mean-centered, and as such, \\(\\mathbb{C}ov(\\mathbf{Y}) = \\mathbf{Y}^{*}\\mathbf{Y}^{*&#39;}\\). This gives us \\[\\begin{align} \\mathbf{Y}^{*} \\mathbf{Y}^{*&#39;} = (\\mathbf{Z}\\mu_{0i} + \\boldsymbol{\\epsilon})(\\mathbf{Z}\\mu_{0i} + \\boldsymbol{\\epsilon})^{&#39;} \\\\ = (\\mathbf{Z}\\mu_{0i} + \\boldsymbol{\\epsilon})(\\mu_{0i}^{&#39;}\\mathbf{Z}^{&#39;} + \\boldsymbol{\\epsilon}^{&#39;}) &amp; \\quad \\text{[Distribute transpose]}\\\\ = \\mathbf{Z}\\mu_{0i}\\mu_{0i}^{&#39;}\\mathbf{Z}^{&#39;} + \\mathbf{Z}\\mu_{0i}\\boldsymbol{\\epsilon}^{&#39;} + \\boldsymbol{\\epsilon}\\mu_{0i}^{&#39;}\\mathbf{Z}^{&#39;} + \\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^{&#39;} &amp; \\quad \\text{[Expand]}\\\\ = \\mathbf{Z}\\mu_{0i}\\mu_{0i}^{&#39;}\\mathbf{Z}^{&#39;} + \\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^{&#39;} &amp; \\quad \\text{[Orthogonality]}\\\\ = \\mathbf{Z}\\boldsymbol{\\Psi}\\mathbf{Z}^{&#39;} + \\boldsymbol{\\Lambda} &amp; \\quad \\text{[De. \\: Covariances]}\\\\ \\end{align}\\] Or we can alternatively see this in matrix form, without recalculating all the steps. \\[\\left[\\begin{array} {r} Y^*_{i0} \\\\ Y^*_{i1} \\\\ Y^*_{i2} \\end{array}\\right] = \\left[\\begin{array} {r} Z_{0} \\\\ Z_{1} \\\\ Z_{2} \\end{array}\\right] \\left[\\begin{array} {r} u_{0i} \\end{array}\\right] + \\left[\\begin{array} {r} e_{i0} \\\\ e_{i1} \\\\ e_{i2} \\end{array}\\right]\\] Let’s calculate var-cov matrix … \\[\\left[\\begin{array} {r} Y^*_{i0} \\\\ Y^*_{i1} \\\\ Y^*_{i2} \\end{array}\\right] \\left[\\begin{array} {r} Y^*_{i0} &amp; Y^*_{i1} &amp; Y^*_{i2} \\end{array}\\right] = \\left[\\begin{array} {r} Z_{0} \\\\ Z_{1} \\\\ Z_{2} \\end{array}\\right] \\left[\\begin{array} {r} u_{0i} \\end{array}\\right] \\left[\\begin{array} {r} u_{0i} \\end{array}\\right] \\left[\\begin{array} {r} Z_{0} &amp; Z_{1} &amp; Z_{2} \\end{array}\\right] + \\left[\\begin{array} {r} e_{i0} \\\\ e_{i1} \\\\ e_{i2} \\end{array}\\right] \\left[\\begin{array} {r} e_{i0} &amp; e_{i1} &amp; e_{i2} \\end{array}\\right]\\] From above we get \\[\\left[\\begin{array} {r} \\hat\\Sigma \\end{array}\\right] = \\left[\\begin{array} {r} Z_{0} \\\\ Z_{1} \\\\ Z_{2} \\end{array}\\right] \\left[\\begin{array} {r} \\Psi \\end{array}\\right] \\left[\\begin{array} {r} Z_{0} &amp; Z_{1} &amp; Z_{2} \\end{array}\\right] + \\left[\\begin{array} {r} \\sigma^2_{e} \\end{array}\\right] \\left[\\begin{array} {r} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{array}\\right]\\] 12.4.9.4 Covariance Matrix in R #parsing the model variances &amp; covariances VarCorr(um_fit) ## id = pdLogChol(1) ## Variance StdDev ## (Intercept) 19.41339 4.406063 ## Residual 105.61668 10.276997 So, in order to reconstruct the implied variance-covariances, we need to find \\(\\mathbf{\\Psi}\\) and \\(\\mathbf{\\sigma^2}\\) to create \\(\\boldsymbol{\\Lambda}\\), and do some multiplication. Parse the between-person variances from the model output. Psi &lt;- matrix(c(as.numeric(VarCorr(um_fit)[1])),nrow = 1, ncol = 1) Psi ## [,1] ## [1,] 19.41339 Create the model design matrix, \\(\\mathbf{Z}\\), for the random effects. In this model this is a matrix of order \\(3 \\times 1\\) Z &lt;- matrix(c(1,1,1), nrow=3,ncol=1) Z ## [,1] ## [1,] 1 ## [2,] 1 ## [3,] 1 So, the implied variance-covariance matrix of the between-person random effects for the three occasions is: Cov1 = Z %*% Psi %*% t(Z) Cov1 ## [,1] [,2] [,3] ## [1,] 19.41339 19.41339 19.41339 ## [2,] 19.41339 19.41339 19.41339 ## [3,] 19.41339 19.41339 19.41339 Which in correlation units implies Delta = diag(3) Delta[1,1] = 1/sqrt(Cov1[1,1]) Delta[2,2] = 1/sqrt(Cov1[2,2]) Delta[3,3] = 1/sqrt(Cov1[3,3]) Delta %*% Cov1 %*% Delta ## [,1] [,2] [,3] ## [1,] 1 1 1 ## [2,] 1 1 1 ## [3,] 1 1 1 Now, let’s look at the residual error variance-covariance matrix, sigma2 &lt;- as.numeric(VarCorr(um_fit)[2]) Lambda &lt;- sigma2 * diag(1, nrow = 3, ncol = 3) Finally, can put the between- and within- pieces together to calculate the model-implied variance-covariance matrix as follows varcovmatrix_um &lt;- Z %*% Psi %*% t(Z) + (sigma2 * Lambda) varcovmatrix_um ## [,1] [,2] [,3] ## [1,] 11174.29648 19.41339 19.41339 ## [2,] 19.41339 11174.29648 19.41339 ## [3,] 19.41339 19.41339 11174.29648 Note, we have a compound symmetry structure in our model-implied covariance matrix. Together with the implied mean vector, we have the entire picture provided by all the model components. 12.4.10 Model Residuals Recall what the observed mean and var-cov were … meanvector ## verb2 verb4 verb6 ## 25.41534 32.60775 43.74990 varcovmatrix ## verb2 verb4 verb6 ## verb2 37.28784 33.81957 47.40488 ## verb4 33.81957 53.58070 62.25489 ## verb6 47.40488 62.25489 113.74332 For fun, let’s look at the misfit to the data (observed matrix - model implied matrix) meanvector - meanvector_um ## [,1] ## [1,] -8.508987 ## [2,] -1.316585 ## [3,] 9.825572 varcovmatrix - varcovmatrix_um ## verb2 verb4 verb6 ## verb2 -11137.00864 14.40618 27.99149 ## verb4 14.40618 -11120.71578 42.84150 ## verb6 27.99149 42.84150 -11060.55317 Fit is not so good. Let’s visualize the implied model. #Calculating predicted scores from the models verblong$pred_um &lt;- predict(um_fit) #Making the prototype from the implied means proto_um &lt;- data.frame(cbind(c(1000,1000,1000),c(0,1,2),meanvector_um)) names(proto_um) &lt;- c(&quot;id&quot;,&quot;time0&quot;,&quot;pred_um&quot;) #plotting implied individual scores ggplot(data = verblong, aes(x = time0, y = pred_um, group = id)) + ggtitle(&quot;Unconditional Means Model&quot;) + geom_point() + geom_line() + geom_point(data=proto_um, color=&quot;red&quot;, size=2) + geom_line(data=proto_um, color=&quot;red&quot;, size=1) + xlab(&quot;Time&quot;) + ylab(&quot;WISC Verbal Score&quot;) + ylim(0,100) + xlim(0,2) "],["12.5-repeated-measures-anova.html", "12.5 Repeated Measures ANOVA", " 12.5 Repeated Measures ANOVA OK, now let’s move to an repeated measures (RM) ANOVA by adding in effects for categorical time. Recall, we must tell R that time0 is a categorical variable using factor() verblong$time0 &lt;- factor(verblong$time0, ordered=FALSE) str(verblong$time0) ## Factor w/ 3 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;: 1 2 3 1 2 3 1 2 3 1 ... Here is our RM ANOVA model with time as within-person factor timecat_fit &lt;- lme( fixed = verb ~ 1 + time0, random = ~ 1|id, data = verblong, na.action = na.exclude) summary(timecat_fit) ## Linear mixed-effects model fit by REML ## Data: verblong ## AIC BIC logLik ## 4013.176 4035.235 -2001.588 ## ## Random effects: ## Formula: ~1 | id ## (Intercept) Residual ## StdDev: 6.915667 4.514145 ## ## Fixed effects: verb ~ 1 + time0 ## Value Std.Error DF t-value p-value ## (Intercept) 25.415343 0.5782155 406 43.95480 0 ## time01 7.192402 0.4469670 406 16.09157 0 ## time02 18.334559 0.4469670 406 41.01994 0 ## Correlation: ## (Intr) time01 ## time01 -0.387 ## time02 -0.387 0.500 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -2.61263084 -0.54466998 -0.02451831 0.48627468 3.50536334 ## ## Number of Observations: 612 ## Number of Groups: 204 # timecat_fit2 &lt;- lmer(verb ~ 1 + time0 + (1|id), # data=verblong, # na.action = na.exclude) # summary(timecat_fit2) Remember that interpretation is with respect to time0, with the first category set as default intercept which is str(verblong$time0) ## Factor w/ 3 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;: 1 2 3 1 2 3 1 2 3 1 ... 12.5.1 Intra-Class Correlation The intra-class correlation (ICC) as the ratio of the random intercept variance (between-person) to the total variance, defined as the sum of the random intercept variance and residual variance (between + within). Specifically, \\[ICC_{between} = \\frac{\\sigma^{2}_{u0}}{\\sigma^{2}_{u0} + \\sigma^{2}_{e}}\\] 12.5.1.1 Calculating the ICC The ICC is the ratio of the random intercept variance (between-person var) over the total variance (between + within var): # Simple function for computing ICC from lme() output ICClme &lt;- function(out) { varests &lt;- as.numeric(VarCorr(out)[1:2]) return(paste(&quot;ICC =&quot;, varests[1]/sum(varests))) } ICClme(timecat_fit) ## [1] &quot;ICC = 0.701226878908497&quot; From the current model, the ICC was calculated, which indicated that of the total variance in verbal scores, approximately 70%, is attributable to between-person variation whereas 30% is attributable to within-person variation. 12.5.2 Model-Implied Mean Vector Let’s be explicit with our model \\[\\left[\\begin{array} {r} Y_{i0} \\\\ Y_{i1} \\\\ Y_{i2} \\end{array}\\right] = \\left[\\begin{array} {r} 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 \\end{array}\\right] \\left[\\begin{array} {r} \\beta_{0} \\\\ \\beta_{1} \\\\ \\beta_{2} \\end{array}\\right] + \\left[\\begin{array} {r} 1 \\\\ 1 \\\\ 1 \\end{array}\\right] \\left[\\begin{array} {r} u_{0i} \\end{array}\\right] + \\left[\\begin{array} {r} e_{i0} \\\\ e_{i1} \\\\ e_{i2} \\end{array}\\right]\\]. Making the implied mean vector fixef(timecat_fit) ## (Intercept) time01 time02 ## 25.415343 7.192402 18.334559 beta &lt;- matrix( c( fixef(timecat_fit)[1], fixef(timecat_fit)[2], fixef(timecat_fit)[3] ), nrow =3, ncol=1) beta ## [,1] ## [1,] 25.415343 ## [2,] 7.192402 ## [3,] 18.334559 Create the model design matrix for the fixed effects. In this model this is a matrix of order \\(3 \\times 3\\). X &lt;- matrix(c(1,1,1,0,1,0,0,0,1), nrow=3, ncol=3) X ## [,1] [,2] [,3] ## [1,] 1 0 0 ## [2,] 1 1 0 ## [3,] 1 0 1 Creating the model implied mean vector through multiplication \\[ \\mathbf{Y} = \\mathbf{X}\\mathbf{\\beta} + 0 + \\] meanvector_timecat &lt;- X %*% beta meanvector_timecat ## [,1] ## [1,] 25.41534 ## [2,] 32.60775 ## [3,] 43.74990 See the differences in the means across levels of time0. 12.5.3 Model-Implied Covariance Matrix Making the implied variance-covariance matrix. VarCorr(timecat_fit) ## id = pdLogChol(1) ## Variance StdDev ## (Intercept) 47.82645 6.915667 ## Residual 20.37751 4.514145 From this, we need to create the model implied variance-covariance. Parse the between-person variances from the model output. Psi &lt;- matrix(c(as.numeric(VarCorr(timecat_fit)[1])), nrow=1,ncol=1) Psi ## [,1] ## [1,] 47.82645 Create the model design matrix for the random effects. In this model this is a matrix of order \\(3 \\times 1\\) Z &lt;- matrix(c(1,1,1), nrow=3,ncol=1) Z ## [,1] ## [1,] 1 ## [2,] 1 ## [3,] 1 So, the implied variance-covariance matrix of the between-person random effects for the three occasions is: Z %*% Psi %*% t(Z) ## [,1] [,2] [,3] ## [1,] 47.82645 47.82645 47.82645 ## [2,] 47.82645 47.82645 47.82645 ## [3,] 47.82645 47.82645 47.82645 Next, we parse the residual/“error” variance-covariance. sigma2 &lt;- as.numeric(VarCorr(timecat_fit)[2]) Lambda &lt;- sigma2 * diag(1,nrow=3,ncol=3) So the residual within-person residual/“error” structure Lambda ## [,1] [,2] [,3] ## [1,] 20.37751 0.00000 0.00000 ## [2,] 0.00000 20.37751 0.00000 ## [3,] 0.00000 0.00000 20.37751 As before, we have homogeneity and uncorrelated errors. Finally, calculate the implied variance-covariances of total model varcovmatrix_timecat &lt;- Z %*% Psi %*% t(Z) + Lambda varcovmatrix_timecat ## [,1] [,2] [,3] ## [1,] 68.20396 47.82645 47.82645 ## [2,] 47.82645 68.20396 47.82645 ## [3,] 47.82645 47.82645 68.20396 Again, notice the compound symmetry structure. For fun, let’s look at the misfit (real matrix - model implied) #misfit of means meanvector - meanvector_timecat ## [,1] ## [1,] 4.618528e-14 ## [2,] -9.947598e-14 ## [3,] 1.421085e-14 #misfit of var-cov varcovmatrix - varcovmatrix_timecat ## verb2 verb4 verb6 ## verb2 -30.9161173 -14.00688 -0.4215677 ## verb4 -14.0068803 -14.62326 14.4284384 ## verb6 -0.4215677 14.42844 45.5393553 The means are now perfectly reproduced, however, the variances and covariances are not so good, particularly the variances Let’s make a picture of the implied model. #Calculating predicted scores from the models verblong$pred_timecat &lt;- predict(timecat_fit) #Making the prototype from the implied means proto_timecat &lt;- data.frame(cbind(c(1000,1000,1000),c(0,1,2),meanvector_timecat)) names(proto_timecat) &lt;- c(&quot;id&quot;,&quot;time0&quot;,&quot;pred_timecat&quot;) #need to convert time0 back into a continuous variable for plotting as intraindividual change verblong$time0 &lt;- as.numeric(unclass(verblong$time0))-1 #plotting implied individual scores ggplot(data = verblong, aes(x = time0, y = pred_timecat, group = id)) + ggtitle(&quot;RM ANOVA Model (time categorical)&quot;) + geom_point() + geom_line() + geom_point(data=proto_timecat, color=&quot;red&quot;, size=2) + geom_line(data=proto_timecat, color=&quot;red&quot;, size=1, linetype=2) + xlab(&quot;Time&quot;) + ylab(&quot;WISC Verbal Score&quot;) + ylim(0,100) + xlim(0,2) + theme_classic() Notice that the implied lines are all parallel. This is a model of mean differences. Can also see what the residuals look like. #Calculating residual scores from the models verblong$resid_timecat &lt;- residuals(timecat_fit) #plotting implied individual scores ggplot(data = verblong, aes(x = time0, y = resid_timecat, group = id)) + ggtitle(&quot;RM ANOVA Model Residuals (time categorical)&quot;) + geom_point() + xlab(&quot;Time&quot;) + ylab(&quot;WISC Verbal Score&quot;) + xlim(0,2) + theme_classic() Note, that the points are not connected - this is to highlight the model and the implication that the within-person residuals are from the occasion-specific mean (NOT from a trajectory).Note also the heteroskedasticity of the residuals. "],["12.6-repeated-measures-manova.html", "12.6 Repeated Measures MANOVA", " 12.6 Repeated Measures MANOVA Now let’s adjust the RM ANOVA error structure to get a RM MANOVA. The multivariate part of the MANOVA, has to do with relaxing the assumption on the error structure - this means more flexibility than compound symmetry. Here is our RM ANOVA model with time as within-person factor. The error structure is compound symmetry with heterogeneous variances. verblong$time0 &lt;- factor(verblong$time0, ordered=FALSE) timecathet_fit &lt;- lme( fixed= verb ~ 1 + time0, random= ~ 1|id, weights=varIdent(form=~1|time0), data=verblong, na.action = na.exclude ) summary(timecathet_fit) ## Linear mixed-effects model fit by REML ## Data: verblong ## AIC BIC logLik ## 3930.253 3961.136 -1958.127 ## ## Random effects: ## Formula: ~1 | id ## (Intercept) Residual ## StdDev: 6.086682 2.871709 ## ## Variance function: ## Structure: Different standard deviations per stratum ## Formula: ~1 | time0 ## Parameter estimates: ## 0 1 2 ## 1.000000 1.254186 2.341456 ## Fixed effects: verb ~ 1 + time0 ## Value Std.Error DF t-value p-value ## (Intercept) 25.415343 0.4712021 406 53.93724 0 ## time01 7.192402 0.3225105 406 22.30130 0 ## time02 18.334559 0.5119102 406 35.81597 0 ## Correlation: ## (Intr) time01 ## time01 -0.266 ## time02 -0.168 0.245 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -2.24995336 -0.55415412 -0.05694858 0.51324627 2.99087738 ## ## Number of Observations: 612 ## Number of Groups: 204 12.6.1 Model-Implied Mean Vector So what is the implied representation of the basic information Making the implied mean vector. fixef(timecathet_fit) ## (Intercept) time01 time02 ## 25.415343 7.192402 18.334559 beta &lt;- matrix( c( fixef(timecathet_fit)[1], fixef(timecathet_fit)[2], fixef(timecathet_fit)[3] ), nrow=3, ncol=1) beta ## [,1] ## [1,] 25.415343 ## [2,] 7.192402 ## [3,] 18.334559 Create the model design matrix, X, for the fixed effects. In this model this is a matrix of order \\(3 \\times 3\\). X &lt;- matrix(c(1,1,1,0,1,0,0,0,1), nrow=3,ncol=3) X ## [,1] [,2] [,3] ## [1,] 1 0 0 ## [2,] 1 1 0 ## [3,] 1 0 1 Creating the model implied mean vector through multiplication \\[ \\mathbf{Y} = \\mathbf{X}\\mathbf{\\beta} + 0 + 0 \\] meanvector_timecathet &lt;- X %*% beta meanvector_timecathet ## [,1] ## [1,] 25.41534 ## [2,] 32.60775 ## [3,] 43.74990 See the differences in the means across levels of time0. This is exactly the same as in the last model 12.6.2 Model-Implied Covariance Matrix #parse the between-person variances Psi &lt;- matrix(c(as.numeric(VarCorr(timecathet_fit)[1])),nrow=1,ncol=1) Psi ## [,1] ## [1,] 37.0477 #create the model design matrix Z &lt;- matrix(c(1,1,1), nrow=3,ncol=1) Z ## [,1] ## [1,] 1 ## [2,] 1 ## [3,] 1 So the implied variance covariance of the between-person random effects for the three repeated measures is: Z %*% Psi %*% t(Z) ## [,1] [,2] [,3] ## [1,] 37.0477 37.0477 37.0477 ## [2,] 37.0477 37.0477 37.0477 ## [3,] 37.0477 37.0477 37.0477 Now for the within-person residual var-cov. #parse the residual/&quot;error&quot; variance-covariance sigma &lt;- as.numeric(VarCorr(timecathet_fit)[4]) #note this is standard deviation sigma ## [1] 2.871709 Note: Now we have another step due to the heterogeneous variances. We have to get the heterogeneous weights for the residual standard deviation from the summary(timecathet_fit). # Variance function: # Structure: Different standard deviations per stratum # Formula: ~1 | time0 # Parameter estimates: # 0 1 2 # 1.000000 1.254186 2.341456 Pulling from the output. sigmahet &lt;- sigma * (diag(c(1.000000, 1.254186, 2.341456), nrow=3,ncol=3)) #Calculate the implied residual error variance-covariance Lambda &lt;- sigmahet^2 Finally calculating the model implied between- + within var-cov structure. varcovmatrix_timecathet &lt;- Z %*% Psi %*% t(Z) + Lambda varcovmatrix_timecathet ## [,1] [,2] [,3] ## [1,] 45.29441 37.04770 37.04770 ## [2,] 37.04770 50.01964 37.04770 ## [3,] 37.04770 37.04770 82.25961 Note the heterogeneity we now have along the diagonal. Again, Let’s look at the misfit (real matrix - model implied) meanvector - meanvector_timecathet ## [,1] ## [1,] -5.329071e-14 ## [2,] -4.973799e-14 ## [3,] 1.136868e-13 varcovmatrix - varcovmatrix_timecathet ## verb2 verb4 verb6 ## verb2 -8.006572 -3.228132 10.35718 ## verb4 -3.228132 3.561067 25.20719 ## verb6 10.357180 25.207186 31.48370 Getting better. Note, we can formally compare the models. anova(timecat_fit, timecathet_fit) ## Model df AIC BIC logLik Test L.Ratio p-value ## timecat_fit 1 5 4013.176 4035.235 -2001.588 ## timecathet_fit 2 7 3930.253 3961.136 -1958.127 1 vs 2 86.92304 &lt;.0001 Remember, the null hypothesis of the LRT states that the more constrained model provides as good a fit for the data as the less constrained model. If the null hypothesis is rejected, then the alternative, unconstrained model provides a significant improvement in fit over the smaller model. Thus, allowing for the additional heterogeneity improved our model. "],["12.7-repeated-measures-manova-unstructured.html", "12.7 Repeated Measures MANOVA (Unstructured)", " 12.7 Repeated Measures MANOVA (Unstructured) Let’s adjust the RM MANOVA error structure to have no constraints. To do this in lme() we can use the correlation argument. We can now use the weights argument to indicate that the residual variances will be a function of time. Option correlation = corSymm(form=~1|id) specifies that the correlation structure is unstructured. verblong$time0 &lt;- factor(verblong$time0, ordered=FALSE) timecatunst_fit &lt;- lme( fixed= verb ~ 1 + time0, random= ~ 1|id, weights=varIdent(form=~1|time0), correlation=corSymm(form=~1|id), data=verblong, na.action = na.exclude ) summary(timecatunst_fit) ## Linear mixed-effects model fit by REML ## Data: verblong ## AIC BIC logLik ## 3869.06 3913.178 -1924.53 ## ## Random effects: ## Formula: ~1 | id ## (Intercept) Residual ## StdDev: 5.506716 2.638933 ## ## Correlation Structure: General ## Formula: ~1 | id ## Parameter estimate(s): ## Correlation: ## 1 2 ## 2 0.275 ## 3 0.709 0.725 ## Variance function: ## Structure: Different standard deviations per stratum ## Formula: ~1 | time0 ## Parameter estimates: ## 0 1 2 ## 1.000000 1.827454 3.461032 ## Fixed effects: verb ~ 1 + time0 ## Value Std.Error DF t-value p-value ## (Intercept) 25.415343 0.4275322 406 59.44662 0 ## time01 7.192402 0.3374453 406 21.31428 0 ## time02 18.334559 0.5249720 406 34.92483 0 ## Correlation: ## (Intr) time01 ## time01 -0.118 ## time02 0.221 0.507 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -2.24169379 -0.62481084 -0.06273466 0.63549118 3.06610136 ## ## Number of Observations: 612 ## Number of Groups: 204 12.7.1 Model-Implied Mean Vector There will be no differences in the model implied mean vector in this model. beta &lt;- matrix(fixef(timecatunst_fit), nrow=3, ncol=1) X &lt;- matrix(c(1,1,1,0,1,0,0,0,1), nrow=3,ncol=3) meanvector_timecatunst &lt;- X %*% beta meanvector_timecatunst ## [,1] ## [1,] 25.41534 ## [2,] 32.60775 ## [3,] 43.74990 12.7.2 Model-Implied Covariance Matrix Again, we start with the between-part of our model. #parsing the model variances &amp; covariances VarCorr(timecatunst_fit) ## id = pdLogChol(1) ## Variance StdDev ## (Intercept) 30.323922 5.506716 ## Residual 6.963968 2.638933 Psi &lt;- matrix(c(as.numeric(VarCorr(timecatunst_fit)[1])),nrow=1,ncol=1) Z &lt;- matrix(c(1,1,1), nrow=3,ncol=1) Z %*% Psi %*% t(Z) ## [,1] [,2] [,3] ## [1,] 30.32392 30.32392 30.32392 ## [2,] 30.32392 30.32392 30.32392 ## [3,] 30.32392 30.32392 30.32392 Now it gets a bit more complicated, because we have a fully unstructured matrix for the within-person residual variance-covariance. #parse the residual/&quot;error&quot; variance-covariance sigma &lt;- as.numeric(VarCorr(timecatunst_fit)[4]) #note this is standard deviation #From the summary(timecatunst_fit) above # Variance function: # Structure: Different standard deviations per stratum # Formula: ~1 | time0 # Parameter estimates: # 0 1 2 # 1.000000 1.827573 3.461330 sigmaunst &lt;- sigma * (diag(c(1.000000, 1.827573, 3.461330), nrow=3,ncol=3)) #From the summary(timecatunst_fit) above # Correlation Structure: General # Formula: ~1 | id # Parameter estimate(s): # Correlation: # 1 2 # 2 0.275 # 3 0.709 0.725 cormatrixunst &lt;- matrix(c(1.000, 0.275, 0.709, 0.275, 1.000, 0.725, 0.709, 0.725, 1.000), nrow=3,ncol=3) #Pre and post multiply by SDs to convert Correlation matrix into #Covariance matrix covresidunst &lt;- sigmaunst %*% cormatrixunst %*% t(sigmaunst) covresidunst ## [,1] [,2] [,3] ## [1,] 6.963967 3.499969 17.09015 ## [2,] 3.499969 23.259812 31.93835 ## [3,] 17.090154 31.938350 83.43394 Finally, calculate the implied between- + within-person variance-covariances varcovmatrix_timecatunst &lt;- Z %*% Psi %*% t(Z) + covresidunst varcovmatrix_timecatunst ## [,1] [,2] [,3] ## [1,] 37.28789 33.82389 47.41408 ## [2,] 33.82389 53.58373 62.26227 ## [3,] 47.41408 62.26227 113.75786 Let’s look at the misfit (real matrix - model implied). meanvector - meanvector_timecatunst ## [,1] ## [1,] 1.421085e-13 ## [2,] 2.629008e-13 ## [3,] 2.557954e-13 varcovmatrix - varcovmatrix_timecatunst ## verb2 verb4 verb6 ## verb2 -4.663967e-05 -0.004320914 -0.009193447 ## verb4 -4.320914e-03 -0.003030440 -0.007383449 ## verb6 -9.193447e-03 -0.007383449 -0.014544496 It appears we have fully reproduced our observed data. We can formally test the improvement of this model using the anove() function. anova(timecathet_fit, timecatunst_fit) ## Model df AIC BIC logLik Test L.Ratio p-value ## timecathet_fit 1 7 3930.253 3961.136 -1958.127 ## timecatunst_fit 2 10 3869.060 3913.178 -1924.530 1 vs 2 67.19366 &lt;.0001 12.7.3 References "],["13-chapter-13.html", "Chapter 13 Growth Curve Modeling", " Chapter 13 Growth Curve Modeling In this chapter we work through basics of growth modeling. We describe the data, run some individual level models, and work through no-growth and linear growth modeling examples. We then expand on that model in two ways. We add a predictor - to get the conditional growth model and we change the time-metric - to illustrate how alternative time metrics facilitate different interpretations. "],["13.1-introduction-2.html", "13.1 Introduction", " 13.1 Introduction With growth curve modeling we are typically interested in anwsering some basic questions about change: How does the construct change over time/age? Are there interindividual differences in the level of a process? Are there interindividual differences in the rate of change over time? How is the level of the construct (at a specific time) related to the rate of change? What interindividual characteristic relate to interindividual differences in the level and/or rate of change? How are changes in one variable associated with changes in another variable? Are there time-dependent relationships in the development of two or more variables? "],["13.2-data-preparation-and-description-1.html", "13.2 Data Preparation and Description", " 13.2 Data Preparation and Description 13.2.1 Loading libraries used in this script. library(psych) #for basic functions library(ggplot2) #for plotting library(data.table) #for fast data management library(nlme) #for mixed effects models library(plyr) #for data management library(see) library(ggeffects) For our examples, we use 4-occasion WISC data. Load the repeated measures data. filepath &lt;- &quot;https://quantdev.ssri.psu.edu/sites/qdev/files/wisc3raw.csv&quot; wisc3raw &lt;- read.csv(file=url(filepath),header=TRUE) Subsetting to the variables of interest. Specifically, we include the id variable; the repeated measures outcome variables verb1, verb2, verb4, verb6; and the predictors grad and momed variables. varnames &lt;- c(&quot;id&quot;,&quot;verb1&quot;,&quot;verb2&quot;,&quot;verb4&quot;,&quot;verb6&quot;,&quot;grad&quot;,&quot;momed&quot;) wiscsub &lt;- wisc3raw[ ,varnames] describe(wiscsub) ## vars n mean sd median trimmed mad min max range skew ## id 1 204 102.50 59.03 102.50 102.50 75.61 1.00 204.00 203.00 0.00 ## verb1 2 204 19.59 5.81 19.34 19.50 5.41 3.33 35.15 31.82 0.13 ## verb2 3 204 25.42 6.11 25.98 25.40 6.57 5.95 39.85 33.90 -0.06 ## verb4 4 204 32.61 7.32 32.82 32.42 7.18 12.60 52.84 40.24 0.23 ## verb6 5 204 43.75 10.67 42.55 43.46 11.30 17.35 72.59 55.24 0.24 ## grad 6 204 0.23 0.42 0.00 0.16 0.00 0.00 1.00 1.00 1.30 ## momed 7 204 10.81 2.70 11.50 11.00 2.97 5.50 18.00 12.50 -0.36 ## kurtosis se ## id -1.22 4.13 ## verb1 -0.05 0.41 ## verb2 -0.34 0.43 ## verb4 -0.08 0.51 ## verb6 -0.36 0.75 ## grad -0.30 0.03 ## momed 0.01 0.19 Multilevel modeling analyses typically require a tall (long) data set. So, we reshape from wide to tall: verblong &lt;- reshape( data=wiscsub, varying=c(&quot;verb1&quot;,&quot;verb2&quot;,&quot;verb4&quot;,&quot;verb6&quot;), timevar=&quot;grade&quot;, idvar=&quot;id&quot;, direction=&quot;long&quot;, sep=&quot;&quot; ) verblong &lt;- verblong[order(verblong$id,verblong$grade),c(&quot;id&quot;,&quot;grade&quot;,&quot;verb&quot;,&quot;grad&quot;,&quot;momed&quot;)] head(verblong,12) ## id grade verb grad momed ## 1.1 1 1 24.42 0 9.5 ## 1.2 1 2 26.98 0 9.5 ## 1.4 1 4 39.61 0 9.5 ## 1.6 1 6 55.64 0 9.5 ## 2.1 2 1 12.44 0 5.5 ## 2.2 2 2 14.38 0 5.5 ## 2.4 2 4 21.92 0 5.5 ## 2.6 2 6 37.81 0 5.5 ## 3.1 3 1 32.43 1 14.0 ## 3.2 3 2 33.51 1 14.0 ## 3.4 3 4 34.30 1 14.0 ## 3.6 3 6 50.18 1 14.0 "],["13.3-individual-growth-models.html", "13.3 Individual Growth Models", " 13.3 Individual Growth Models To introduce growth modeling we will begin with data from a single individual. Let’s make a dataset with just 1 person of interest, id = 23, verb_id23 &lt;- verblong[which(verblong$id == 23), ] 13.3.1 Visualizing Individual Change Let’s make a plot of this person’s data, ggplot(data = verb_id23, aes(x = grade, y = verb, group = id)) + geom_point() + xlab(&quot;Grade&quot;) + ylab(&quot;WISC Verbal Score&quot;) + ylim(0,100) + scale_x_continuous(breaks=seq(1,6,by=1)) + theme_classic() We could connect the dots to see time-adjacent changes. ggplot(data = verb_id23, aes(x = grade, y = verb, group = id)) + geom_point() + geom_line() + xlab(&quot;Grade&quot;) + ylab(&quot;WISC Verbal Score&quot;) + ylim(0,100) + scale_x_continuous(breaks=seq(1,6,by=1)) + theme_classic() We could also smooth over the repeated measuring using a line of best fit for this individual. ggplot(data = verb_id23, aes(x = grade, y = verb, group = id)) + geom_point() + geom_line() + geom_smooth(method=lm, se=FALSE,colour=&quot;red&quot;, size=1) + xlab(&quot;Grade&quot;) + ylab(&quot;WISC Verbal Score&quot;) + ylim(0,100) + scale_x_continuous(breaks=seq(1,6,by=1)) + theme_classic() ## `geom_smooth()` using formula = &#39;y ~ x&#39; Notice, we can summarize this line with two pieces of information, (1) an intercept, and (2) a slope, each unique to individual 23. 13.3.2 Multiple Individuals Let’s do an individual regression with time as a predictor. Conceptually, this is a model of intraindividual change corresponding to the plot above. #regress verb on grade linear_id23 &lt;- lm(formula = verb ~ 1 + grade, data = verb_id23, na.action=na.exclude) #show results summary(linear_id23) ## ## Call: ## lm(formula = verb ~ 1 + grade, data = verb_id23, na.action = na.exclude) ## ## Residuals: ## 23.1 23.2 23.4 23.6 ## -2.6556 3.5536 -0.4681 -0.4298 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 21.3247 3.1147 6.846 0.0207 * ## grade 1.6308 0.8251 1.977 0.1867 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.169 on 2 degrees of freedom ## Multiple R-squared: 0.6614, Adjusted R-squared: 0.4921 ## F-statistic: 3.907 on 1 and 2 DF, p-value: 0.1867 Let’s save the 3 parameters into objects and look at them. id23_reg_linear &lt;- as.list(coef(linear_id23)) id23_reg_linear ## $`(Intercept)` ## [1] 21.32475 ## ## $grade ## [1] 1.630847 Now let’s do the same thing for all the persons. We do this in a speedy way using the data.table package. #converting to a data.table object verblong_dt &lt;- data.table(verblong) #collecting regression output by id indiv_reg &lt;- verblong_dt[,c( reg_1 = as.list(coef(lm(verb ~ grade))) ),by=id] Let’s look at the moments of the parameters #converting back to data.frame names(indiv_reg) ## [1] &quot;id&quot; &quot;reg_1.(Intercept)&quot; &quot;reg_1.grade&quot; indiv_reg_data &lt;- as.data.frame(indiv_reg) #descriptives describe(indiv_reg_data[-1]) ## vars n mean sd median trimmed mad min max range ## reg_1.(Intercept) 1 204 15.15 5.27 15.14 15.23 5.12 0.50 29.41 28.91 ## reg_1.grade 2 204 4.67 1.55 4.60 4.61 1.53 1.41 9.38 7.97 ## skew kurtosis se ## reg_1.(Intercept) -0.08 0.06 0.37 ## reg_1.grade 0.38 0.02 0.11 #correlations among parameters cor(indiv_reg_data[-1], use=&quot;complete.obs&quot;,method=&quot;pearson&quot;) ## reg_1.(Intercept) reg_1.grade ## reg_1.(Intercept) 1.0000000 -0.1551763 ## reg_1.grade -0.1551763 1.0000000 #pairs in the psych library pairs.panels(indiv_reg_data[-1]) Here, each person has 2 scores (intercept + slope) from the individual-level regression models. Now, let’s plot some of the individual regressions #making intraindividual change plot ggplot(data = verblong[which(verblong$id &lt; 30),], aes(x = grade, y = verb, group = id)) + geom_smooth(method=lm,se=FALSE,colour=&quot;red&quot;, size=1) + xlab(&quot;Grade&quot;) + ylab(&quot;WISC Verbal Score&quot;) + ylim(0,100) + scale_x_continuous(breaks=seq(1,6,by=1)) + theme_classic() ## `geom_smooth()` using formula = &#39;y ~ x&#39; The characteristics of the latent trajectories are captured in two ways: Trajectory Means: The average value of the parameters governing the growth trajectory, pooled over the all individuals in the sample. This is the mean starting point and mean rate of change for the sample. These are often called fixed effects. Trajectory Variances: The variability of individual cases around the mean trajectory parameters. This is the individual variability in starting point and rate of change over time. Larger variances reflect larger variability in growth. These are often called random effects. To recap, means captures overall values of parameters that define growth. Variances capture individual variability in those parameters. Let’s move to our more familiar framework for handling the analysis of “collections” of regressions. "],["13.4-unconditional-means-model-1.html", "13.4 Unconditional Means Model", " 13.4 Unconditional Means Model We will begin by fitting the unconditional means model, or no growth model to the 4-ocasion WISC data. We use the nlme package for fitting mixed effects models, also known as multilevel (MLM) or hierarchical linear models (HLM). Specifically, we use the lme() function to fits the MLMs: - The ‘fixed’ argument takes the fixed model - The ‘random’ argument takes the random model - The ‘data’ argument specifies the data sources - The ‘na.action’ argument specifies how to handle missing data Baseline Model The Unconditional Means model contains a fixed and random intercept only. You can use the constant 1 to designate that only intercepts are being modeled. um_fit &lt;- lme( fixed= verb ~ 1, random = ~ 1|id, data = verblong, na.action = na.exclude, method = &quot;ML&quot; ) summary(um_fit) ## Linear mixed-effects model fit by maximum likelihood ## Data: verblong ## AIC BIC logLik ## 6347.936 6362.05 -3170.968 ## ## Random effects: ## Formula: ~1 | id ## (Intercept) Residual ## StdDev: 3.575939 11.30169 ## ## Fixed effects: verb ~ 1 ## Value Std.Error DF t-value p-value ## (Intercept) 30.33951 0.4684887 612 64.76039 0 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -1.9419919 -0.7619298 -0.1621987 0.5568904 3.3055064 ## ## Number of Observations: 816 ## Number of Groups: 204 Let’s extract the random effects with the VarCorr() function VarCorr(um_fit) ## id = pdLogChol(1) ## Variance StdDev ## (Intercept) 12.78734 3.575939 ## Residual 127.72821 11.301691 We can compute the intra-class correlation (ICC) as the ratio of the random intercept variance (between-person) to the total variance (between + within), that includes the error. First let’s store the variance estimates, which will be the first column of the VarCorr object (see above). RandomEffects &lt;- as.numeric(VarCorr(um_fit)[,1]) RandomEffects ## [1] 12.78734 127.72821 Next let’s compute the ICC. It is the ratio of the random intercept variance (between-person var) over the total variance (between + within var). ICC_between &lt;- RandomEffects[1]/(RandomEffects[1]+RandomEffects[2]) ICC_between ## [1] 0.09100302 From the results we seeM there is lots of within-person variance for us to explain. between-person variance = \\(9.2\\%\\) within-person variance = \\(100 - 9.2 = 91.8\\%\\) 13.4.1 Predicted Trajectories Place individual predictions and residuals from the unconditional means model um_fit into the dataframe verblong$pred_um &lt;- predict(um_fit) verblong$resid_um &lt;- residuals(um_fit) head(verblong) ## id grade verb grad momed pred_um resid_um ## 1.1 1 1 24.42 0 9.5 32.14754 -7.727545 ## 1.2 1 2 26.98 0 9.5 32.14754 -5.167545 ## 1.4 1 4 39.61 0 9.5 32.14754 7.462455 ## 1.6 1 6 55.64 0 9.5 32.14754 23.492455 ## 2.1 2 1 12.44 0 5.5 27.85120 -15.411203 ## 2.2 2 2 14.38 0 5.5 27.85120 -13.471203 We can make plots of the model outputs. Here we plot the between-person differences in levels (\\(9\\%\\)). ggplot(data = verblong, aes(x = grade, y = pred_um, group = id)) + ggtitle(&quot;Unconditional Means Model&quot;) + # geom_point() + geom_line() + xlab(&quot;Grade&quot;) + ylab(&quot;PREDICTED WISC Verbal Score&quot;) + ylim(0,100) + scale_x_continuous(breaks=seq(1,6,by=1)) + theme_classic() Here we plot the between-person differences in levels (\\(91\\%\\)). #plotting RESIDUAL intraindividual change ggplot(data = verblong, aes(x = grade, y = resid_um, group = id)) + ggtitle(&quot;Unconditional Means Model (residuals)&quot;) + # geom_point() + geom_line() + xlab(&quot;Grade&quot;) + ylab(&quot;RESIDUAL WISC Verbal Score&quot;) + #ylim(0,100) + Note the removal of limits on y-axis scale_x_continuous(breaks=seq(1,6,by=1)) + theme_classic() We cab also plot the predicted intraindividual change alongside the mean trajectory. fun_um &lt;- function(x) { 30.33951 + 0*x } ggplot(data = verblong, aes(x = grade, y = pred_um, group = id)) + ggtitle(&quot;Unconditional Means Model&quot;) + # geom_point() + geom_line() + xlab(&quot;Grade&quot;) + ylab(&quot;PREDICTED WISC Verbal Score&quot;) + ylim(0,100) + scale_x_continuous(breaks=seq(1,6,by=1)) + stat_function(fun=fun_um, color=&quot;red&quot;, size = 2) + theme_classic() ## Warning: Multiple drawing groups in `geom_function()` ## ℹ Did you use the correct group, colour, or fill aesthetics? Since it is often too messy to plot all individuals we can also subset the plot. randomsample &lt;- sample(verblong$id,20) ggplot(data = verblong[verblong$id %in% randomsample,], aes(x = grade, y = pred_um, group = id)) + ggtitle(&quot;Unconditional Means Model&quot;) + # geom_point() + geom_line() + xlab(&quot;Grade&quot;) + ylab(&quot;PREDICTED WISC Verbal Score&quot;) + ylim(0,100) + scale_x_continuous(breaks=seq(1,6,by=1)) + stat_function(fun=fun_um, color=&quot;red&quot;, size = 2) ## Warning: Multiple drawing groups in `geom_function()` ## ℹ Did you use the correct group, colour, or fill aesthetics? "],["13.5-linear-growth-model.html", "13.5 Linear Growth Model", " 13.5 Linear Growth Model Now let’s add in grade as a (time-varying) predictor. We look at the linear relation between the time variable (grade) and the outcome variable (verb). 13.5.1 Random Intercept Model My naming convention for objects here is: fixed linear (fl) and random intercept (ri) fl_ri_fit &lt;- lme( fixed = verb ~ 1 + grade, random = ~ 1|id, data=verblong, na.action = na.exclude, method = &quot;ML&quot; ) summary(fl_ri_fit) ## Linear mixed-effects model fit by maximum likelihood ## Data: verblong ## AIC BIC logLik ## 5225.662 5244.48 -2608.831 ## ## Random effects: ## Formula: ~1 | id ## (Intercept) Residual ## StdDev: 6.295478 4.510587 ## ## Fixed effects: verb ~ 1 + grade ## Value Std.Error DF t-value p-value ## (Intercept) 15.15099 0.5397641 611 28.06966 0 ## grade 4.67339 0.0823294 611 56.76454 0 ## Correlation: ## (Intr) ## grade -0.496 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -2.70731017 -0.54837346 -0.03479899 0.52061645 4.13309807 ## ## Number of Observations: 816 ## Number of Groups: 204 Let’s look at the predicted trajectories from this model #Place individual predictions and residuals into the dataframe verblong$pred_fl_ri &lt;- predict(fl_ri_fit) verblong$resid_fl_ri &lt;- residuals(fl_ri_fit) #Create a function for the mean trajectory fun_fl_ri &lt;- function(x) { 15.15099 + 4.67339*x } #plotting PREDICTED intraindividual change ggplot(data = verblong, aes(x = grade, y = pred_fl_ri, group = id)) + ggtitle(&quot;Fixed Linear, Random Intercept&quot;) + # geom_point() + geom_line() + xlab(&quot;Grade&quot;) + ylab(&quot;PREDICTED WISC Verbal Score&quot;) + ylim(0,100) + scale_x_continuous(breaks=seq(1,6,by=1)) + stat_function(fun=fun_fl_ri, color=&quot;red&quot;, size = 2) + theme_classic() ## Warning: Multiple drawing groups in `geom_function()` ## ℹ Did you use the correct group, colour, or fill aesthetics? Note how all the lines are parallel. This imples individual variability in starting point but a constant rate of change over time. Let’s look at the residuals. ggplot(data = verblong, aes(x = grade, y = resid_fl_ri, group = id)) + ggtitle(&quot;Fixed Linear, Random Intercept&quot;) + # geom_point() + geom_line() + xlab(&quot;Grade&quot;) + ylab(&quot;RESIDUAL WISC Verbal Score&quot;) + scale_x_continuous(breaks=seq(1,6,by=1)) + theme_classic() Note the differences in variance. 13.5.2 Random Intercept and Slopes Model Linear Growth Model fl_rl_fit &lt;- lme( fixed = verb ~ 1 + grade, random = ~ 1 + grade|id, data=verblong, na.action = na.exclude, method = &quot;ML&quot; ) summary(fl_rl_fit) ## Linear mixed-effects model fit by maximum likelihood ## Data: verblong ## AIC BIC logLik ## 5050.844 5079.071 -2519.422 ## ## Random effects: ## Formula: ~1 + grade | id ## Structure: General positive-definite, Log-Cholesky parametrization ## StdDev Corr ## (Intercept) 3.898138 (Intr) ## grade 1.236530 0.324 ## Residual 3.581590 ## ## Fixed effects: verb ~ 1 + grade ## Value Std.Error DF t-value p-value ## (Intercept) 15.15099 0.3681979 611 41.14905 0 ## grade 4.67339 0.1085687 611 43.04546 0 ## Correlation: ## (Intr) ## grade -0.155 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -2.61515106 -0.54356124 -0.02857258 0.52294874 3.17283557 ## ## Number of Observations: 816 ## Number of Groups: 204 We can take a look at the ICCs to see if we are doing any better explaining the within-person variance. RandomEffects &lt;- as.numeric(VarCorr(fl_rl_fit)[,1]) ICC_between &lt;- sum(RandomEffects[1:2])/sum(RandomEffects) ICC_between ## [1] 0.5659289 Let’s look at the predicted trajectories from this model. We can start by saving the predicted trajectories and residuals into our dataframe. verblong$pred_fl_rl &lt;- predict(fl_rl_fit) verblong$resid_fl_rl &lt;- residuals(fl_rl_fit) Next, we make a small function to plot the mean trajectory. Now we use the fixedf() function directly rather than type the numbers manually from the output. #Create a function for the prototype #fun_fl_rl &lt;- function(x) { # 15.15099 + 4.67339*x #} fun_fl_rl &lt;- function(x) { fixef(fl_rl_fit)[[1]] + fixef(fl_rl_fit)[[2]]*x } #plotting PREDICTED intraindividual change ggplot(data = verblong, aes(x = grade, y = pred_fl_rl, group = id)) + ggtitle(&quot;Fixed Linear, Random Linear&quot;) + # geom_point() + geom_line() + xlab(&quot;Grade&quot;) + ylab(&quot;PREDICTED WISC Verbal Score&quot;) + ylim(0,100) + scale_x_continuous(breaks=seq(1,6,by=1)) + stat_function(fun=fun_fl_rl, color=&quot;red&quot;, size = 2) + theme_classic() ## Warning: Multiple drawing groups in `geom_function()` ## ℹ Did you use the correct group, colour, or fill aesthetics? Note how the lines are no longer parallel. We now have a model that implies individual variability in the starting point and rate of change. We can also plot the residuals. #plotting RESIDUAL intraindividual change ggplot(data = verblong, aes(x = grade, y = resid_fl_rl, group = id)) + ggtitle(&quot;Fixed Linear, Random Linear&quot;) + # geom_point() + geom_line() + xlab(&quot;Grade&quot;) + ylab(&quot;RESIDUAL WISC Verbal Score&quot;) + scale_x_continuous(breaks=seq(1,6,by=1)) + theme_classic() This model did a bit better on getting the residual variances similar at all grades (in line with assumptions). 13.5.3 Model Comparison Let’s test the significance of having random slopes. We compare models by applying anova() function to examine difference in fit between the two nested models. anova(fl_ri_fit, fl_rl_fit) ## Model df AIC BIC logLik Test L.Ratio p-value ## fl_ri_fit 1 4 5225.662 5244.480 -2608.831 ## fl_rl_fit 2 6 5050.844 5079.071 -2519.422 1 vs 2 178.8176 &lt;.0001 From the test results we see they are different. This provides a significance test for the variance and covariance (2 degrees of freedom). 13.5.4 MLM and Individual Models Remember, earlier, we ran individual-level regressions. head(indiv_reg_data,12) ## id reg_1.(Intercept) reg_1.grade ## 1 1 15.940169 6.376102 ## 2 2 5.232712 5.047627 ## 3 3 26.838136 3.312881 ## 4 4 19.493729 4.614237 ## 5 5 19.145424 7.805254 ## 6 6 11.554576 5.224746 ## 7 7 2.903559 6.378136 ## 8 8 14.198814 1.957288 ## 9 9 10.816441 6.041864 ## 10 10 18.385763 5.097458 ## 11 11 16.319661 6.437797 ## 12 12 12.089661 3.667797 Let’s also obtain individual-level estimates from the MLM model. FE &lt;- fixef(fl_rl_fit) # fixed effects FE ## (Intercept) grade ## 15.15099 4.67339 RE &lt;- ranef(fl_rl_fit) # random effects head(RE) ## (Intercept) grade ## 1 2.112943 1.23363573 ## 2 -5.539719 -0.61673143 ## 3 5.640184 0.09800766 ## 4 2.526637 0.34162935 ## 5 5.399309 2.49279591 ## 6 -1.607547 0.06056861 We add the fixed effect and random effect parameters together to get analogues of the individual-level parameters. #Individual intercepts (MLM model based) MLM_intercept &lt;- FE[1] + RE[,1] #Individual slopes (MLM model based) MLM_grade &lt;- FE[2] + RE[,2] Let’s combine the individual regression intercepts and slopes and the model based intercepts and slopes together in order to compare. indiv_parm_combined &lt;- cbind(MLM_intercept,MLM_grade,indiv_reg_data[,2:3]) head(indiv_parm_combined) ## MLM_intercept MLM_grade reg_1.(Intercept) reg_1.grade ## 1 17.263936 5.907026 15.940169 6.376102 ## 2 9.611274 4.056658 5.232712 5.047627 ## 3 20.791176 4.771397 26.838136 3.312881 ## 4 17.677630 5.015019 19.493729 4.614237 ## 5 20.550302 7.166186 19.145424 7.805254 ## 6 13.543446 4.733958 11.554576 5.224746 Look at the descriptives statistics. describe(indiv_parm_combined) ## vars n mean sd median trimmed mad min max range ## MLM_intercept 1 204 15.15 3.26 15.35 15.12 3.25 4.69 23.78 19.09 ## MLM_grade 2 204 4.67 1.09 4.56 4.64 1.09 2.15 7.69 5.54 ## reg_1.(Intercept) 3 204 15.15 5.27 15.14 15.23 5.12 0.50 29.41 28.91 ## reg_1.grade 4 204 4.67 1.55 4.60 4.61 1.53 1.41 9.38 7.97 ## skew kurtosis se ## MLM_intercept 0.00 -0.23 0.23 ## MLM_grade 0.26 -0.31 0.08 ## reg_1.(Intercept) -0.08 0.06 0.37 ## reg_1.grade 0.38 0.02 0.11 round(cor(indiv_parm_combined),2) ## MLM_intercept MLM_grade reg_1.(Intercept) reg_1.grade ## MLM_intercept 1.00 0.68 0.89 0.31 ## MLM_grade 0.68 1.00 0.27 0.91 ## reg_1.(Intercept) 0.89 0.27 1.00 -0.16 ## reg_1.grade 0.31 0.91 -0.16 1.00 pairs.panels(indiv_parm_combined) Let’s compare the two predictions. What do you notice? library(see) #plotting PREDICTED intraindividual change GCMpred = ggplot(data = verblong, aes(x = grade, y = pred_fl_rl, group = id)) + ggtitle(&quot;Fixed Linear, Random Linear&quot;) + # geom_point() + geom_line() + xlab(&quot;Grade&quot;) + ylab(&quot;PREDICTED WISC Verbal Score&quot;) + ylim(0,100) + scale_x_continuous(breaks=seq(1,6,by=1)) + stat_function(fun=fun_fl_rl, color=&quot;red&quot;, size = 2) + theme_classic() #making intraindividual change plot IndPred = ggplot(data = verblong, aes(x = grade, y = verb, group = id)) + ggtitle(&quot;Individual level regressions only&quot;) + geom_smooth(method=lm,se=FALSE,colour=&quot;black&quot;, size=1) + xlab(&quot;Grade&quot;) + ylab(&quot;WISC Verbal Score&quot;) + ylim(0,100) + scale_x_continuous(breaks=seq(1,6,by=1)) + theme_classic() plots(GCMpred, IndPred) ## Warning: Multiple drawing groups in `geom_function()` ## ℹ Did you use the correct group, colour, or fill aesthetics? ## `geom_smooth()` using formula = &#39;y ~ x&#39; "],["13.6-quadratic-growth-model.html", "13.6 Quadratic Growth Model", " 13.6 Quadratic Growth Model Quadratic time model (MLM): random intercepts and linear and quadratic slopes. Quadratic Growth Model verblong$gradeSquared &lt;- (verblong$grade)^2 fq_rq_fit &lt;- lme( fixed = verb ~ 1 + grade + gradeSquared, random = ~ 1 + grade|id + gradeSquared|id, data=verblong, na.action = na.exclude, method = &quot;ML&quot; ) summary(fq_rq_fit) ## Linear mixed-effects model fit by maximum likelihood ## Data: verblong ## AIC BIC logLik ## 5226.412 5259.343 -2606.206 ## ## Random effects: ## Formula: ~1 + grade | id + gradeSquared | id ## Structure: General positive-definite, Log-Cholesky parametrization ## StdDev Corr ## (Intercept) 5.277258 (Intr) ## 1 + grade | id + gradeSquaredTRUE 5.277258 -0.288 ## Residual 4.491283 ## ## Fixed effects: verb ~ 1 + grade + gradeSquared ## Value Std.Error DF t-value p-value ## (Intercept) 16.347780 0.7508427 610 21.772576 0.0000 ## grade 3.695700 0.4344000 610 8.507596 0.0000 ## gradeSquared 0.138997 0.0606470 610 2.291899 0.0223 ## Correlation: ## (Intr) grade ## grade -0.750 ## gradeSquared 0.695 -0.982 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -2.79052272 -0.55253516 -0.02974189 0.51054065 4.07582660 ## ## Number of Observations: 816 ## Number of Groups: 204 Let’s look at the predicted trajectories. verblong$pred_fq_rq &lt;- predict(fq_rq_fit) verblong$resid_fq_rq &lt;- residuals(fq_rq_fit) fun_fq_rq &lt;- function(x) { fixef(fq_rq_fit)[[1]] + fixef(fq_rq_fit)[[2]]*x + fixef(fq_rq_fit)[[3]]*x^2 } ggplot(data = verblong, aes(x = grade, y = pred_fq_rq, group = id)) + ggtitle(&quot;Fixed Quadratic, Random Quadratic&quot;) + geom_line() + xlab(&quot;Grade&quot;) + ylab(&quot;PREDICTED WISC Verbal Score&quot;) + ylim(0,100) + scale_x_continuous(breaks=seq(1,6,by=1)) + stat_function(fun=fun_fq_rq, color=&quot;red&quot;, size = 2) + theme_classic() ## Warning: Multiple drawing groups in `geom_function()` ## ℹ Did you use the correct group, colour, or fill aesthetics? "],["13.7-conditional-growth-model.html", "13.7 Conditional Growth Model", " 13.7 Conditional Growth Model The growth curve modeling framework also allows us to consider other important questions regarding developmental change. For example, Are the noted between-person differences in the trajectories of change related to other measured between-person difference variables? Can the variability we see in in the intercept and slope be explained by one or more time-invariant covariates? 13.7.1 Conditional Growth Equation Conditional Growth Model 13.7.2 Conditional Growth Model 1 Let’s go back and look at our data. The data include 2 additional time-invariant covariates, momed and grad. Let’s add a predictor to our model. In this case we will add grad (which is coded 0,1) as a conditional predictor. cgm1_fit &lt;- lme( fixed= verb ~ 1 + grade + grad + grade:grad, random= ~ 1 + grade|id, data=verblong, na.action = na.exclude ) summary(cgm1_fit) ## Linear mixed-effects model fit by REML ## Data: verblong ## AIC BIC logLik ## 5031.753 5069.349 -2507.876 ## ## Random effects: ## Formula: ~1 + grade | id ## Structure: General positive-definite, Log-Cholesky parametrization ## StdDev Corr ## (Intercept) 3.761291 (Intr) ## grade 1.194941 0.255 ## Residual 3.581590 ## ## Fixed effects: verb ~ 1 + grade + grad + grade:grad ## Value Std.Error DF t-value p-value ## (Intercept) 14.533795 0.4098491 610 35.46133 0.0000 ## grade 4.483657 0.1205883 610 37.18152 0.0000 ## grad 2.737137 0.8630981 202 3.17129 0.0018 ## grade:grad 0.841424 0.2539460 610 3.31340 0.0010 ## Correlation: ## (Intr) grade grad ## grade -0.215 ## grad -0.475 0.102 ## grade:grad 0.102 -0.475 -0.215 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -2.57955642 -0.53942829 -0.01662113 0.51067259 3.19891929 ## ## Number of Observations: 816 ## Number of Groups: 204 13.7.2.1 Marginal Effects Let’s make grad (\\(=1\\)) and non-grad (\\(=0\\)) prototypical trajectories. #First lets extract the fixed effects FE &lt;- fixef(cgm1_fit) FE ## (Intercept) grade grad grade:grad ## 14.5337953 4.4836569 2.7371369 0.8414241 Create a function for the mean trends. #for grad = 0 fun_cgm_grad0 &lt;- function(x) { grad=0 FE[1] + FE[2]*x + FE[3]*grad + FE[4]*x*grad } #for grad = 1 fun_cgm_grad1 &lt;- function(x) { grad=1 FE[1] + FE[2]*x + FE[3]*grad + FE[4]*x*grad } Plot with the mean trends for no-grad (red) and grad (blue) trajectories. #plotting intraindividual change with overlay of group trends ggplot(data = verblong, aes(x = grade, y = verb, group = id)) + ggtitle(&quot;Raw Trajectories + Condition&quot;) + # geom_point() + geom_line() + xlab(&quot;Grade&quot;) + ylab(&quot;WISC Verbal Score&quot;) + ylim(0,100) + scale_x_continuous(breaks=seq(1,6,by=1)) + stat_function(fun=fun_cgm_grad0, color=&quot;red&quot;, size = 2) + stat_function(fun=fun_cgm_grad1, color=&quot;blue&quot;, size = 2) + theme_classic() ## Warning: Multiple drawing groups in `geom_function()` ## ℹ Did you use the correct group, colour, or fill aesthetics? ## Multiple drawing groups in `geom_function()` ## ℹ Did you use the correct group, colour, or fill aesthetics? Note, we can also use packages designed to produce and plot marginal effects. The ggeffects package is capable of producing marginal effects plots from `lme models. Here we produce the plots above in a single line of code. pr &lt;- ggeffects::ggpredict(cgm1_fit, terms = c(&quot;grade&quot;,&quot;grad&quot;)) plot(pr) 13.7.3 Conditional Growth Model 1 Now, let’s consider momed as a (continuous) predictor. First, we will center momed at sample-level mean. Note this is done using the wide data set. Note, it is important to remember that the substantive interpretation of the level-2 regression parameters is affected by the scaling and centering of the time-invariant covariates. As in standard regression models, when time-invariant covariates are centered at their sample level means, the coefficients indicate the expected intercept and slope for an average person rather than for a hypothetical person with scores of zero on all the predictors. describe(wiscsub$momed) ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 204 10.81 2.7 11.5 11 2.97 5.5 18 12.5 -0.36 0.01 0.19 #Calculating the mean momed_mean &lt;- mean(wiscsub$momed) momed_mean ## [1] 10.81127 #Calculating the sd for later use in plots momed_sd &lt;- sd(wiscsub$momed) momed_sd ## [1] 2.698279 #Computing centered variable in long data verblong$momed_c &lt;- (verblong$momed-momed_mean) describe(verblong$momed_c) ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 816 0 2.69 0.69 0.19 2.97 -5.31 7.19 12.5 -0.36 0.03 0.09 Fitting conditional growth model with momed (centered) as predictor cgm2_fit &lt;- lme( fixed= verb ~ 1 + grade + momed_c + grade:momed_c, random= ~ 1 + grade|id, data=verblong, na.action = na.exclude ) summary(cgm2_fit) ## Linear mixed-effects model fit by REML ## Data: verblong ## AIC BIC logLik ## 5000.914 5038.51 -2492.457 ## ## Random effects: ## Formula: ~1 + grade | id ## Structure: General positive-definite, Log-Cholesky parametrization ## StdDev Corr ## (Intercept) 3.394993 (Intr) ## grade 1.158315 0.163 ## Residual 3.581594 ## ## Fixed effects: verb ~ 1 + grade + momed_c + grade:momed_c ## Value Std.Error DF t-value p-value ## (Intercept) 15.150993 0.3424175 610 44.24713 0 ## grade 4.673390 0.1041157 610 44.88652 0 ## momed_c 0.734066 0.1272144 202 5.77031 0 ## grade:momed_c 0.169838 0.0386809 610 4.39075 0 ## Correlation: ## (Intr) grade momd_c ## grade -0.301 ## momed_c 0.000 0.000 ## grade:momed_c 0.000 0.000 -0.301 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -2.56783083 -0.54388373 -0.01862407 0.51810887 3.19394049 ## ## Number of Observations: 816 ## Number of Groups: 204 13.7.3.1 Marginal Effects We can extract the fixed effects for a prototypical trajectory. #Extract the fixed effects FE2 &lt;- fixef(cgm2_fit) FE2 ## (Intercept) grade momed_c grade:momed_c ## 15.1509929 4.6733898 0.7340657 0.1698381 Now, let’s consider momed at the mean value, as well as low (-1SD) and high (+1SD) values. Low value of momed. fun_cgm_momed_low &lt;- function(x) { momed=0-1*momed_sd FE2[1] + FE2[2]*x + FE2[3]*momed + FE2[4]*x*momed } Average value of momed. fun_cgm_momed_ave &lt;- function(x) { momed=0 FE2[1] + FE2[2]*x + FE2[3]*momed + FE2[4]*x*momed } High value of momed. fun_cgm_momed_high &lt;- function(x) { momed=0+1*momed_sd FE2[1] + FE2[2]*x + FE2[3]*momed + FE2[4]*x*momed } Plot with the prototypical -1SD (red), prototypical 0SD (magenta) and prototypical +1SD (blue) trajectories. #plotting intraindividual change with overlay ggplot(data = verblong, aes(x = grade, y = verb, group = id)) + ggtitle(&quot;Raw Trajectories + Levels of Predictor&quot;) + # geom_point() + geom_line() + xlab(&quot;Grade&quot;) + ylab(&quot;WISC Verbal Score&quot;) + ylim(0,100) + scale_x_continuous(breaks=seq(1,6,by=1)) + stat_function(fun=fun_cgm_momed_low, color=&quot;red&quot;, size = 2) + stat_function(fun=fun_cgm_momed_ave, color=&quot;purple&quot;, size = 2) + stat_function(fun=fun_cgm_momed_high, color=&quot;blue&quot;, size = 2) + theme_classic() ## Warning: Multiple drawing groups in `geom_function()` ## ℹ Did you use the correct group, colour, or fill aesthetics? ## Multiple drawing groups in `geom_function()` ## ℹ Did you use the correct group, colour, or fill aesthetics? ## Multiple drawing groups in `geom_function()` ## ℹ Did you use the correct group, colour, or fill aesthetics? Again, we can obtain a similar plot using packages designed to plot marginal effects. mydf &lt;- ggeffects::ggpredict(cgm2_fit, terms = c(&quot;grade&quot;,&quot;momed_c [meansd]&quot;)) plot(mydf) "],["13.8-alternative-time-metrics.html", "13.8 Alternative Time Metrics", " 13.8 Alternative Time Metrics Let’s go back to the simple linear growth model, this time being very explicit about the scaling and centering of the time variable. Time metric = original scores, Grade = 1, 2, 4, 6. Fitting the linear model with grade (as originally coded) linear_grade_fit &lt;- lme( fixed= verb ~ 1 + grade, random= ~ 1 + grade|id, data=verblong, na.action = na.exclude ) summary(linear_grade_fit) ## Linear mixed-effects model fit by REML ## Data: verblong ## AIC BIC logLik ## 5053.632 5081.844 -2520.816 ## ## Random effects: ## Formula: ~1 + grade | id ## Structure: General positive-definite, Log-Cholesky parametrization ## StdDev Corr ## (Intercept) 3.915535 (Intr) ## grade 1.241299 0.321 ## Residual 3.581589 ## ## Fixed effects: verb ~ 1 + grade ## Value Std.Error DF t-value p-value ## (Intercept) 15.15099 0.3686512 611 41.09845 0 ## grade 4.67339 0.1087023 611 42.99255 0 ## Correlation: ## (Intr) ## grade -0.155 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -2.61615651 -0.54270692 -0.02898121 0.52402086 3.16468350 ## ## Number of Observations: 816 ## Number of Groups: 204 Important Questions to Ask Where should the intercept be located? Is there a meaningful zero-time? (choosing c1) time since birth, time to death, time since surgery, time since meeting the research staff What should the time units be? (choosing c2) years, months, minutes, grades, # of psychological events? Using these quantities (c1 and c2) and the formula below it is possible to create alternative time metrics that may be more appropriate for the specific change process we are interested in: \\[timenew = (time - c1)/c2\\] where - c1 is a centering constant, and - c2 is a scaling constant. 13.8.1 Recentering time metrics time_cG1 = Grade centered 0-point = grade 1: time_cG1 = 0, 1, 3, 5 time_cG6 = Grade centered 0-point = grade 6: time_cG6 = -5, -4, -2, 0 verblong$time_cG1 = (verblong$grade - 1)/1 verblong$time_cG6 = (verblong$grade - 6)/1 13.8.2 Rescaling time metric time_cG6 = Grade centered 0-point = grade 6: time_cG6rescale = -1.0, -0.8, -0.4, 0.0 verblong$time_cG6rescale = (verblong$grade - 6)/5 13.8.3 Remapping Time We may also want to remap time based on some other metric. For example, if we wanted to look at the number of assessment as indexing time. Here assessment would be the number of assessments youth have been exposed to: (assessment = 1, 2, 3, 4). Note: the way the mapping is done here only works in this case with no missing data. If have missing data, need to remap in a different way. #remapping using plyr verblong$assessment = mapvalues(verblong$grade,from= c(1,2,4,6),to= c(1,2,3,4)) 13.8.4 Compare Growth Metrics Let’s look at the models with different time-metrics. 13.8.4.1 Time metric = grade linear_grade_fit &lt;- lme( fixed= verb ~ 1 + grade, random= ~ 1 + grade|id, data=verblong, na.action = na.exclude ) summary(linear_grade_fit) ## Linear mixed-effects model fit by REML ## Data: verblong ## AIC BIC logLik ## 5053.632 5081.844 -2520.816 ## ## Random effects: ## Formula: ~1 + grade | id ## Structure: General positive-definite, Log-Cholesky parametrization ## StdDev Corr ## (Intercept) 3.915535 (Intr) ## grade 1.241299 0.321 ## Residual 3.581589 ## ## Fixed effects: verb ~ 1 + grade ## Value Std.Error DF t-value p-value ## (Intercept) 15.15099 0.3686512 611 41.09845 0 ## grade 4.67339 0.1087023 611 42.99255 0 ## Correlation: ## (Intr) ## grade -0.155 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -2.61615651 -0.54270692 -0.02898121 0.52402086 3.16468350 ## ## Number of Observations: 816 ## Number of Groups: 204 13.8.4.2 Time metric = grade_cG1 time_cG1 = Grade centered 0-point = grade 1: time_cG1 = 0, 1, 3, 5 linear_time_cG1_fit &lt;- lme( fixed= verb ~ 1 + time_cG1, random= ~ 1 + time_cG1|id, data=verblong, na.action = na.exclude ) summary(linear_time_cG1_fit) ## Linear mixed-effects model fit by REML ## Data: verblong ## AIC BIC logLik ## 5053.632 5081.844 -2520.816 ## ## Random effects: ## Formula: ~1 + time_cG1 | id ## Structure: General positive-definite, Log-Cholesky parametrization ## StdDev Corr ## (Intercept) 4.470800 (Intr) ## time_cG1 1.241299 0.558 ## Residual 3.581590 ## ## Fixed effects: verb ~ 1 + time_cG1 ## Value Std.Error DF t-value p-value ## (Intercept) 19.82438 0.3678085 611 53.89865 0 ## time_cG1 4.67339 0.1087023 611 42.99256 0 ## Correlation: ## (Intr) ## time_cG1 0.14 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -2.61615611 -0.54270689 -0.02898113 0.52402090 3.16468325 ## ## Number of Observations: 816 ## Number of Groups: 204 13.8.4.3 Time metric = grade_cG6 time_cG6 = Grade centered 0-point = grade 6: time_cG6 = -5, -4, -2, 0 linear_time_cG6_fit &lt;- lme( fixed= verb ~ 1 + time_cG6, random= ~ 1 + time_cG6|id, data=verblong, na.action = na.exclude ) summary(linear_time_cG6_fit) ## Linear mixed-effects model fit by REML ## Data: verblong ## AIC BIC logLik ## 5053.632 5081.844 -2520.816 ## ## Random effects: ## Formula: ~1 + time_cG6 | id ## Structure: General positive-definite, Log-Cholesky parametrization ## StdDev Corr ## (Intercept) 9.460226 (Intr) ## time_cG6 1.241299 0.92 ## Residual 3.581590 ## ## Fixed effects: verb ~ 1 + time_cG6 ## Value Std.Error DF t-value p-value ## (Intercept) 43.19133 0.6976142 611 61.91292 0 ## time_cG6 4.67339 0.1087023 611 42.99256 0 ## Correlation: ## (Intr) ## time_cG6 0.853 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -2.61615613 -0.54270686 -0.02898114 0.52402089 3.16468319 ## ## Number of Observations: 816 ## Number of Groups: 204 13.8.4.4 Time metric = grade_cG6rescale time_cG6 = Grade centered 0-point = grade 6: time_cG6rescale = -1.0, -0.8, -0.4, 0.0 linear_time_cG6rescale_fit &lt;- lme( fixed= verb ~ 1 + time_cG6rescale, random= ~ 1 + time_cG6rescale|id, data=verblong, na.action = na.exclude ) summary(linear_time_cG6rescale_fit) ## Linear mixed-effects model fit by REML ## Data: verblong ## AIC BIC logLik ## 5050.413 5078.625 -2519.207 ## ## Random effects: ## Formula: ~1 + time_cG6rescale | id ## Structure: General positive-definite, Log-Cholesky parametrization ## StdDev Corr ## (Intercept) 9.460223 (Intr) ## time_cG6rescale 6.206483 0.92 ## Residual 3.581591 ## ## Fixed effects: verb ~ 1 + time_cG6rescale ## Value Std.Error DF t-value p-value ## (Intercept) 43.19133 0.6976141 611 61.91293 0 ## time_cG6rescale 23.36695 0.5435110 611 42.99260 0 ## Correlation: ## (Intr) ## time_cG6rescale 0.853 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -2.61615435 -0.54270813 -0.02898075 0.52402103 3.16468675 ## ## Number of Observations: 816 ## Number of Groups: 204 13.8.4.5 Time metric = assessment Assessment = 1, 2, 3, 4 linear_assessment_fit &lt;- lme( fixed= verb ~ 1 + assessment, random= ~ 1 + assessment|id, data=verblong, na.action = na.exclude ) summary(linear_assessment_fit) ## Linear mixed-effects model fit by REML ## Data: verblong ## AIC BIC logLik ## 5134.792 5163.004 -2561.396 ## ## Random effects: ## Formula: ~1 + assessment | id ## Structure: General positive-definite, Log-Cholesky parametrization ## StdDev Corr ## (Intercept) 2.960137 (Intr) ## assessment 1.892481 0.35 ## Residual 3.997613 ## ## Fixed effects: verb ~ 1 + assessment ## Value Std.Error DF t-value p-value ## (Intercept) 10.417770 0.4005742 611 26.00709 0 ## assessment 7.968696 0.1822741 611 43.71820 0 ## Correlation: ## (Intr) ## assessment -0.405 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -2.41214177 -0.56890698 -0.05446236 0.56436062 3.60513496 ## ## Number of Observations: 816 ## Number of Groups: 204 "],["13.9-intperpreting-interactions.html", "13.9 Intperpreting Interactions", " 13.9 Intperpreting Interactions Below is a quick note on visualizing slope intercept interactions in linear growth curve models. Interpreting interactions. "],["14-chapter-14.html", "Chapter 14 Nonlinear Growth Curves", " Chapter 14 Nonlinear Growth Curves This chapter introduces the concept of nonlinear change. In this chapter we will cover the following topics: Review of Linear Growth Theory of Linear Growth Characteristics of Linear Growth No Growth Model Equations Code (NLME/LME) Random Intercept Model Equations Code (NLME/LME) Linear Growth Model Equations Code (NLME/LME) Quadratic Growth Model Equations Code (NLME/LME) Introducing Nonlinear Growth Types of Nonlinear Models Type I: Nonlinear in Time Type II: Nonlinear in Parameters Type III: Nonlinear in Random Coefficients Flexibility of Nonlinear Growth Utility of Nonlinear Growth Some Nonlinear Growth Models Need for Nonlinear Models Example Data (Ram and Grimm, 2007) Read in Cortisol Data Reshaping Data Descriptives Density Plots Individual-level Trajectories Linear Growth (Cortisol) Equation Fit Model _ Plots Interpretation Quadratic Growth (Cortisol) Equation Fit Model _ Plots Interpretation Latent Basis (Cortisol) Equation Fit Model _ Plots Interpretation Exponential Growth (Cortisol) Equation Fit Model _ Plots Interpretation Multiphase Growth (Cortisol) Equation Fit Model _ Plots Interpretation "],["14.1-review-of-linear-growth.html", "14.1 Review of Linear Growth", " 14.1 Review of Linear Growth Examples of Linear Growth 14.1.1 Theory of Linear Growth Before diving into nonlinear model let’s briefly review linear growth. Theory of Intraindividual Change Individuals’ behavior changes over time at a pre-determined, unchanging (stable) rate. For a linear growth model to hold we need an explanation regarding what is driving the pre-determined rate of change A one-time event with a forever lasting effect?, or A continuous event with a stable effect? Theory of Interindividual Differences We suppose individuals can differ in their “initial” level of behavior Need an explanation of why individuals have different “initial” levels We suppose individuals differ in their rate of change Need an explanation of why individuals have different rates of change 14.1.2 Characteristics of Linear Growth Benefits of Linear Growth Models Simple developmental pattern Easily interpretable parameters Level: predicted score when \\(t = 0\\) Slope: rate of change in \\(y\\) for a \\(1-unit\\) change in \\(t\\) Limitations of Linear Growth Models Often does not truly match the developmental process theories Difficult to generalize outside of observation period 14.1.3 No Growth Model First, let’s consider the no growth model. Remember, the no growth model suggests individuals differ only in terms of their overall level of a given construct, and this level does not change across time. It is often useful as an initial model to determine whether future analyses are warranted. 14.1.3.1 No Growth Plot No Growth Model 14.1.3.2 No Growth Equations \\[\\begin{align} y_{ti} = &amp; \\beta_{0i} + e_{ti}, &amp; e_{ti} \\sim \\mathcal{N}(0,\\sigma^{2}_{e}) &amp;&amp; \\: [\\text{Level 1 Equation}] \\\\ \\beta_{0i} = &amp; \\gamma_{00} + u_{0i}, &amp; u_{0i} \\sim \\mathcal{N}(0,\\sigma^{2}_{u0}) &amp;&amp; \\: [\\text{Level 2 Equation}] \\\\ y_{ti} = &amp; \\underbrace{\\gamma_{00}}_{fixed} + \\underbrace{u_{0i}}_{random} + e_{ti}, &amp; &amp;&amp; \\: [\\text{Combined Equation}] \\end{align}\\] where \\(y_{ti}\\) is the repeated measures score for individual \\(i\\) at time \\(t\\) \\(\\beta_{0i}\\) is the random intercept for individual \\(i\\) (person-specific mean) \\(e_{ti}\\) is the time-specific residual score (within-person deviation) \\(\\gamma_{00}\\) is the sample mean for the intercept (grand mean) \\(u_{0i}\\) is individual \\(i\\)’s deviation from the sample mean (between person deviation) 14.1.3.3 No Growth Code Here, we present code for fitting the no growth model in the nlme package using both compact and detailed model syntax. um_nlme &lt;- nlme::nlme( verb ~ gamma_00 + u_0i, data = verblong, fixed = gamma_00~1, random = u_0i~1, group = ~id, na.action=&quot;na.omit&quot;, start = c(gamma_00 = mean(verblong$verb)) ) summary(um_nlme) um_lme &lt;- nlme::lme( fixed= verb ~ 1, random = ~ 1|id, data = verblong, na.action = na.exclude ) summary(um_lme) 14.1.4 Random Intercept Model 14.1.4.1 Random Intercept Plot Random Intercept Model 14.1.4.2 Random Intercept Equations \\[\\begin{align} y_{ti} = &amp; \\beta_{0i} + \\beta_{1i}\\frac{\\text{time}_{}-c_{1}}{c_{2}} + e_{ti}, &amp; e_{ti} \\sim \\mathcal{N}(0,\\sigma^{2}_{e}) &amp;&amp; \\: [\\text{Level 1}] \\\\ \\beta_{0i} = &amp; \\gamma_{00} + u_{0i}, &amp; u_{0i} \\sim \\mathcal{N}(0,\\sigma^{2}_{u0}) &amp;&amp; \\: [\\text{Level 2}] \\\\ \\beta_{1i} = &amp; \\gamma_{10}, &amp; &amp;&amp; \\: \\\\ y_{ti} = &amp; \\underbrace{\\gamma_{00} + \\gamma_{10}\\frac{\\text{time}-c_{1}}{c_{2}}}_{fixed} + \\underbrace{u_{0i}}_{random} + e_{ti}, &amp; &amp;&amp; \\: [\\text{Combined}] \\end{align}\\] where \\[\\begin{align} u_{0i} \\sim \\left( \\begin{array}{r} 0 \\end{array}, \\begin{array}{c} \\sigma^{2}_{u0} \\end{array}\\right), \\end{align}\\] and \\(y_{ti}\\) is the repeated measures score for individual \\(i\\) at time \\(t\\) \\(\\beta_{0i}\\) is the random intercept for individual \\(i\\) predicted score for individual \\(i\\) when \\(\\text{time}=0\\) \\(\\beta_{1i}\\) is the sample-level mean for the slope (\\(\\beta_{1i}=\\gamma_{10}\\)) predicted rate of change for individual \\(i\\) with a 1-unit change in \\(\\text{time}\\) \\(\\text{time}\\) represents time and could be grade, age, year, etc. predicted rate of change for individual \\(i\\) with a 1-unit change in \\(\\text{time}\\) \\(c_{1}\\) constant used to center the intercept \\(c_1\\) is often set to \\(1\\) to center the intercept at the first occasion \\(c_{2}\\) constant chosen to scale the slope \\(c_2\\) is often set to \\(1\\) to scale the slope in terms of the units of \\(\\text{time}\\) \\(e_{it}\\) is the time-specific residual score (within-person deviation) \\(\\gamma_{00}\\) is the sample-level mean for the intercept \\(\\gamma_{10}\\) is the sample-level mean for the slope \\(u_{0i}\\) is individual \\(i\\)’s deviation from the sample-level mean of the intercept 14.1.4.3 Random Intercept Code Here, we present code for fitting the random intercept model in the nlme package using both compact and detailed model syntax. ri_nlme &lt;- nlme::nlme( verb ~ (gamma_00 + u_0i) + (gamma_10)*grade, data = verblong, fixed = gamma_00 + gamma_10~1, random = u_0i~1, group = ~id, na.action=&quot;na.omit&quot;, start = c(gamma_00 = 30, gamma_10=10) ) summary(ri_nlme) ri_lme &lt;- nlme::lme( fixed = verb ~ 1 + grade, random = ~ 1|id, data=verblong, na.action = na.exclude, method = &quot;ML&quot; ) summary(ri_lme) 14.1.5 Linear Growth Model Now let’s consider the linear growth model 14.1.5.1 Linear Growth Plot Linear Growth Model 14.1.5.2 Linear Growth Equations \\[\\begin{align} y_{ti} = &amp; \\beta_{0i} + \\beta_{1i}\\text{time} + e_{ti}, &amp; e_{ti} \\sim \\mathcal{N}(0,\\sigma^{2}_{e}) &amp;&amp; \\: [\\text{Level 1}] \\\\ \\beta_{0i} = &amp; \\gamma_{00} + u_{0i}, &amp; u_{0i} \\sim \\mathcal{N}(0,\\sigma^{2}_{u0}) &amp;&amp; \\: [\\text{Level 2}] \\\\ \\beta_{1i} = &amp; \\gamma_{10} + u_{1i}, &amp; u_{1i} \\sim \\mathcal{N}(0,\\sigma^{2}_{u1}) &amp;&amp; \\: \\\\ y_{ti} = &amp; \\underbrace{\\gamma_{00} + \\gamma_{10}\\text{time}}_{fixed} + \\underbrace{u_{0i} + u_{1i}\\text{time}}_{random} + e_{ti}, &amp; &amp;&amp; \\: [\\text{Combined}] \\end{align}\\] where \\[\\begin{align} u_{0i}, u_{1i} \\sim \\left( \\left[\\begin{array}{r} 0 \\\\ 0 \\end{array}\\right], \\left[\\begin{array}{c} \\sigma^{2}_{u0} &amp; \\\\ \\sigma^{2}_{u1u0} &amp; \\sigma^{2}_{u1} \\end{array}\\right]\\right), \\end{align}\\] and \\(y_{ti}\\) is the repeated measures score for individual \\(i\\) at time \\(t\\) \\(\\beta_{0i}\\) is the random intercept for individual \\(i\\) predicted score for individual \\(i\\) when \\(\\text{time}=0\\) \\(\\beta_{1i}\\) is the random slope for individual \\(i\\) predicted rate of change for individual \\(i\\) with a 1-unit change in \\(\\text{time}\\) \\(\\text{time}\\) represents time and could be grade, age, year, etc. predicted rate of change for individual \\(i\\) with a 1-unit change in \\(\\text{time}\\) \\(c_{1}, c_{2}\\) have been dropped (set to \\(1\\)) to simplify notation \\(e_{it}\\) is the time-specific residual score (within-person deviation) \\(\\gamma_{00}\\) is the sample-level mean for the intercept \\(\\gamma_{10}\\) is the sample-level mean for the slope \\(u_{0i}\\) is individual \\(i\\)’s deviation from the sample-level mean of the intercept \\(u_{1i}\\) is individual \\(i\\)’s deviation from the sample-level mean of the slope 14.1.5.3 Linear Growth Code Here, we present code for fitting the linear growth model in the nlme package using both compact and detailed model syntax. lin_nlme &lt;- nlme::nlme( verb ~ (gamma_00 + u_0i) + (gamma_10 + u_1i)*grade, data = verblong, fixed = gamma_00 + gamma_10~1, random = u_0i + u_1i~1, group = ~id, na.action=&quot;na.omit&quot;, start = c(gamma_00 = 30, gamma_10=10) ) summary(lin_nlme) lin_lme &lt;- nlme::lme( fixed = verb ~ 1 + grade, random = ~ 1 + grade|id, data=verblong, na.action = na.exclude, method = &quot;ML&quot; ) summary(lin_lme) 14.1.6 Quadratic Growth Model Now, let’s review the quadratic growth model. The quadratic growth model accounts for nonlinearity by adding a second-order power of time to the linear growth model. 14.1.6.1 Quadratic Growth Plot Quadratic Growth Model 14.1.7 Quadratic Growth Equations The quadratic growth model can be written as \\[\\begin{align} y_{ti} = &amp; \\beta_{0i} + \\beta_{1i}\\text{time}+ \\beta_{2i}\\text{time}^{2} + e_{ti}, &amp; e_{ti} \\sim \\mathcal{N}(0,\\sigma^{2}_{e}) &amp;&amp; \\: [\\text{Level 1}] \\\\ \\beta_{0i} = &amp; \\gamma_{00} + u_{0i}, &amp; u_{0i} \\sim \\mathcal{N}(0,\\sigma^{2}_{u0}) &amp;&amp; \\: [\\text{Level 2}] \\\\ \\beta_{1i} = &amp; \\gamma_{10} + u_{1i}, &amp; u_{1i} \\sim \\mathcal{N}(0,\\sigma^{2}_{u1}) &amp;&amp; \\: \\\\ \\beta_{2i} = &amp; \\gamma_{20} + u_{2i}, &amp; u_{2i} \\sim \\mathcal{N}(0,\\sigma^{2}_{u2}) &amp;&amp; \\: \\\\ y_{ti} = &amp; \\underbrace{\\gamma_{00} + \\gamma_{10}\\text{time} + \\gamma_{20}\\text{time}^2}_{fixed} + \\underbrace{u_{0i} + u_{1i}\\text{time} + u_{2i}\\text{time}^{2}}_{random} + e_{ti}, &amp; &amp;&amp; \\: [\\text{Combined}] \\end{align}\\] where \\[\\begin{align} u_{0i}, u_{1i} \\sim \\left( \\left[\\begin{array}{r} 0 \\\\ 0 \\\\ 0 \\end{array}\\right], \\left[\\begin{array}{c} \\sigma^{2}_{u0} &amp; \\\\ \\sigma^{2}_{u1u0} &amp; \\sigma^{2}_{u1} \\\\ \\sigma^{2}_{u2u0} &amp; \\sigma^{2}_{u2u1} &amp; \\sigma^{2}_{u2}\\\\ \\end{array}\\right]\\right), \\end{align}\\] and \\(y_{ti}\\) is the repeated measures score for individual \\(i\\) at time \\(t\\) \\(\\beta_{0i}\\) is the random intercept for individual \\(i\\) predicted score for individual \\(i\\) when \\(\\text{time}=0\\) \\(\\beta_{1i}\\) is the random linear component for individual \\(i\\) the linear component of change or the rate of change when \\(\\text{time}=0\\) \\(\\beta_{2i}\\) is the random quadratic component for individual \\(i\\) the quadratic component of change or acceleration (how quickly the rate of change is changing) \\(\\text{time}\\) represents time and could be grade, age, year, etc. predicted rate of change for individual \\(i\\) with a 1-unit change in \\(\\text{time}\\) \\(c_{1}, c_{2}\\) have been dropped (set to \\(1\\)) to simplify notation \\(e_{it}\\) is the time-specific residual score (within-person deviation) \\(\\gamma_{00}\\) is the fixed effect for the intercept component \\(\\gamma_{10}\\) is the fixed effect for the linear component \\(\\gamma_{20}\\) is the fixed effect for the quadratic component \\(u_{0i}\\) is individual \\(i\\)’s deviation from the intercept component \\(u_{1i}\\) is individual \\(i\\)’s deviation from the linear component \\(u_{2i}\\) is individual \\(i\\)’s deviation from the quadratic component 14.1.7.1 Quadratic Growth Code Here, we present code for fitting the quadratic growth model in the nlme package using both compact and detailed model syntax. verblong$gradeSquared &lt;- (verblong$grade)^2 quad_nlme &lt;- nlme::nlme( verb ~ (gamma_00 + u_0i) + (gamma_10 + u_1i)*grade + (gamma_20 + u_2i)*gradeSquared, data = verblong, fixed = gamma_00 + gamma_10 + gamma_20~1, random = u_0i + u_1i + u_2i~1, group = ~id, na.action=&quot;na.omit&quot;, start = c(gamma_00 = 20, gamma_10=10, gamma_20=1) ) summary(quad_nlme) quad_lme &lt;- nlme::lme( fixed = verb ~ 1 + grade + gradeSquared, random = ~ 1 + grade|id + gradeSquared|id, data=verblong, na.action = na.exclude, method = &quot;REML&quot; # does not converge with ML ) summary(quad_lme) "],["14.2-introducing-nonlinear-growth.html", "14.2 Introducing Nonlinear Growth", " 14.2 Introducing Nonlinear Growth 14.2.1 Types of Nonlinearity Here, we consider three different types of nonlinearity. These types represent increasing in difficulty so you have to choose a software accordingly. Type I: Non-linear with respect to time examples include \\(t^2\\), \\(log(t)\\) included as predictors the quadratic growth model is a Type I model none of the partial derivatives are dependent on an unknown parameter or random coefficient Type II: Non-linear with respect to parameters (non-random coefficients) random coefficients are additive the exponential growth model is a Type II model one or more partial derivatives is a function of an estimated parameter Type III: Non-linear with respect to random coefficients or latent variables random coefficients in non-linear functions often more difficult to fit in SEM software one or more partial derivatives is a function of a random coefficient 14.2.2 Flexibility of Nonlinear Growth Our goals in growth modeling is often to Understand, estimate, and capture the defining characteristics of “growth” (i.e., change) for a process of interest. This endeavor is complicated by the multiple potential characteristics of growth, including: initial levels rates of change periods of acceleration and deceleration when the process enters and leaves different developmental phases the final or asymptotic levels 14.2.3 Utility of Nonlinear Growth Many applications consider and model linear change patterns because of their simplicity and interpretability. However, many developmental processes are more complex, several core theoretical notions of development do not posit simple linear change (e.g., Bronfenbrenner, 1979), and many empirical data are not best characterized by linear change patterns. Researchers must consider models capable of representing nonlinear developmental patterns and be specific about where between-person differences appear in those patterns. Examples of Nonlinear Trajectories 14.2.4 Some Nonlinear Growth Models Acknowledging the nonlinearity present in many growth processes, consider models that allow for nonlinear change patterns (trajectories), including: Polynomial (e.g., quadratic, cubic) Latent basis model Specific nonlinear functions (e.g., exponential, logistic) with and without variation in nonlinear terms Multiphase or Spline Models Model-Implied Nonlinear Trajectories 14.2.5 Need for Nonlinear Models Most theoretical or verbals models of change specify relations that do not fall along a straight line exponential decay/growth, logistic growth Developmental theory often dictates that a relation shifts and/or stops at some point changes in context, goals, constraints Empirical inquiry (scientific method) suggests useful to reject some forms of nonlinearity often a tension between theoretical and empirical fit https://xkcd.com/2048 "],["14.3-example-data-ram-grimm-2007.html", "14.3 Example Data (Ram &amp; Grimm, 2007)", " 14.3 Example Data (Ram &amp; Grimm, 2007) This chapter’s exercises are based on a script from Drs. Nilam Ram and Kevin Grimm, which was made for the purpose of illustrating the analyses that were done in their paper (Ram and Grimm 2007). This script includes the cortisol data for \\(34\\) subjects across \\(9\\) timepoints (\\(N = 34, T = 9\\)) collected over the course of a few hours. The experimental design consisted of three phases used to inestigate the time-course of cortisol production and dispersion in response to a controlled intervention: Baseline: \\(t=0,1\\) Intervention (driving simulator): \\(t=2,3,4\\) Post-Intervention Follow-up: \\(t=5,6,7,8\\) This data set has been used to illustrate a variety of growth modeling and mixture modeling methods (Grimm et al. 2013; Ram and Grimm 2009). We describe the data and then walk through R-based implementations of the models covered in Ram, N., &amp; Grimm, K. (2007). Using simple and complex growth models to articulate developmental change: Matching theory to method. International Journal of Behavioral Development, 31, 303-316. The data are shared with intent that others may find them useful for learning about growth modeling or developing new methods for analysis of change. New publications based on these data require citation and acknowledgement of the full set of papers that have used the data, including the above papers. Thanks are also given to John Nesselroade, Teresa Seeman, and Marilyn Albert for data collection and inspiration. 14.3.1 Read in Cortisol Data library(psych) library(ggplot2) library(nlme) library(lme4) filepath &lt;- &quot;https://quantdev.ssri.psu.edu/sites/qdev/files/TheCortisolData.csv&quot; cortisol_wide &lt;- read.csv(file=url(filepath),header=TRUE) head(cortisol_wide,6) ## id cort_0 cort_1 cort_2 cort_3 cort_4 cort_5 cort_6 cort_7 cort_8 ## 1 1 4.2 4.1 9.7 14.0 19.0 18.0 20.0 23.0 24.0 ## 2 2 5.5 5.6 14.0 16.0 19.0 17.0 18.0 20.0 19.0 ## 3 3 4.0 3.8 7.5 12.0 14.0 13.0 9.1 8.2 7.9 ## 4 4 6.1 5.6 14.0 20.0 26.0 23.0 26.0 25.0 26.0 ## 5 5 4.6 4.4 7.2 12.3 15.8 16.1 17.0 17.8 19.1 ## 6 6 6.8 9.5 14.2 19.6 19.0 13.9 13.4 12.5 11.7 14.3.2 Reshaping Data We already have the wide format data. We make a set of long format data. cortisol_long &lt;- reshape( data = cortisol_wide, timevar = c(&quot;time&quot;), idvar = &quot;id&quot;, varying = c(&quot;cort_0&quot;,&quot;cort_1&quot;,&quot;cort_2&quot;, &quot;cort_3&quot;,&quot;cort_4&quot;,&quot;cort_5&quot;, &quot;cort_6&quot;,&quot;cort_7&quot;,&quot;cort_8&quot;), direction=&quot;long&quot;, sep=&quot;_&quot; ) # sorting for easy viewing order by id and time cortisol_long &lt;- cortisol_long[order(cortisol_long$id,cortisol_long$time), ] To match the scaling of time used in some of the papers we add an additional time variable that runs from 0 to 1. cortisol_long$timescaled &lt;- (cortisol_long$time - 0)/8 Looking at the top few rows of the long data. head(cortisol_long,18) ## id time cort timescaled ## 1.0 1 0 4.2 0.000 ## 1.1 1 1 4.1 0.125 ## 1.2 1 2 9.7 0.250 ## 1.3 1 3 14.0 0.375 ## 1.4 1 4 19.0 0.500 ## 1.5 1 5 18.0 0.625 ## 1.6 1 6 20.0 0.750 ## 1.7 1 7 23.0 0.875 ## 1.8 1 8 24.0 1.000 ## 2.0 2 0 5.5 0.000 ## 2.1 2 1 5.6 0.125 ## 2.2 2 2 14.0 0.250 ## 2.3 2 3 16.0 0.375 ## 2.4 2 4 19.0 0.500 ## 2.5 2 5 17.0 0.625 ## 2.6 2 6 18.0 0.750 ## 2.7 2 7 20.0 0.875 ## 2.8 2 8 19.0 1.000 14.3.3 Descriptives Basic descriptives of the 9-occasion data. #means, sd, etc. describe(cortisol_wide) ## vars n mean sd median trimmed mad min max range skew kurtosis ## id 1 34 17.50 9.96 17.50 17.50 12.60 1 34.0 33.0 0.00 -1.31 ## cort_0 2 34 5.25 2.45 5.00 4.90 2.30 2 12.0 10.0 1.18 0.92 ## cort_1 3 34 5.06 2.59 4.50 4.78 2.22 2 11.3 9.3 1.02 -0.01 ## cort_2 4 34 10.85 3.11 10.05 10.79 3.85 6 16.7 10.7 0.09 -1.17 ## cort_3 5 34 17.36 3.69 17.35 17.39 3.48 7 24.9 17.9 -0.29 0.39 ## cort_4 6 34 19.48 3.72 19.10 19.49 2.82 12 27.1 15.1 -0.04 -0.56 ## cort_5 7 34 16.46 4.18 16.95 16.41 4.52 8 26.0 18.0 0.08 -0.58 ## cort_6 8 34 15.03 4.57 14.65 14.90 4.15 6 26.0 20.0 0.26 -0.31 ## cort_7 9 34 15.62 4.81 16.15 15.60 5.56 7 25.0 18.0 -0.02 -0.99 ## cort_8 10 34 15.86 5.61 16.00 15.81 6.08 6 26.0 20.0 0.14 -1.08 ## se ## id 1.71 ## cort_0 0.42 ## cort_1 0.44 ## cort_2 0.53 ## cort_3 0.63 ## cort_4 0.64 ## cort_5 0.72 ## cort_6 0.78 ## cort_7 0.82 ## cort_8 0.96 #boxplot by day ggplot(data=cortisol_long, aes(x=factor(time), y=cort)) + geom_boxplot(notch = TRUE) + stat_summary(fun.y=&quot;mean&quot;, geom=&quot;point&quot;, shape=23, size=3, fill=&quot;white&quot;) + labs(x = &quot;Time&quot;, y = &quot;Cortisol&quot;) + theme_classic() #pairs plot from the psych library psych::pairs.panels( cortisol_wide[,c( &quot;cort_0&quot;,&quot;cort_1&quot;,&quot;cort_2&quot;, &quot;cort_3&quot;,&quot;cort_4&quot;,&quot;cort_5&quot;, &quot;cort_6&quot;,&quot;cort_7&quot;,&quot;cort_8&quot; ),]) 14.3.4 Density Plots #Density distribution by day ggplot(data=cortisol_long, aes(x=cort)) + geom_density(aes(group=factor(time), colour=factor(time), fill=factor(time)), alpha=0.3) + theme_classic() 14.3.5 Individual-level Trajectories #intraindividual change trajetories ggplot(data = cortisol_long, aes(x = time, y = cort, group = id)) + geom_point(color=&quot;black&quot;) + geom_line(color=&quot;black&quot;) + xlab(&quot;Time&quot;) + ylab(&quot;Cortisol&quot;) + ylim(0,30) + scale_x_continuous(breaks=seq(0,8,by=1)) + theme_classic() References "],["14.4-linear-growth-cortisol.html", "14.4 Linear Growth (Cortisol)", " 14.4 Linear Growth (Cortisol) Now lets fit the series of growth models to the Cortisol data starting with the linear growth model. 14.4.1 Equation We can write the linear growth model equation as \\[\\begin{align} y_{ti} = &amp; \\beta_{0i} + \\beta_{1i}\\text{time}_{1} + e_{ti}, &amp; e_{ti} \\sim \\mathcal{N}(0,\\sigma^{2}_{e}) &amp;&amp; \\: [\\text{Level 1}] \\\\ \\beta_{0i} = &amp; \\gamma_{00} + u_{0i}, &amp; u_{0i} \\sim \\mathcal{N}(0,\\sigma^{2}_{u0}) &amp;&amp; \\: [\\text{Level 2}] \\\\ \\beta_{1i} = &amp; \\gamma_{10} + u_{1i}, &amp; u_{1i} \\sim \\mathcal{N}(0,\\sigma^{2}_{u1}) &amp;&amp; \\: \\\\ y_{ti} = &amp; \\underbrace{\\gamma_{00} + \\gamma_{10}\\text{time}_{1}}_{fixed} + \\underbrace{u_{0i} + u_{1i}\\text{time}_{1}}_{random} + e_{ti}, &amp; &amp;&amp; \\: [\\text{Combined}] \\end{align}\\] where \\[\\begin{align} \\text{time}_{1} = &amp; \\{0/8, 1/8, 2/8, 3/8, 4/8, 5/8, 6/8, 7/8, 8/8\\}\\\\ = &amp; \\{0.000, 0.125, 0.250, 0.375, 0.500, 0.625, 0.750, 0.875, 1.000\\} \\end{align}\\] and \\(y_{ti}\\) is the cortisol level for individual \\(i\\) at time \\(t\\) \\(\\beta_{0i}\\) is the expected baseline level of cortisol for individual \\(i\\) \\(\\beta_{1i}\\) is the expected total change in cortisol for individual \\(i\\) look at coding of time here \\(\\text{time}\\) represents the measurement occasion \\(c_{1}\\) has been set to 0 \\(c_{2}\\) has been set to 8 \\(e_{it}\\) is the time-specific residual score \\(\\gamma_{00}\\) is the sample-level mean for the intercept \\(\\gamma_{10}\\) is the sample-level mean for the slope \\(u_{0i}\\) is individual \\(i\\)’s deviation from the sample-level mean of the intercept \\(u_{1i}\\) is individual \\(i\\)’s deviation from the sample-level mean of the slope 14.4.2 Fit Model # #linear model # cort_linear &lt;- lme(cort ~ 1 + timescaled, # random = ~ 1 + timescaled | id, # data = cortisol_long, # na.action = &quot;na.exclude&quot;) # #convergence issues # # cort_linear2 &lt;- lmer(cort ~ 1 + timescaled + (1 + timescaled | id), # data = cortisol_long, # na.action = &quot;na.exclude&quot;) # #convergence issues # cort_linear &lt;- nlme( # cort ~ (gamma_00 + u_0i) + (gamma_10 + u_1i)*timescaled, # fixed = gamma_00 + gamma_10~1, # random = u_0i + u_1i~1, # group = ~id, # start = c(gamma_00 = 5, gamma_10=1), # data = cortisol_long, # na.action = &quot;na.exclude&quot;, # control = lmeControl(maxIter = 1e8, msMaxIter = 1e8) # ) # # summary(cort_linear) # VarCorr(cort_linear) cort_linear &lt;- nlme( cort ~ (gamma_00) + (gamma_10)*timescaled, fixed = gamma_00 + gamma_10~1, random = gamma_00 + gamma_10~1, group = ~id, start = c(gamma_00 = 5, gamma_10=1), data = cortisol_long, na.action = &quot;na.exclude&quot;, control = lmeControl(maxIter = 1e8, msMaxIter = 1e8) ) summary(cort_linear) ## Nonlinear mixed-effects model fit by maximum likelihood ## Model: cort ~ (gamma_00) + (gamma_10) * timescaled ## Data: cortisol_long ## AIC BIC logLik ## 1837.972 1860.313 -912.9859 ## ## Random effects: ## Formula: list(gamma_00 ~ 1, gamma_10 ~ 1) ## Level: id ## Structure: General positive-definite, Log-Cholesky parametrization ## StdDev Corr ## gamma_00 0.2031226 gmm_00 ## gamma_10 4.4908362 0.999 ## Residual 4.3824268 ## ## Fixed effects: gamma_00 + gamma_10 ~ 1 ## Value Std.Error DF t-value p-value ## gamma_00 8.000588 0.4647813 271 17.21366 0 ## gamma_10 10.881176 1.0970698 271 9.91840 0 ## Correlation: ## gmm_00 ## gamma_10 -0.542 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -1.8427469 -0.7510157 -0.1820384 0.5969159 2.9508066 ## ## Number of Observations: 306 ## Number of Groups: 34 VarCorr(cort_linear) ## id = pdLogChol(list(gamma_00 ~ 1,gamma_10 ~ 1)) ## Variance StdDev Corr ## gamma_00 0.04125879 0.2031226 gmm_00 ## gamma_10 20.16760943 4.4908362 0.999 ## Residual 19.20566500 4.3824268 14.4.3 Predicted Trajectories #obtaining predicted scores for individuals cortisol_long$pred_linear &lt;- predict(cort_linear) #obtaining predicted scores for prototype cortisol_long$proto_linear &lt;- predict(cort_linear, level=0) # plotting predicted trajectories # intraindividual change trajetories ggplot(data = cortisol_long, aes(x = time, y = pred_linear, group = id)) + #geom_point(color=&quot;black&quot;) + geom_line(color=&quot;black&quot;) + geom_line(aes(x = time, y = proto_linear), color=&quot;red&quot;,size=2) + xlab(&quot;Time&quot;) + ylab(&quot;Cortisol&quot;) + ylim(0,30) + scale_x_continuous(breaks=seq(0,8,by=1)) + theme_classic() Note that the model has some convergence issues, and the solution has hit a parameter boundary. In this model, and some of the models that follow, the correlation between the random effects in intercept and slope is questionable. 14.4.4 Interpretation Although we would be reluctant to interpret this model based on the convergence issues. We might say the parameters from the linear model indicate the average person’s baseline level of cortisol is approximately \\(8\\) mcg/dl and their level of cortisol increases incrementally at each occasion, in total increasing by \\(10.9\\) units over the course of the remaining eight occasions. From the plot it appears the linear model fails to capture the intraindividual changes or the interindividual differences in intraindividual change noted in the raw data. It also fails to capture the complexity of the process. "],["14.5-quadratic-growth-cortisol.html", "14.5 Quadratic Growth (Cortisol)", " 14.5 Quadratic Growth (Cortisol) The quadratic growth model is a nice first pass at nonlinear modeling. It is a Type I nonlinear method: a linear model that produces nonlinear trajectories. 14.5.1 Equation The quadratic growth model can be written as \\[\\begin{align} y_{ti} = &amp; \\beta_{0i} + \\beta_{1i}\\text{time}_{1}+ \\beta_{2i}\\text{time}_{2} + e_{ti}, &amp; e_{ti} \\sim \\mathcal{N}(0,\\sigma^{2}_{e}) &amp;&amp; \\: [\\text{Level 1}] \\\\ \\beta_{0i} = &amp; \\gamma_{00} + u_{0i}, &amp; u_{0i} \\sim \\mathcal{N}(0,\\sigma^{2}_{u0}) &amp;&amp; \\: [\\text{Level 2}] \\\\ \\beta_{1i} = &amp; \\gamma_{10} + u_{1i}, &amp; u_{1i} \\sim \\mathcal{N}(0,\\sigma^{2}_{u1}) &amp;&amp; \\: \\\\ \\beta_{2i} = &amp; \\gamma_{20} + u_{2i}, &amp; u_{2i} \\sim \\mathcal{N}(0,\\sigma^{2}_{u2}) &amp;&amp; \\: \\\\ y_{ti} = &amp; \\underbrace{\\gamma_{00} + \\gamma_{10}\\text{time}_{1} + \\gamma_{20}\\text{time}_{2}}_{fixed} + \\underbrace{u_{0i} + u_{1i}\\text{time}_{1} + u_{2i}\\text{time}_{2}}_{random} + e_{ti}, &amp; &amp;&amp; \\: [\\text{Combined}] \\end{align}\\] where \\[\\begin{align} \\text{time}_{1} = \\{0.000, 0.125, 0.250, 0.375, 0.500, 0.625, 0.750, 0.875, 1.000\\} \\text{time}_{2} = \\text{time}_{1}^{2} = \\{0.000, 0.0157, 0.063, 0.140, 0.250, 0.391, 0.563, 0.766, 1.000\\} \\end{align}\\] and \\(y_{ti}\\) is the cortisol level for individual \\(i\\) at time \\(t\\) \\(\\beta_{0i}\\) is the cortisol level at baseline individual \\(i\\) \\(\\beta_{1i}\\) is the expected linear change in cortisol for individual \\(i\\) at each occasion \\(\\beta_{2i}\\) is the expected quadratic change in cortisol for individual \\(i\\) at each occasion \\(\\text{time}_{1}\\) represents the linear change in time \\(\\text{time}_{2}\\) represents the quadratic change in time \\(e_{it}\\) is the time-specific residual score (within-person deviation) \\(\\gamma_{00}\\) is the fixed effect for the intercept component \\(\\gamma_{10}\\) is the fixed effect for the linear component \\(\\gamma_{20}\\) is the fixed effect for the quadratic component \\(u_{0i}\\) is individual \\(i\\)’s deviation from the intercept component \\(u_{1i}\\) is individual \\(i\\)’s deviation from the linear component \\(u_{2i}\\) is individual \\(i\\)’s deviation from the quadratic component 14.5.2 Fit Model #creating the quadratic time variable cortisol_long$timesq &lt;- cortisol_long$timescaled^2 #quadratic model # cort_quad &lt;- lme(cort ~ 1 + timescaled + timesq, # random = ~ 1 + timescaled + timesq| id, # data = cortisol_long, # na.action = &quot;na.exclude&quot;) # #convergence issues # # cort_quad &lt;- lmer(cort ~ 1 + timescaled + timesq + (1 + timescaled + timesq | id), # data = cortisol_long, # na.action = &quot;na.exclude&quot;) # #convergence issues # summary(cort_quad) # cort_quad &lt;- nlme( # cort ~ (gamma_00 + u_0i) + (gamma_10 + u_1i)*timescaled + (gamma_20 + u_2i)*timesq, # fixed = gamma_00 + gamma_10 + gamma_20 ~1, # random = u_0i + u_1i + u_2i ~1, # group = ~id, # start = c(gamma_00 = 5, gamma_10=1, gamma_20=-1), # data = cortisol_long, # na.action = &quot;na.exclude&quot;, # control = lmeControl(maxIter = 1e8, msMaxIter = 1e8) # ) # # summary(cort_quad) # VarCorr(cort_quad) cort_quad &lt;- nlme( cort ~ (gamma_00) + (gamma_10)*timescaled + (gamma_20)*timesq, fixed = gamma_00 + gamma_10 + gamma_20 ~1, random = gamma_00 + gamma_10 + gamma_20 ~1, group = ~id, start = c(gamma_00 = 5, gamma_10=1, gamma_20=-1), data = cortisol_long, na.action = &quot;na.exclude&quot;, control = lmeControl(maxIter = 1e8, msMaxIter = 1e8) ) summary(cort_quad) ## Nonlinear mixed-effects model fit by maximum likelihood ## Model: cort ~ (gamma_00) + (gamma_10) * timescaled + (gamma_20) * timesq ## Data: cortisol_long ## AIC BIC logLik ## 1675.113 1712.349 -827.5567 ## ## Random effects: ## Formula: list(gamma_00 ~ 1, gamma_10 ~ 1, gamma_20 ~ 1) ## Level: id ## Structure: General positive-definite, Log-Cholesky parametrization ## StdDev Corr ## gamma_00 1.114483 gmm_00 gmm_10 ## gamma_10 5.234697 0.633 ## gamma_20 5.022927 -0.972 -0.433 ## Residual 3.051823 ## ## Fixed effects: gamma_00 + gamma_10 + gamma_20 ~ 1 ## Value Std.Error DF t-value p-value ## gamma_00 3.60086 0.4686628 270 7.683254 0 ## gamma_10 41.05077 2.1881480 270 18.760510 0 ## gamma_20 -30.16960 2.1043726 270 -14.336622 0 ## Correlation: ## gmm_00 gmm_10 ## gamma_10 -0.560 ## gamma_20 0.380 -0.872 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -2.24547222 -0.67200638 0.03091654 0.53187170 2.57538830 ## ## Number of Observations: 306 ## Number of Groups: 34 VarCorr(cort_quad) ## id = pdLogChol(list(gamma_00 ~ 1,gamma_10 ~ 1,gamma_20 ~ 1)) ## Variance StdDev Corr ## gamma_00 1.242072 1.114483 gmm_00 gmm_10 ## gamma_10 27.402048 5.234697 0.633 ## gamma_20 25.229796 5.022927 -0.972 -0.433 ## Residual 9.313626 3.051823 14.5.3 Predicted Trajectories #obtaining predicted scores for individuals cortisol_long$pred_quad &lt;- predict(cort_quad) #obtaining predicted scores for prototype cortisol_long$proto_quad &lt;- predict(cort_quad, level=0) #plotting predicted trajectories #intraindividual change trajetories ggplot(data = cortisol_long, aes(x = time, y = pred_quad, group = id)) + #geom_point(color=&quot;black&quot;) + geom_line(color=&quot;black&quot;) + geom_line(aes(x = time, y = proto_quad), color=&quot;red&quot;,size=2) + xlab(&quot;Time&quot;) + ylab(&quot;Cortisol&quot;) + ylim(0,30) + scale_x_continuous(breaks=seq(0,8,by=1)) 14.5.4 Interpretation In isolation, the meaning of each parameter in the quadratic growth model is difficult to interpret. The slope parameters must be examined in relation to one another, as it is only their combination that produces a specific trajectory. For example, consider the mean values of \\(\\beta_{0}\\),\\(\\beta_{1}\\), and \\(\\beta_{2}\\) obtained from the fitted model. The mean of \\(\\beta_{0} = 3.6\\) is interpreted as the average level of cortisol at the baseline assessment (\\(t=0\\)). The mean of \\(\\beta_{1} = 41.1\\) could be interpreted as a “gas pedal” of sorts, representing some underlying process that propels cortisol levels higher by the same amount on each successive occasion. At the same time, the mean of \\(\\beta_{2} = -30.2\\) could be interpreted as representing another underlying process, that of a “brake pedal” that brings cortisol levels lower and that is pressed harder and harder as time moves forward (because \\(\\beta_{2}\\) operates on \\(\\text{time}_{2}=\\text{time}_{1}^{2}\\), which consists of squared terms, its influence gains strength on each successive occasion). From the plot we can see the co-existing “gas pedal” linear process and “brake pedal” quadratic process acting in concert on the individual trajectories. During the first few measurement occasions, when the brake pedal is less influential, cortisol increases, however, during the later occasions, when the brake pedal is stronger, cortisol levels decrease. Furthermore, \\(\\beta_{1}\\) and \\(\\beta_{2}\\) correlate negatively (\\(-.43\\)) indicating that when the linear process is strong, the quadratic process is relatively weak. 14.5.5 Interpretational Caution In closing, quadratic (and most polynomial models) can be difficult to interpret (see Cudeck and du Toit (2002) for more details) in terms of the coefficients alone because multiple combination of gas and brake pedals produce similar trajectories difficult to understand big versus small gas pedal is without knowing brake pedal interindividual differences in these parameters are difficult to map on to particular aspects of an underlying developmental theory of change theories are often not so explicit regarding the expected interindividual differences in intraindividual accelerations and deceleration. 14.5.6 Nonlinear or Linear Model? As we previously mentioned a model is properly termed as nonlinear if the derivatives of the model with respect to the model parameters depend on one or more parameters. Here is why a polynomial model is not formally a nonlinear model \\[\\begin{align} y_{ti} = &amp; \\beta_{0i} + \\beta_{1i}\\text{time}+ \\beta_{2i}\\text{time}^{2} + e_{ti} \\end{align}\\] If we take derivatives of \\(y\\) with respect to parameters \\(\\beta_{0i}, \\beta_{1i}, \\beta_{2i}\\) we get \\[\\begin{align} \\frac{\\partial y}{\\partial \\beta_{0}} = &amp; 1 \\\\ \\frac{\\partial y}{\\partial \\beta_{1}} = &amp; t \\\\ \\frac{\\partial y}{\\partial \\beta_{2}} = &amp; t^2 \\\\ \\end{align}\\] The derivatives do not depend on a model parameter; thus, the polynomial model is linear in its parameters (a linear model that produces a nonlinear trajectory). References "],["14.6-latent-basis-cortisol.html", "14.6 Latent Basis (Cortisol)", " 14.6 Latent Basis (Cortisol) Another model we can consider is the latent basis growth model. The latent basis growth model is a flexible model that can approximate a variety of nonlinear trajectories. 14.6.1 Equation We can write the latent basis model as \\[\\begin{align} y_{ti} = &amp; \\beta_{0i} + \\beta_{1i}\\text{time}_{LB} + e_{ti}, &amp; e_{ti} \\sim \\mathcal{N}(0,\\sigma^{2}_{e}) &amp;&amp; \\: [\\text{Level 1}] \\\\ \\beta_{0i} = &amp; \\gamma_{00} + u_{0i}, &amp; u_{0i} \\sim \\mathcal{N}(0,\\sigma^{2}_{u0}) &amp;&amp; \\: [\\text{Level 2}] \\\\ \\beta_{1i} = &amp; \\gamma_{10} + u_{1i}, &amp; u_{1i} \\sim \\mathcal{N}(0,\\sigma^{2}_{u1}) &amp;&amp; \\: \\\\ y_{ti} = &amp; \\gamma_{00} + \\gamma_{10}\\text{time}_{LB} + u_{0i} + u_{1i}\\text{time} + e_{ti}, &amp; &amp;&amp; \\: [\\text{Combined}] \\end{align}\\] where \\[\\begin{align} \\text{time}_{LB} = \\{0, a_{1},a_{2},a_{3},a_{4},a_{5},a_{6}, a_{7},1\\} \\end{align}\\] and \\(y_{ti}\\) is the repeated measures score for individual \\(i\\) at time \\(t\\) \\(\\beta_{0i}\\) is the random intercept for individual \\(i\\) \\(\\beta_{1i}\\) is the shape factor for individual \\(i\\) \\(\\text{time}_{LB}\\) is the basis coefficient at time \\(t\\) basis coefficients describe the pattern of change w.r.t time. \\(e_{it}\\) is the time-specific residual score \\(\\gamma_{00}\\) is the fixed-effect for the intercept \\(\\gamma_{10}\\) is the fixed-effect for the shape factor \\(u_{0i}\\) is individual \\(i\\)’s deviation from the intercept fixed effect \\(u_{1i}\\) is individual \\(i\\)’s deviation from the shape fixed effect 14.6.2 Fit Model #creating time-dummy variables cortisol_long$time0 &lt;- ifelse(cortisol_long$time ==0, 1, 0) cortisol_long$time1 &lt;- ifelse(cortisol_long$time ==1, 1, 0) cortisol_long$time2 &lt;- ifelse(cortisol_long$time ==2, 1, 0) cortisol_long$time3 &lt;- ifelse(cortisol_long$time ==3, 1, 0) cortisol_long$time4 &lt;- ifelse(cortisol_long$time ==4, 1, 0) cortisol_long$time5 &lt;- ifelse(cortisol_long$time ==5, 1, 0) cortisol_long$time6 &lt;- ifelse(cortisol_long$time ==6, 1, 0) cortisol_long$time7 &lt;- ifelse(cortisol_long$time ==7, 1, 0) cortisol_long$time8 &lt;- ifelse(cortisol_long$time ==8, 1, 0) #latent basis model cort_latentb &lt;- nlme( cort ~ gamma_00 + time0*(gamma_10*0) + time1*(gamma_10*a1) + time2*(gamma_10*a2) + time3*(gamma_10*a3) + time4*(gamma_10*a4) + time5*(gamma_10*a5) + time6*(gamma_10*a6) + time7*(gamma_10*a7) + time8*(gamma_10*1), fixed = gamma_00 + gamma_10 + a1 + a2 + a3 + a4 + a5 + a6 + a7 ~ 1, random = gamma_00 + gamma_10 ~ 1, groups =~ id, start = c(gamma_00=5.5,gamma_10=11.5,a1=.3, a2=.4, a3=.5, a4=.6, a5=.7, a6=.8, a7=.9), data = cortisol_long, na.action = na.exclude, control = lmeControl(maxIter = 1e8, msMaxIter = 1e8) ) summary(cort_latentb) ## Nonlinear mixed-effects model fit by maximum likelihood ## Model: cort ~ gamma_00 + time0 * (gamma_10 * 0) + time1 * (gamma_10 * a1) + time2 * (gamma_10 * a2) + time3 * (gamma_10 * a3) + time4 * (gamma_10 * a4) + time5 * (gamma_10 * a5) + time6 * (gamma_10 * a6) + time7 * (gamma_10 * a7) + time8 * (gamma_10 * 1) ## Data: cortisol_long ## AIC BIC logLik ## 1569.164 1617.571 -771.5821 ## ## Random effects: ## Formula: list(gamma_00 ~ 1, gamma_10 ~ 1) ## Level: id ## Structure: General positive-definite, Log-Cholesky parametrization ## StdDev Corr ## gamma_00 1.698578 gmm_00 ## gamma_10 2.735103 0.001 ## Residual 2.529224 ## ## Fixed effects: gamma_00 + gamma_10 + a1 + a2 + a3 + a4 + a5 + a6 + a7 ~ 1 ## Value Std.Error DF t-value p-value ## gamma_00 5.342011 0.5207451 264 10.25840 0.0000 ## gamma_10 10.447225 0.7660285 264 13.63817 0.0000 ## a1 -0.041829 0.0582607 264 -0.71796 0.4734 ## a2 0.522586 0.0500425 264 10.44285 0.0000 ## a3 1.118113 0.0611387 264 18.28813 0.0000 ## a4 1.336652 0.0689221 264 19.39364 0.0000 ## a5 1.081029 0.0599665 264 18.02722 0.0000 ## a6 0.951968 0.0562971 264 16.90972 0.0000 ## a7 1.008691 0.0578269 264 17.44327 0.0000 ## Correlation: ## gmm_00 gmm_10 a1 a2 a3 a4 a5 a6 ## gamma_10 -0.455 ## a1 -0.581 0.404 ## a2 -0.314 -0.023 0.262 ## a3 0.056 -0.446 -0.066 0.287 ## a4 0.151 -0.534 -0.150 0.261 0.589 ## a5 0.037 -0.427 -0.050 0.291 0.540 0.577 ## a6 -0.034 -0.355 0.013 0.306 0.501 0.526 0.495 ## a7 -0.002 -0.388 -0.015 0.300 0.519 0.550 0.512 0.481 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -2.4004317 -0.6084323 -0.1077279 0.5806294 2.8252173 ## ## Number of Observations: 306 ## Number of Groups: 34 VarCorr(cort_latentb) ## id = pdLogChol(list(gamma_00 ~ 1,gamma_10 ~ 1)) ## Variance StdDev Corr ## gamma_00 2.885168 1.698578 gmm_00 ## gamma_10 7.480787 2.735103 0.001 ## Residual 6.396974 2.529224 14.6.3 Predicted Trajectories #obtaining predicted scores for individuals cortisol_long$pred_latentb &lt;- predict(cort_latentb) #obtaining predicted scores for prototype cortisol_long$proto_latentb &lt;- predict(cort_latentb, level=0) #plotting predicted trajectories #intraindividual change trajetories ggplot(data = cortisol_long, aes(x = time, y = pred_latentb, group = id)) + #geom_point(color=&quot;black&quot;) + geom_line(color=&quot;black&quot;) + geom_line(aes(x = time, y = proto_latentb), color=&quot;red&quot;,size=2) + xlab(&quot;Time&quot;) + ylab(&quot;Cortisol&quot;) + ylim(0,30) + scale_x_continuous(breaks=seq(0,8,by=1)) + theme_classic() 14.6.4 Interpretation In the latent basis model, rather than fixing the shape coefficients like in the linear model (e.g. \\(t=\\{1,2,3,4,\\dots,T\\}\\)), we estimate the shape directly from the data. The mean intercept level of cortisol was \\(5.3\\), meaning that on average individual “development” of cortisol proceeded from this initial baseline level, taken at the outset of the experiment. The average total amount of growth is given by the mean value of \\(10.4\\). From the predicted trajectories we can see that all individuals’ cortisol levels rise and fall over time in the same way, only the extent of the rise and fall (and the starting point) differs between persons. In other words, the underlying theory suggests that, from their individual baseline levels, the process of cortisol response proceeds via the same nonlinear process for all individuals, with individuals only differing in the amplitude of this response. "],["14.7-exponential-growth-cortisol.html", "14.7 Exponential Growth (Cortisol)", " 14.7 Exponential Growth (Cortisol) Now let’s consider a model that is nonlinear in the parameters (Type II), the exponential growth model. This is a nonlinear model that also produces a nonlinear trajectory. \\[\\begin{align} y_{ti} = &amp; \\beta_{0i} + \\beta_{1i}(1-e^{-\\alpha\\text{time}}) + e_{ti}, &amp; e_{ti} \\sim \\mathcal{N}(0,\\sigma^{2}_{e}) &amp;&amp; \\: [\\text{Level 1}] \\\\ \\beta_{0i} = &amp; \\gamma_{00} + u_{0i}, &amp; u_{0i} \\sim \\mathcal{N}(0,\\sigma^{2}_{u0}) &amp;&amp; \\: [\\text{Level 2}] \\\\ \\beta_{1i} = &amp; \\gamma_{10} + u_{1i}, &amp; u_{1i} \\sim \\mathcal{N}(0,\\sigma^{2}_{u1}) &amp;&amp; \\: \\\\ y_{ti} = &amp; \\gamma_{00} + \\gamma_{10}\\text{time} + u_{0i} + u_{1i}\\text{time} + e_{ti}, &amp; &amp;&amp; \\: [\\text{Combined}] \\end{align}\\] where \\[\\begin{align} \\text{time} = &amp; \\{0/8, 1/8, 2/8, 3/8, 4/8, 5/8, 6/8, 7/8, 8/8\\}\\\\ = &amp; \\{0.000, 0.125, 0.250, 0.375, 0.500, 0.625, 0.750, 0.875, 1.000\\} \\end{align}\\] and \\(y_{ti}\\) is the repeated measures score for individual \\(i\\) at time \\(t\\) \\(\\beta_{0i}\\) is the random intercept for individual \\(i\\) individual \\(i\\)’s asymptotic level or the limit of individual capacity for cortisol note in this model, the intercept represents individual performance toward the end of observation period \\(\\beta_{1i}\\) is the amount of change from the intercept to the asymptotic level for individual \\(i\\) \\(\\beta_{1i}\\) represents an individual’s potential for change from his or her initial level \\(\\beta_{0i}+\\beta_{1i}\\) represents the baseline level for individual \\(i\\) \\(\\alpha\\) is an estimated parameter that represents the rate of approach to the asymptotic level \\(\\text{time}\\) represents the measurement occasion \\(e_{it}\\) is the time-specific residual score \\(\\gamma_{00}\\) is the fixed-effect for the intercept \\(\\gamma_{10}\\) is the fixed-effect for the rate of change \\(u_{0i}\\) is individual \\(i\\)’s deviation from the intercept fixed effect \\(u_{1i}\\) is individual \\(i\\)’s deviation from the shape fixed effect 14.7.1 Fit Model # cort_expo &lt;- nlme( # cort ~ (gamma_00 + u_0i) + (gamma_10 + u_1i)*(exp(-1*alpha*timescaled)), # fixed = gamma_00 + gamma_10 + alpha ~1, # random = u_0i + u_1i~1, # group = ~id, # start = c(gamma_00=17, gamma_10=-14, alpha=0.5), # data = cortisol_long, # na.action = &quot;na.exclude&quot;, # control = lmeControl(maxIter = 1e8, msMaxIter = 1e8) # ) # # summary(cort_expo) # VarCorr(cort_expo) cort_expo &lt;- nlme( cort ~ (gamma_00) + (gamma_10)*(exp(-1*alpha*timescaled)), fixed = gamma_00 + gamma_10 + alpha ~1, random = gamma_00 + gamma_10~1, group = ~id, start = c(gamma_00=17, gamma_10=-14, alpha=0.5), data = cortisol_long, na.action = &quot;na.exclude&quot;, control = lmeControl(maxIter = 1e8, msMaxIter = 1e8) ) ## Warning in nlme.formula(cort ~ (gamma_00) + (gamma_10) * (exp(-1 * alpha * : ## Iteration 4, LME step: nlminb() did not converge (code = 1). PORT message: ## function evaluation limit reached without convergence (9) summary(cort_expo) ## Nonlinear mixed-effects model fit by maximum likelihood ## Model: cort ~ (gamma_00) + (gamma_10) * (exp(-1 * alpha * timescaled)) ## Data: cortisol_long ## AIC BIC logLik ## 1730.24 1756.305 -858.1202 ## ## Random effects: ## Formula: list(gamma_00 ~ 1, gamma_10 ~ 1) ## Level: id ## Structure: General positive-definite, Log-Cholesky parametrization ## StdDev Corr ## gamma_00 3.671965 gmm_00 ## gamma_10 3.984989 -1 ## Residual 3.594782 ## ## Fixed effects: gamma_00 + gamma_10 + alpha ~ 1 ## Value Std.Error DF t-value p-value ## gamma_00 17.202417 0.7688476 270 22.37429 0 ## gamma_10 -13.735036 0.9550830 270 -14.38099 0 ## alpha 4.110184 0.4853526 270 8.46845 0 ## Correlation: ## gmm_00 gmm_10 ## gamma_10 -0.788 ## alpha -0.442 0.077 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -2.0678417 -0.6955952 -0.0344403 0.6058747 2.7299355 ## ## Number of Observations: 306 ## Number of Groups: 34 VarCorr(cort_expo) ## id = pdLogChol(list(gamma_00 ~ 1,gamma_10 ~ 1)) ## Variance StdDev Corr ## gamma_00 13.48333 3.671965 gmm_00 ## gamma_10 15.88014 3.984989 -1 ## Residual 12.92245 3.594782 14.7.2 Predicted Trajectories #obtaining predicted scores for individuals cortisol_long$pred_expo &lt;- predict(cort_expo) #obtaining predicted scores for prototype cortisol_long$proto_expo &lt;- predict(cort_expo, level=0) #plotting predicted trajectories #intraindividual change trajetories ggplot(data = cortisol_long, aes(x = time, y = pred_expo, group = id)) + #geom_point(color=&quot;black&quot;) + geom_line(color=&quot;black&quot;) + geom_line(aes(x = time, y = proto_expo), color=&quot;red&quot;,size=2) + xlab(&quot;Time&quot;) + ylab(&quot;Cortisol&quot;) + ylim(0,30) + scale_x_continuous(breaks=seq(0,8,by=1)) + theme_classic() 14.7.3 Interpretation Fitted to our example data, we articulate a theory wherein individuals enter the testing situation at some baseline level of cortisol (which on average is given by mean of \\(beta_{0}\\), \\(17.2\\), plus the mean of \\(\\beta_{1}\\), \\(-13.7\\), which equals \\(3.5\\)). After beginning the simulator cortisol levels are driven at the exponential rate of \\(\\alpha=4.1\\) to some personal limit or asymptote (with the average limit being \\(17.2\\)). 14.7.4 Nonlinear or Linear Model? What about the exponential model. Here is is why a exponential model is formally a nonlinear model \\[\\begin{align} y_{ti} = &amp; \\beta_{0i} + \\beta_{1i}(1-e^{-\\alpha\\text{time}}) + e_{ti} \\end{align}\\] If we take derivatives of \\(y\\) with respect to parameters \\(\\beta_{0i}, \\beta_{1i}, \\alpha\\) we get \\[\\begin{align} \\frac{\\partial y}{\\partial \\beta_{0}} = &amp; 1 \\\\ \\frac{\\partial y}{\\partial \\beta_{1}} = &amp; (1-e^{-\\alpha\\text{time}}) \\\\ \\frac{\\partial y}{\\partial \\alpha} = &amp; \\beta_{1} \\text{time}(1-e^{-\\alpha\\text{time}}) \\end{align}\\] The derivatives depend on non-constant (or estimated coefficients). This model is multiplicative nonlinear it its parameters. A nonlinear model that produces a nonlinear trajectory. "],["14.8-multiphase-growth-cortisol.html", "14.8 Multiphase Growth (Cortisol)", " 14.8 Multiphase Growth (Cortisol) Now let’s consider a multiphase growth model. Multiphase models are based on spline regression models in which two or more regression lines are connected, allowing for the modeling of multiple processes that together may influence intraindividual change over time. For example, consider our data as being potentially characterized by a baseline phase, an intervention phase and a recovery phase. The multiphase model allows us to articulate how and when each of these intraindividual change processes engages and contributes to the observed scores. The change points, when one process is “turned off” and another “turned on” are \\(t = 2\\) and \\(t = 5\\). Thus, baseline level process, articulated by the intercept, drive observed levels of cortisol when \\(t &lt; 2\\); the production process, \\(\\text{time}_{1}\\), is engaged when \\(2 ≤ t &lt; 5\\), and the dissipation process, \\(\\text{time}_{1}\\), is engaged when \\(t ≥ 5\\). To allow for nonlinearity in the production and dissipation processes we adapt the latent basis GCM presented above to estimate the relevant basis coefficients within each of the multiple intraindividual change process vectors used here. 14.8.1 Equation The equation for the multiphase model can be written as \\[\\begin{align} y_{ti} = &amp; \\beta_{0i} + \\beta_{1i}\\text{time}_{1} + \\beta_{2i}\\text{time}_{2}+ e_{ti}, &amp; e_{ti} \\sim \\mathcal{N}(0,\\sigma^{2}_{e}) &amp;&amp; \\: [\\text{Level 1}] \\\\ \\beta_{0i} = &amp; \\gamma_{00} + u_{0i}, &amp; u_{0i} \\sim \\mathcal{N}(0,\\sigma^{2}_{u0}) &amp;&amp; \\: [\\text{Level 2}] \\\\ \\beta_{1i} = &amp; \\gamma_{10} + u_{1i}, &amp; u_{1i} \\sim \\mathcal{N}(0,\\sigma^{2}_{u1}) &amp;&amp; \\: \\\\ \\beta_{2i} = &amp; \\gamma_{20} + u_{2i}, &amp; u_{2i} \\sim \\mathcal{N}(0,\\sigma^{2}_{u2}) &amp;&amp; \\: \\\\ y_{ti} = &amp; \\gamma_{00} + \\gamma_{10}\\text{time}_{1} + \\gamma_{20}\\text{time}_{2}+ u_{0i} + u_{1i}\\text{time}_{2} + u_{2i}\\text{time}_{2} + e_{ti}, &amp; &amp;&amp; \\: [\\text{Combined}] \\end{align}\\] where \\[\\begin{align} \\text{time}_{1} = &amp; \\{0, 0, $alpha_2$, $alpha_3$, 1, 1, 1, 1, 1\\}\\\\ \\text{time}_{2} = &amp; \\{0, 0, 0, 0, 0, $\\alpha_{5}$, $\\alpha_{6}$, $\\alpha_{7}$, 1}\\\\ \\end{align}\\] and \\(y_{ti}\\) is the repeated measures score for individual \\(i\\) at time \\(t\\) \\(\\beta_{0i}\\) is the random intercept for individual \\(i\\) at \\(t=0\\) the cortisol level for individual \\(i\\) during baseline \\(\\beta_{1i}\\) is the random slope for individual \\(i\\) during production \\(\\beta_{2i}\\) is the random slope for individual \\(i\\) during recovery \\(\\text{time}_{1}\\) represents the shape factor for the production phase \\(\\text{time}_{2}\\) represents the shape factor for the recovery phase \\(e_{it}\\) is the time-specific residual score \\(\\gamma_{00}\\) is the fixed-effect for the baseline phase \\(\\gamma_{10}\\) is the fixed-effect for the production phase \\(\\gamma_{10}\\) is the fixed-effect for the recovery phase \\(u_{0i}\\) is individual \\(i\\)’s deviation from the baseline fixed effect \\(u_{1i}\\) is individual \\(i\\)’s deviation from the production fixed effect \\(u_{1i}\\) is individual \\(i\\)’s deviation from the recovery fixed effect 14.8.2 Fit Model cort_multi &lt;- nlme(cort ~ gamma_00 + (0*gamma_10 + 0*gamma_20)*time0 + (0*gamma_10 + 0*gamma_20)*time1 + (a2*gamma_10 + 0*gamma_20)*time2 + (a3*gamma_10 + 0*gamma_20)*time3 + (1*gamma_10 + 0*gamma_20)*time4 + (1*gamma_10 + a5*gamma_20)*time5 + (1*gamma_10 + a6*gamma_20)*time6 + (1*gamma_10 + a7*gamma_20)*time7 + (1*gamma_10 + 1*gamma_20)*time8, fixed = gamma_00 + gamma_10 + gamma_20 + a2 + a3 + a5 + a6 + a7 ~ 1, random = gamma_00 + gamma_10 + gamma_20 ~ 1, groups =~ id, start = c(gamma_00=15,gamma_10=10,gamma_20=-4, a2=.4, a3=.5, a5=.7, a6=.8, a7=.9), data = cortisol_long, na.action = na.exclude, control = lmeControl(maxIter = 1e8, msMaxIter = 1e8) ) summary(cort_multi) ## Nonlinear mixed-effects model fit by maximum likelihood ## Model: cort ~ gamma_00 + (0 * gamma_10 + 0 * gamma_20) * time0 + (0 * gamma_10 + 0 * gamma_20) * time1 + (a2 * gamma_10 + 0 * gamma_20) * time2 + (a3 * gamma_10 + 0 * gamma_20) * time3 + (1 * gamma_10 + 0 * gamma_20) * time4 + (1 * gamma_10 + a5 * gamma_20) * time5 + (1 * gamma_10 + a6 * gamma_20) * time6 + (1 * gamma_10 + a7 * gamma_20) * time7 + (1 * gamma_10 + 1 * gamma_20) * time8 ## Data: cortisol_long ## AIC BIC logLik ## 1412.881 1468.735 -691.4407 ## ## Random effects: ## Formula: list(gamma_00 ~ 1, gamma_10 ~ 1, gamma_20 ~ 1) ## Level: id ## Structure: General positive-definite, Log-Cholesky parametrization ## StdDev Corr ## gamma_00 2.161867 gmm_00 gmm_10 ## gamma_10 3.580496 -0.315 ## gamma_20 4.241375 -0.041 -0.286 ## Residual 1.535205 ## ## Fixed effects: gamma_00 + gamma_10 + gamma_20 + a2 + a3 + a5 + a6 + a7 ~ 1 ## Value Std.Error DF t-value p-value ## gamma_00 5.118082 0.4201167 265 12.18252 0 ## gamma_10 14.248891 0.6998856 265 20.35889 0 ## gamma_20 -3.797260 0.8089645 265 -4.69398 0 ## a2 0.410745 0.0210662 265 19.49782 0 ## a3 0.860230 0.0238885 265 36.01023 0 ## a5 0.715784 0.0598000 265 11.96963 0 ## a6 1.046663 0.0683974 265 15.30267 0 ## a7 1.042029 0.0682431 265 15.26937 0 ## Correlation: ## gmm_00 gmm_10 gmm_20 a2 a3 a5 a6 ## gamma_10 -0.372 ## gamma_20 -0.033 -0.352 ## a2 -0.158 -0.032 0.111 ## a3 -0.033 -0.214 0.205 0.250 ## a5 0.000 0.059 0.073 -0.056 -0.104 ## a6 0.000 -0.008 0.165 0.007 0.014 0.393 ## a7 0.000 -0.007 0.164 0.006 0.012 0.392 0.504 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -3.4084324 -0.5077968 -0.0178037 0.4909589 3.7024985 ## ## Number of Observations: 306 ## Number of Groups: 34 VarCorr(cort_multi) ## id = pdLogChol(list(gamma_00 ~ 1,gamma_10 ~ 1,gamma_20 ~ 1)) ## Variance StdDev Corr ## gamma_00 4.673669 2.161867 gmm_00 gmm_10 ## gamma_10 12.819948 3.580496 -0.315 ## gamma_20 17.989264 4.241375 -0.041 -0.286 ## Residual 2.356854 1.535205 14.8.3 Predicted Trajectories #obtaining predicted scores for individuals cortisol_long$pred_multi &lt;- predict(cort_multi) #obtaining predicted scores for prototype cortisol_long$proto_multi &lt;- predict(cort_multi, level=0) #plotting predicted trajectories #intraindividual change trajetories ggplot(data = cortisol_long, aes(x = time, y = pred_multi, group = id)) + #geom_point(color=&quot;black&quot;) + geom_line(color=&quot;black&quot;) + geom_line(aes(x = time, y = proto_multi), color=&quot;red&quot;,size=2) + xlab(&quot;Time&quot;) + ylab(&quot;Cortisol&quot;) + ylim(0,30) + scale_x_continuous(breaks=seq(0,8,by=1)) + theme_classic() 14.8.4 Interpretation The average baseline cortisol level (\\(+5.1\\)); the average amount of cortisol production (\\(+14.3\\)); and the average amount by which cortisol declines in the dissipation phase (\\(–3.8\\)) can all be read from the fixed effects table. Looking at the predicted trajectories it is clear the multiphase growth curve model represents the complexity of intraindividual change and interindividual differences in intraindividual change we observed in the raw data. Furthermore, the model parameters can be understood in a straightforward manner – one in which intraindividual changes in cortisol are characterized by multiple processes, each of which is engaged at know predetermined time in this experimental context. "],["14.9-bilinear-spline-cortisol.html", "14.9 Bilinear Spline (Cortisol)", " 14.9 Bilinear Spline (Cortisol) Spline growth models partition time into discrete periods. Observed changes within each phase can then be described with simple growth models. The discrete segments of the growth model connect at knot points or transition points. Specifically, a bilinear growth model partitions a trajectory into two phases, each defined by a linear trajectory. Simple spline growth models can be useful for testing specific hypotheses (e.g. does cognitive decline begin at 60 years of age?). 14.9.1 Equation The equation for the bilinear spline model can be written as \\[\\begin{align} y_{ti} = &amp; \\beta_{0i} + \\beta_{1i}\\text{time} + \\beta_{2i}\\text{max}(\\text{time}-k_{1},0)+ e_{ti}, &amp; e_{ti} \\sim \\mathcal{N}(0,\\sigma^{2}_{e}) &amp;&amp; \\: [\\text{Level 1}] \\\\ \\beta_{0i} = &amp; \\gamma_{00} + u_{0i}, &amp; u_{0i} \\sim \\mathcal{N}(0,\\sigma^{2}_{u0}) &amp;&amp; \\: [\\text{Level 2}] \\\\ \\beta_{1i} = &amp; \\gamma_{10} + u_{1i}, &amp; u_{1i} \\sim \\mathcal{N}(0,\\sigma^{2}_{u1}) &amp;&amp; \\: \\\\ \\beta_{2i} = &amp; \\gamma_{20} + u_{2i}, &amp; u_{2i} \\sim \\mathcal{N}(0,\\sigma^{2}_{u2}) &amp;&amp; \\: \\\\ y_{ti} = &amp; \\gamma_{00} + \\gamma_{10}\\text{time} + \\gamma_{20}\\text{max}(\\text{time}-k_{1},0) + u_{0i} + u_{1i}\\text{time} + u_{2i}\\text{max}(\\text{time}-k_{1},0) + e_{ti}, &amp; &amp;&amp; \\: [\\text{Combined}] \\end{align}\\] where \\[\\begin{align} \\text{time} = &amp; \\{0, 1, 2, 3, 4, 5, 6, 7, 8\\} \\end{align}\\] and \\(y_{ti}\\) is the repeated measures score for individual \\(i\\) at time \\(t\\) \\(\\beta_{0i}\\) is the random intercept for individual \\(i\\) at \\(t=0\\) \\(\\beta_{1i}\\) is the random slope for individual \\(i\\) when \\(t &lt; k\\) \\(\\beta_{2i}\\) is the change in the random slope for individual \\(i\\) when \\(t &gt; k\\) rate of change when \\(t &gt; k\\) is equal to \\(\\beta_{1i}+\\beta_{2i}\\) \\(\\text{time}\\) represents the time index \\(k_{1}\\) is the knot point (here we set \\(k_{1}=4\\)) \\(e_{it}\\) is the time-specific residual score \\(\\gamma_{00}\\) is the fixed-effect for the intercept \\(\\gamma_{10}\\) is the fixed-effect for the pre-knot phase \\(\\gamma_{10}\\) is the fixed-effect for the post-knot phase \\(u_{0i}\\) is individual \\(i\\)’s deviation from the intercept fixed effect \\(u_{1i}\\) is individual \\(i\\)’s deviation from the pre-knot fixed effect \\(u_{1i}\\) is individual \\(i\\)’s deviation from the post-knot fixed effect 14.9.2 Fit Model cort_spline &lt;- nlme( cort ~ g_00 + g_10*time + g_20*(pmax(time-4,0)), data = cortisol_long, fixed = g_00 + g_10 + g_20 ~1, random = g_00 + g_10 + g_20 ~1, groups =~ id, start = c(5,5,-5), na.action = na.exclude, control = lmeControl(maxIter = 1e8, msMaxIter = 1e8) ) summary(cort_spline) ## Nonlinear mixed-effects model fit by maximum likelihood ## Model: cort ~ g_00 + g_10 * time + g_20 * (pmax(time - 4, 0)) ## Data: cortisol_long ## AIC BIC logLik ## 1589.921 1627.157 -784.9606 ## ## Random effects: ## Formula: list(g_00 ~ 1, g_10 ~ 1, g_20 ~ 1) ## Level: id ## Structure: General positive-definite, Log-Cholesky parametrization ## StdDev Corr ## g_00 1.8542723 g_00 g_10 ## g_10 0.8201397 -0.327 ## g_20 1.3000111 0.043 -0.539 ## Residual 2.4548968 ## ## Fixed effects: g_00 + g_10 + g_20 ~ 1 ## Value Std.Error DF t-value p-value ## g_00 3.800588 0.4540068 270 8.371215 0 ## g_10 3.722647 0.1856610 270 20.050776 0 ## g_20 -4.725000 0.3102078 270 -15.231727 0 ## Correlation: ## g_00 g_10 ## g_10 -0.560 ## g_20 0.312 -0.696 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -2.29855101 -0.61835285 0.04122671 0.53521318 2.85932987 ## ## Number of Observations: 306 ## Number of Groups: 34 VarCorr(cort_spline) ## id = pdLogChol(list(g_00 ~ 1,g_10 ~ 1,g_20 ~ 1)) ## Variance StdDev Corr ## g_00 3.4383258 1.8542723 g_00 g_10 ## g_10 0.6726292 0.8201397 -0.327 ## g_20 1.6900288 1.3000111 0.043 -0.539 ## Residual 6.0265184 2.4548968 intervals(cort_spline) ## Approximate 95% confidence intervals ## ## Fixed effects: ## lower est. upper ## g_00 2.911137 3.800588 4.690039 ## g_10 3.358916 3.722647 4.086378 ## g_20 -5.332732 -4.725000 -4.117268 ## ## Random Effects: ## Level: id ## lower est. upper ## sd(g_00) 1.1565201 1.85427231 2.97299278 ## sd(g_10) 0.5540587 0.82013973 1.21400349 ## sd(g_20) 0.8287891 1.30001107 2.03915425 ## cor(g_00,g_10) -0.7008445 -0.32741524 0.18687545 ## cor(g_00,g_20) -0.4462538 0.04284904 0.51224108 ## cor(g_10,g_20) -0.8086235 -0.53881013 -0.08173286 ## ## Within-group standard error: ## lower est. upper ## 2.228837 2.454897 2.703884 14.9.3 Predicted Trajectories Note, this method of plotting can break down with spline models as the plotted trajectories are simply meant to approximate the trajectories of interest. Here they do not connect at the know point, while in the real model they would. ggplot(data = cortisol_long) + geom_line(data=cortisol_long, aes(x = time, y = cort, group=factor(id))) + stat_smooth(data=cortisol_long[which(cortisol_long$time&lt;=4),], aes(x=time,y=cort), method=&quot;lm&quot;,colour=&quot;#A3A500&quot;,size=2) + stat_smooth(data=cortisol_long[which(cortisol_long$time&gt;=4),], aes(x=time,y=cort), method=&quot;lm&quot;,colour=&quot;#A3A500&quot;,size=2) + xlab(&quot;Timepoint&quot;) + ylab(&quot;Cortisol Level&quot;) + scale_x_continuous(breaks=seq(0,8,by=10)) + scale_y_continuous(breaks = seq(0,50,by=10)) + theme_classic() + theme(title=element_text(size=12), axis.text=element_text(size=12,colour=&quot;black&quot;), legend.text=element_text(size=12, colour=&quot;black&quot;), plot.title = element_text(hjust=.5, size=12)) ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## `geom_smooth()` using formula = &#39;y ~ x&#39; 14.9.4 Interpretation The mean of the random intercept was \\(3.8\\) mg/dl, indicating the predicted average cortisol at baseline. The mean of the preknot slope was \\(3.7\\), indicating the rate of cortisol growht during the baseline and experimental phases. The mean postknot slope was \\(-4.7\\), indicating the rate of change decreased by \\(4.7\\) after timepoint 4. The variances of the random coefficients were all significant, indicating that subjects varied in each aspect of the growth model. 14.9.4.1 Additional Parameterization We can also obtain a measure of the post-knot slope directly by reparameterizing our model in the following manner: \\[\\begin{align} y_{ti} = &amp; \\beta_{0i} + \\beta_{1i}\\text{min}(\\text{time}-k_{1},0) + \\beta_{2i}\\text{max}(\\text{time}-k_{1},0)+ e_{ti}, &amp; e_{ti} \\sim \\mathcal{N}(0,\\sigma^{2}_{e}) &amp;&amp; \\: [\\text{Level 1}] \\\\ \\beta_{0i} = &amp; \\gamma_{00} + u_{0i}, &amp; u_{0i} \\sim \\mathcal{N}(0,\\sigma^{2}_{u0}) &amp;&amp; \\: [\\text{Level 2}] \\\\ \\beta_{1i} = &amp; \\gamma_{10} + u_{1i}, &amp; u_{1i} \\sim \\mathcal{N}(0,\\sigma^{2}_{u1}) &amp;&amp; \\: \\\\ \\beta_{2i} = &amp; \\gamma_{20} + u_{2i}, &amp; u_{2i} \\sim \\mathcal{N}(0,\\sigma^{2}_{u2}) &amp;&amp; \\: \\\\ y_{ti} = &amp; \\gamma_{00} + \\gamma_{10}\\text{min}(\\text{time}-k_{1},0) + \\gamma_{20}\\text{max}(\\text{time}-k_{1},0) + u_{0i} + u_{1i}\\text{min}(\\text{time}-k_{1},0) + u_{2i}\\text{max}(\\text{time}-k_{1},0) + e_{ti}, &amp; &amp;&amp; \\: [\\text{Combined}] \\end{align}\\] where \\[\\begin{align} \\text{time} = &amp; \\{0, 1, 2, 3, 4, 5, 6, 7, 8\\} \\end{align}\\] and \\(y_{ti}\\) is the repeated measures score for individual \\(i\\) at time \\(t\\) \\(\\beta_{0i}\\) is the random intercept for individual \\(i\\) at \\(t=0\\) \\(\\beta_{1i}\\) is the random slope for individual \\(i\\) when \\(t &lt; k\\) \\(\\beta_{2i}\\) is the change in the random slope for individual \\(i\\) when \\(t &gt; k\\) rate of change when \\(t &gt; k\\) is equal to \\(\\beta_{1i}+\\beta_{2i}\\) \\(\\text{time}\\) represents the time index \\(k_{1}\\) is the knot point (here we set \\(k_{1}=4\\)) \\(e_{it}\\) is the time-specific residual score \\(\\gamma_{00}\\) is the fixed-effect for the intercept \\(\\gamma_{10}\\) is the fixed-effect for the pre-knot phase \\(\\gamma_{10}\\) is the fixed-effect for the post-knot phase \\(u_{0i}\\) is individual \\(i\\)’s deviation from the intercept fixed effect \\(u_{1i}\\) is individual \\(i\\)’s deviation from the pre-knot fixed effect \\(u_{1i}\\) is individual \\(i\\)’s deviation from the post-knot fixed effect 14.9.5 Fit Model cort_spline2 &lt;- nlme( cort ~ g_00 + g_10*(pmin(time-4,0)) + g_20*(pmax(time-4,0)), data = cortisol_long, fixed = g_00 + g_10 + g_20 ~1, random = g_00 + g_10 + g_20 ~1, groups =~ id, start = c(5,5,-5), na.action = na.exclude, control = lmeControl(maxIter = 1e8, msMaxIter = 1e8) ) summary(cort_spline2) ## Nonlinear mixed-effects model fit by maximum likelihood ## Model: cort ~ g_00 + g_10 * (pmin(time - 4, 0)) + g_20 * (pmax(time - 4, 0)) ## Data: cortisol_long ## AIC BIC logLik ## 1589.921 1627.157 -784.9606 ## ## Random effects: ## Formula: list(g_00 ~ 1, g_10 ~ 1, g_20 ~ 1) ## Level: id ## Structure: General positive-definite, Log-Cholesky parametrization ## StdDev Corr ## g_00 3.1964083 g_00 g_10 ## g_10 0.8201397 0.836 ## g_20 1.1016847 -0.001 0.109 ## Residual 2.4548968 ## ## Fixed effects: g_00 + g_10 + g_20 ~ 1 ## Value Std.Error DF t-value p-value ## g_00 18.691176 0.6165977 270 30.313408 0 ## g_10 3.722647 0.1856610 270 20.050777 0 ## g_20 -1.002353 0.2248148 270 -4.458572 0 ## Correlation: ## g_00 g_10 ## g_10 0.792 ## g_20 -0.185 -0.134 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -2.29855108 -0.61835284 0.04122665 0.53521318 2.85932983 ## ## Number of Observations: 306 ## Number of Groups: 34 VarCorr(cort_spline2) ## id = pdLogChol(list(g_00 ~ 1,g_10 ~ 1,g_20 ~ 1)) ## Variance StdDev Corr ## g_00 10.2170261 3.1964083 g_00 g_10 ## g_10 0.6726291 0.8201397 0.836 ## g_20 1.2137093 1.1016847 -0.001 0.109 ## Residual 6.0265185 2.4548968 #intervals(cort_spline2) "],["14.10-references-6.html", "14.10 References", " 14.10 References "]]
