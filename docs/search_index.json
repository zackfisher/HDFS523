[["12-chapter-12.html", "Chapter 12 Introduction to Growth", " Chapter 12 Introduction to Growth This chapter introduces models for repeated measures data (e.g., RM ANOVA, RM MANOVA) along a continuum. The motivation for this introduction is to present how these models are linked together, making the similarities and differences among them easier to identify and understand. We will tie the models together in a multilevel framework, working from repeated measures ANOVA through repeated measures MANOVA to growth models. "],["12.1-introduction-1.html", "12.1 Introduction", " 12.1 Introduction 12.1.1 What is a multilevel model? Multilevel models (MLM) go by many names: General Linear Mixed Model Random Coefficients Model Hierarchical Linear Model 12.1.2 Two Faces of MLM In traditional MLM we typically have a model for the means, and a model for the variance. 12.1.2.1 A Model for the Means Typicalled known as fixed effects Similar to the quantities tested in single-level models How the expected value of an outcome varies based on the values of predictors in our model 12.1.2.2 A Model for the Variances Typically known as random effects Similar to the assumptions used in single-level models Model for how residuals are distributed and vary across observations (persons, groups, and time) 12.1.3 Two-Level Longitudinal Data Between-Person Variation Level 2 or inter-individual differences Time-invariant More of less than other people Within-Person Variation Level 1 or intra-individual differences Time-varying Can only assess with longitudinal studies More or less than one’s average "],["12.2-example-data-5.html", "12.2 Example Data", " 12.2 Example Data Loading some new libraries used in this chapter. library(psych) # for descriptives etc library(ggplot2) # for plotting library(nlme) # for mixed effects models library(lme4) # for mixed effects models library(lmerTest) # to get significance tests from lmer 12.2.1 Data Preparation and Description For our examples, we use 3-occasion WISC data that are equally spaced. Load the repeated measures data filepath &lt;- &quot;https://quantdev.ssri.psu.edu/sites/qdev/files/wisc3raw.csv&quot; wisc3raw &lt;- read.csv(file=url(filepath),header=TRUE) Next, let’s Subset the variables of interest. For this chapter we will include: 3-occasion equally spaced repeated measures (verb2, verb4, verb6) A person-level grouping variable (grade) An ID variable (id) After subsetting let’s take a look at some basic descriptives. varnames &lt;- c(&quot;id&quot;,&quot;verb2&quot;,&quot;verb4&quot;,&quot;verb6&quot;,&quot;grad&quot;) wiscsub &lt;- wisc3raw[ ,varnames] describe(wiscsub) ## vars n mean sd median trimmed mad min max range skew ## id 1 204 102.50 59.03 102.50 102.50 75.61 1.00 204.00 203.00 0.00 ## verb2 2 204 25.42 6.11 25.98 25.40 6.57 5.95 39.85 33.90 -0.06 ## verb4 3 204 32.61 7.32 32.82 32.42 7.18 12.60 52.84 40.24 0.23 ## verb6 4 204 43.75 10.67 42.55 43.46 11.30 17.35 72.59 55.24 0.24 ## grad 5 204 0.23 0.42 0.00 0.16 0.00 0.00 1.00 1.00 1.30 ## kurtosis se ## id -1.22 4.13 ## verb2 -0.34 0.43 ## verb4 -0.08 0.51 ## verb6 -0.36 0.75 ## grad -0.30 0.03 Multilevel modeling analyses typically require a long data set. So, we also reshape from wide to long in order to have a long data set. verblong &lt;- reshape( data = wiscsub, varying = c(&quot;verb2&quot;,&quot;verb4&quot;,&quot;verb6&quot;), timevar = &quot;grade&quot;, idvar = &quot;id&quot;, direction = &quot;long&quot;, sep = &quot;&quot; ) verblong &lt;- verblong[order(verblong$id,verblong$grade), c(&quot;id&quot;,&quot;grade&quot;,&quot;verb&quot;,&quot;grad&quot;)] head(verblong,12) ## id grade verb grad ## 1.2 1 2 26.98 0 ## 1.4 1 4 39.61 0 ## 1.6 1 6 55.64 0 ## 2.2 2 2 14.38 0 ## 2.4 2 4 21.92 0 ## 2.6 2 6 37.81 0 ## 3.2 3 2 33.51 1 ## 3.4 3 4 34.30 1 ## 3.6 3 6 50.18 1 ## 4.2 4 2 28.39 1 ## 4.4 4 4 42.16 1 ## 4.6 4 6 44.72 1 12.2.2 Sample Moments For clarity, let’s consider the basic information representation of the 3-occasion repeated measures data. In particular, data (even non-repeated measures data) are summarized (at the sample-level) as (1) a vector of means and (2) a variance-covariance matrix. #mean vector (from wide data) meanvector &lt;- sapply(wiscsub[ ,c(&quot;verb2&quot;,&quot;verb4&quot;,&quot;verb6&quot;)], mean, na.rm=TRUE) round(meanvector,2) ## verb2 verb4 verb6 ## 25.42 32.61 43.75 #variance-covariance matrix (from wide data) varcovmatrix &lt;- cov(wiscsub[ ,c(&quot;verb2&quot;,&quot;verb4&quot;,&quot;verb6&quot;)], use=&quot;pairwise.complete.obs&quot;) round(varcovmatrix,2) ## verb2 verb4 verb6 ## verb2 37.29 33.82 47.40 ## verb4 33.82 53.58 62.25 ## verb6 47.40 62.25 113.74 Making visual counterparts can also be extremely useful - especially for facilitating higher-level conversations in a research group. Basic sample-level descriptions in visual form. Note that the time variable has been converted to a factor = categorical ggplot(data=verblong, aes(x=factor(grade), y=verb)) + geom_boxplot(notch = TRUE) + stat_summary(fun.y=&quot;mean&quot;, geom=&quot;point&quot;, shape=23, size=3, fill=&quot;white&quot;) + labs(x = &quot;Grade&quot;, y = &quot;Verbal Ability&quot;) + theme_classic() ## Warning: The `fun.y` argument of `stat_summary()` is deprecated as of ggplot2 3.3.0. ## ℹ Please use the `fun` argument instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. pairs.panels(wiscsub[,c(&quot;verb2&quot;,&quot;verb4&quot;,&quot;verb6&quot;)]) Reminder, we should always be careful about the scaling of the x- and y-axes in these plots. One additional recoding for convenience is to center and scale our time variable. This gives us a specific \\(0\\) point and an intuitive \\(0, 1, 2\\) scale that is useful for our didactic purposes. unique(verblong$grade) ## [1] 2 4 6 verblong$time0 &lt;- (verblong$grade-2)/2 # from 2,4,6 to 0,1,2 unique(verblong$time0) ## [1] 0 1 2 head(verblong,12) ## id grade verb grad time0 ## 1.2 1 2 26.98 0 0 ## 1.4 1 4 39.61 0 1 ## 1.6 1 6 55.64 0 2 ## 2.2 2 2 14.38 0 0 ## 2.4 2 4 21.92 0 1 ## 2.6 2 6 37.81 0 2 ## 3.2 3 2 33.51 1 0 ## 3.4 3 4 34.30 1 1 ## 3.6 3 6 50.18 1 2 ## 4.2 4 2 28.39 1 0 ## 4.4 4 4 42.16 1 1 ## 4.6 4 6 44.72 1 2 Plotting the raw data along this new time variable. #plotting intraindividual change RAW DATA ggplot(data = verblong, aes(x = time0, y = verb, group = id)) + ggtitle(&quot;Raw Data&quot;) + geom_point() + geom_line() + xlab(&quot;Time&quot;) + ylab(&quot;WISC Verbal Score&quot;) + ylim(0,100) + xlim(0,2) + theme_classic() Note that the time variable in this plot has NOT been converted to a factor. It is a continuous variable. "],["12.3-a-general-model.html", "12.3 A General Model", " 12.3 A General Model The presentation of all of these models here is an attempt to integrate traditions that are typically kept separate or set against each other. In reality, they are just a few examples of the many, many possible models that exist. Each model is useful in specific situations. The objective of all of our analyses is to deconstruct the data into meaningful and interpretable pieces. Each model does this in a different way, with different assumptions. We can judge the models based on How well they articulate and test our theory, and How well they recover the data (evaluated as misfit to the impled moments). Recall that the regression model may be compactly written as \\[ \\mathbf{Y} = \\mathbf{X}\\mathbf{\\beta} + \\boldsymbol{e} \\] We make it into a multilevel regression model by further partitioning the predictor space into between-person and within-person components. Note: this is the very same distinction that is made in traditional presentations of ANOVA when examining between-person factors and within-person factors. The general model becomes \\[ \\underbrace{\\boldsymbol{Y}_i}_{\\begin{subarray}{c}\\text{repeated measures}\\\\ \\text{for persion i}\\end{subarray}} = \\underbrace{\\boldsymbol{X}_i}_{\\begin{subarray}{c}\\text{known}\\\\ \\text{covariates}\\end{subarray}}\\underbrace{\\boldsymbol{\\beta}}_{\\begin{subarray}{c}\\text{fixed}\\\\ \\text{effects}\\end{subarray}} + \\underbrace{\\boldsymbol{Z}_i}_{\\begin{subarray}{c}\\text{known}\\\\ \\text{covariates}\\end{subarray}}\\underbrace{\\boldsymbol{u}_i}_{\\begin{subarray}{c}\\text{random}\\\\ \\text{effects}\\end{subarray}} + \\underbrace{\\boldsymbol{e}_i}_{\\begin{subarray}{c}\\text{residuals}\\end{subarray}} \\] which is also called the linear mixed model. "],["12.4-unconditional-means-model.html", "12.4 Unconditional Means Model", " 12.4 Unconditional Means Model It is often recommended to fit the unconditional means model before moving on to more complicated models. This is primarily because the unconditional means model does well at partitioning variance of the outcome across levels. We can use this model to better understand the amount of outcome variation that exists at the within and between levels of our model. If we fail to find sufficient variation at a given level there may be little reason to proceed with attempts to explain variance at that level of analysis. 12.4.1 Level 1 First, let us write out the level-1 (individual) model \\[ y_{it} = 1\\beta_{0i} + e_{it} \\] where \\(y_{it}\\) is the repeated measures score for individual \\(i\\) at time \\(t\\) \\(\\beta_{0i}\\) is the random intercept for individual \\(i\\) (person-specific mean) \\(e_{it}\\) is the time-specific residual score (within-person deviation) \\(e_{it} \\sim \\mathcal{N}(0,\\sigma^{2}_{e})\\) Note, the level 1 model shows us the true individual-level trajectories are completely flat, sitting at \\(\\beta_{0i}\\). 12.4.2 Level 2 The level-2 (sample) equation for the random intercept \\(\\beta_{0i}\\) can be written as \\[ \\beta_{0i} = 1\\gamma_{00} + u_{0i} \\] where \\(\\gamma_{00}\\) is the sample mean for the intercept (grand mean) \\(u_{0i}\\) is individual \\(i\\)’s deviation from the sample mean (between person deviation) \\(u_{0i} \\sim \\mathcal{N}(0,\\psi_{u0})\\) Note, the looking at the level 1 and level 2 model tells us that while these flat trajectories may differ in elevation, across everyone in the population, their average elevation is \\(\\gamma_{00}\\). 12.4.3 Single Equation We can write also both models in a single equation as follows \\[ y_{it} = (1\\gamma_{00} + 1u_{0i}) + e_{it} \\] where \\(y_{it}\\) is the repeated measures score for individual \\(i\\) at time \\(t\\) \\(\\gamma_{00}\\) is the sample mean for the intercept (grand mean) \\(u_{0i}\\) is individual \\(i\\)’s deviation from the sample mean (between person deviation) \\(u_{0i} \\sim \\mathcal{N}(0,\\psi_{u0})\\) \\(e_{it}\\) is the time-specific residual score (within-person deviation) \\(e_{it} \\sim \\mathcal{N}(0,\\sigma^{2}_{e})\\) 12.4.4 Model Elaboration 12.4.4.1 Within-Person Residual Covariance For clarity, Let’s write out the full variance covariance matrix of the within-person residuals (spanning across the \\(T = 3\\) repeated measures). Remember, we wrote \\((e_{it} \\sim \\mathcal{N}(0,\\sigma^{2}_{e}))\\), or in matrix notation, \\[ \\sigma^{2}_{e}\\boldsymbol{I} = \\sigma^{2}_{e} \\left[\\begin{array} {rrr} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{array}\\right] = \\left[\\begin{array} {rrr} \\sigma^{2}_{e} &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma^{2}_{e} &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma^{2}_{e} \\end{array}\\right] = \\boldsymbol{\\Lambda} \\] Note, this is the homoscedasticity of errors assumption. 12.4.4.2 Between-Person Residual Covariance We can now do the same for the full variance covariance matrix of the between-person residuals, \\[ \\left[\\begin{array} {r} \\psi_{u0} \\end{array}\\right] = u_{0i}u_{0i}&#39; \\] Note, in the unconditional means model there is no-growth, each individual has an intercept, but no change in scores is predicted because there are no predictors (e.g. time) in the level-1 equation. 12.4.5 Estimated Quantities Importantly, in the unconditional means model we will be interested in estimating three parameters: The sample-level mean of the random intercept (\\(\\gamma_{00}\\)) or the grand mean across all occasions and individuals. The variance of the random intercept (\\(\\psi_{u0}\\)) Provides information about the magnitude of between person differences in scores at each measurement occasion. The residual variance (\\(\\sigma^{2}_{e}\\)) Provides information about the magnitude of with-person fluctuations in scores over time. 12.4.6 More Notation It is also worth mentioning that through some replacement (e.g., replacing \\(u_{i}\\) = \\(b_{i}\\), the different vectors of 1 with \\(X_{i}\\) and \\(Z_{i}\\) = 1 we can get to … \\[ \\boldsymbol{Y} = \\boldsymbol{X}\\boldsymbol{\\beta} + \\boldsymbol{Z}\\boldsymbol{b} + \\boldsymbol{e} \\] which is just the more general notation often used in the statistics literature. The equation we started with, with another multilevel notation … \\[ \\boldsymbol{Y} = \\boldsymbol{X}\\boldsymbol{\\gamma} + \\boldsymbol{Z}\\boldsymbol{u} + \\boldsymbol{e} \\] 12.4.7 Unconditional Means Model in R We can write the unconditional means model in R as follows. um_fit &lt;- lme( fixed = verb ~ 1, random = ~ 1|id, data = verblong, na.action = na.exclude ) summary(um_fit) ## Linear mixed-effects model fit by REML ## Data: verblong ## AIC BIC logLik ## 4682.66 4695.906 -2338.33 ## ## Random effects: ## Formula: ~1 | id ## (Intercept) Residual ## StdDev: 4.406063 10.277 ## ## Fixed effects: verb ~ 1 ## Value Std.Error DF t-value p-value ## (Intercept) 33.92433 0.5174359 408 65.56238 0 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -1.9626206 -0.6924771 -0.1481960 0.5511044 3.1026127 ## ## Number of Observations: 612 ## Number of Groups: 204 # um_fit2 &lt;- lmer( # verb ~ 1 + (1|id), # data=verblong, # na.action = na.exclude # ) # summary(um_fit2) 12.4.7.1 Interpretation The single fixed effect in our unconditional means model is the grand mean, or \\(\\gamma_{00}\\). Rejection of the null indicates the average verbal score between Grades 2 and 6 is non-zero. Next we look at the random effects. The estimated between-person standard deviation is \\(\\psi_{u0}=4.4\\) and the estimated within-person standard deviation is \\(\\sigma^{2}_{e}=10.28\\). 12.4.8 Intra-Class Correlation The intra-class correlation (ICC) as the ratio of the random intercept variance (between-person) to the total variance, defined as the sum of the random intercept variance and residual variance (between + within). Specifically, \\[ICC_{between} = \\frac{\\sigma^{2}_{u0}}{\\sigma^{2}_{u0} + \\sigma^{2}_{e}}\\] 12.4.8.1 Calculating the ICC The ICC is the ratio of the random intercept variance (between-person var) over the total variance (between + within var): ICC_between &lt;- 4.4^2 / (4.4^2 + 10.28^2) ICC_between ## [1] 0.1548324 # Simple function for computing ICC from lme() output ICClme &lt;- function(out) { varests &lt;- as.numeric(VarCorr(out)[1:2]) return(paste(&quot;ICC =&quot;, varests[1]/sum(varests))) } ICClme(um_fit) ## [1] &quot;ICC = 0.155269768304537&quot; From the unconditional means model, the ICC was calculated, which indicated that of the total variance in verbal scores, approximately 16%, is attributable to between-person variation whereas 84% is attributable to within-person variation. This means there is a good portion of within-person variance sill to be modeled. 12.4.9 Model-Impled Moments What is the implied representation of the basic information? What are the model-implied moments? Let’s remember our original equation. \\[\\left[\\begin{array} {r} Y_{i0} \\\\ Y_{i1} \\\\ Y_{i2} \\end{array}\\right] = \\left[\\begin{array} {r} X_{0} \\\\ X_{1} \\\\ X_{2} \\end{array}\\right] \\left[\\begin{array} {r} \\beta_{0} \\end{array}\\right] + \\left[\\begin{array} {r} Z_{0} \\\\ Z_{1} \\\\ Z_{2} \\end{array}\\right] \\left[\\begin{array} {r} u_{0i} \\end{array}\\right] + \\left[\\begin{array} {r} e_{i0} \\\\ e_{i1} \\\\ e_{i2} \\end{array}\\right]\\] In this unconditional means model the \\(X\\) and \\(Z\\) design matrices are simply vectors of \\(1\\)s, leaving us with \\[\\left[\\begin{array} {r} Y_{i0} \\\\ Y_{i1} \\\\ Y_{i2} \\end{array}\\right] = \\left[\\begin{array} {r} 1 \\\\ 1 \\\\ 1 \\end{array}\\right] \\left[\\begin{array} {r} \\beta_{0} \\end{array}\\right] + \\left[\\begin{array} {r} 1 \\\\ 1 \\\\ 1 \\end{array}\\right] \\left[\\begin{array} {r} u_{0i} \\end{array}\\right] + \\left[\\begin{array} {r} e_{i0} \\\\ e_{i1} \\\\ e_{i2} \\end{array}\\right]\\]. 12.4.9.1 Mean Vector To obtain the model-implied mean vector we want \\(\\mathbb{E}(\\mathbf{Y})\\). Remember, from our covariance algebra \\[ \\mathbb{E}(\\mathbf{A}+\\mathbf{B}+\\mathbf{C}) = \\mathbb{E}(\\mathbf{A}) +\\mathbb{E}(\\mathbf{B})+\\mathbb{E}(\\mathbf{C}) \\] which gives us \\[\\begin{align} \\mathbb{E} \\left( \\left[\\begin{array} {r} Y_{i0} \\\\ Y_{i1} \\\\ Y_{i2} \\end{array}\\right] \\right) = &amp; \\mathbb{E}\\left( \\left[\\begin{array} {r} 1 \\\\ 1 \\\\ 1 \\end{array}\\right] \\left[\\begin{array} {r} \\beta_{0} \\end{array}\\right] \\right) + \\mathbb{E}\\left( \\left[\\begin{array} {r} 1 \\\\ 1 \\\\ 1 \\end{array}\\right] \\left[\\begin{array} {r} u_{0i} \\end{array}\\right] \\right) + \\mathbb{E}\\left( \\left[\\begin{array} {r} e_{i0} \\\\ e_{i1} \\\\ e_{i2} \\end{array}\\right] \\right) \\end{align}\\] or after simplifying \\[\\begin{align} \\mathbb{E} \\left( \\mathbf{Y} \\right) = &amp; \\mathbb{E}\\left( \\beta_{0}\\right) + \\mathbf{0} + \\mathbf{0} \\\\ \\end{align}\\] 12.4.9.2 Mean Vector in R Let’s make the implied mean vector in R. First, extract the fixed effects from the model using fixef(), specifically the contents of the \\(\\beta\\) matrix. fixef(um_fit) ## (Intercept) ## 33.92433 beta &lt;- matrix(fixef(um_fit)[1], nrow = 1, ncol = 1) beta ## [,1] ## [1,] 33.92433 Create the model design matrix for the fixed effects. In this model this is a matrix of order \\(3 \\times 1\\). X &lt;- matrix(c(1,1,1), nrow = 3, ncol = 1) X ## [,1] ## [1,] 1 ## [2,] 1 ## [3,] 1 Creating the model implied mean vector through multiplication \\[ \\mathbf{Y} = \\mathbf{X}\\mathbf{\\beta} + 0 + 0 \\] meanvector_um &lt;- X %*% beta meanvector_um ## [,1] ## [1,] 33.92433 ## [2,] 33.92433 ## [3,] 33.92433 Note this is the overall (grand) mean. 12.4.9.3 Model-Implied Covariance Matrix Now, let’s take a look at the model-implied variance-covariance matrix. Before we start let’s review the model again, \\[ \\boldsymbol{Y}_i = \\boldsymbol{X}_i\\boldsymbol{\\beta} + \\boldsymbol{Z}_i\\boldsymbol{u}_i + \\boldsymbol{e}_i \\] where \\(\\mathbf{Z}_i\\) is the random effects regressor (design) matrix; \\(\\boldsymbol{\\beta}\\) contains the fixed effects; \\(\\boldsymbol{u}_i\\) contains the random effects which are distributed normally with \\(0\\) mean and covariance matrix \\(\\mathbf{\\Psi}\\) and \\(\\boldsymbol{e}_i\\) are errors which are distributed normally with \\(0\\) mean and covariance matrix \\(\\mathbf{\\Lambda_{i}}\\), and our “standard assumption” was that \\(\\mathbf{\\Lambda_{i}} = \\mathbf{\\sigma^2}\\mathbf{I}\\) (homogeneity of errors). We’d like to identify the quantity \\(\\mathbb{C}ov(\\mathbf{Y})\\). We subtract the “means” from both sides … \\[\\left[\\begin{array} {r} Y_{i0} \\\\ Y_{i1} \\\\ Y_{i2} \\end{array}\\right] - \\left[\\begin{array} {r} X_{0} \\\\ X_{1} \\\\ X_{2} \\end{array}\\right] \\left[\\begin{array} {r} \\beta_{0} \\end{array}\\right] = \\left[\\begin{array} {r} Z_{0} \\\\ Z_{1} \\\\ Z_{2} \\end{array}\\right] \\left[\\begin{array} {r} u_{0i} \\end{array}\\right] + \\left[\\begin{array} {r} e_{i0} \\\\ e_{i1} \\\\ e_{i2} \\end{array}\\right]\\] So on the left side we now have de-meaned scores and on the right we have a between-portion part and a within-person part. Note that \\(\\mathbf{Y}^{*}\\) is now mean-centered, and as such, \\(\\mathbb{C}ov(\\mathbf{Y}) = \\mathbf{Y}^{*}\\mathbf{Y}^{*&#39;}\\). This gives us \\[\\begin{align} \\mathbf{Y}^{*} \\mathbf{Y}^{*&#39;} = (\\mathbf{Z}\\mu_{0i} + \\boldsymbol{\\epsilon})(\\mathbf{Z}\\mu_{0i} + \\boldsymbol{\\epsilon})^{&#39;} \\\\ = (\\mathbf{Z}\\mu_{0i} + \\boldsymbol{\\epsilon})(\\mu_{0i}^{&#39;}\\mathbf{Z}^{&#39;} + \\boldsymbol{\\epsilon}^{&#39;}) &amp; \\quad \\text{[Distribute transpose]}\\\\ = \\mathbf{Z}\\mu_{0i}\\mu_{0i}^{&#39;}\\mathbf{Z}^{&#39;} + \\mathbf{Z}\\mu_{0i}\\boldsymbol{\\epsilon}^{&#39;} + \\boldsymbol{\\epsilon}\\mu_{0i}^{&#39;}\\mathbf{Z}^{&#39;} + \\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^{&#39;} &amp; \\quad \\text{[Expand]}\\\\ = \\mathbf{Z}\\mu_{0i}\\mu_{0i}^{&#39;}\\mathbf{Z}^{&#39;} + \\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^{&#39;} &amp; \\quad \\text{[Orthogonality]}\\\\ = \\mathbf{Z}\\boldsymbol{\\Psi}\\mathbf{Z}^{&#39;} + \\boldsymbol{\\Lambda} &amp; \\quad \\text{[De. \\: Covariances]}\\\\ \\end{align}\\] Or we can alternatively see this in matrix form, without recalculating all the steps. \\[\\left[\\begin{array} {r} Y^*_{i0} \\\\ Y^*_{i1} \\\\ Y^*_{i2} \\end{array}\\right] = \\left[\\begin{array} {r} Z_{0} \\\\ Z_{1} \\\\ Z_{2} \\end{array}\\right] \\left[\\begin{array} {r} u_{0i} \\end{array}\\right] + \\left[\\begin{array} {r} e_{i0} \\\\ e_{i1} \\\\ e_{i2} \\end{array}\\right]\\] Let’s calculate var-cov matrix … \\[\\left[\\begin{array} {r} Y^*_{i0} \\\\ Y^*_{i1} \\\\ Y^*_{i2} \\end{array}\\right] \\left[\\begin{array} {r} Y^*_{i0} &amp; Y^*_{i1} &amp; Y^*_{i2} \\end{array}\\right] = \\left[\\begin{array} {r} Z_{0} \\\\ Z_{1} \\\\ Z_{2} \\end{array}\\right] \\left[\\begin{array} {r} u_{0i} \\end{array}\\right] \\left[\\begin{array} {r} u_{0i} \\end{array}\\right] \\left[\\begin{array} {r} Z_{0} &amp; Z_{1} &amp; Z_{2} \\end{array}\\right] + \\left[\\begin{array} {r} e_{i0} \\\\ e_{i1} \\\\ e_{i2} \\end{array}\\right] \\left[\\begin{array} {r} e_{i0} &amp; e_{i1} &amp; e_{i2} \\end{array}\\right]\\] From above we get \\[\\left[\\begin{array} {r} \\hat\\Sigma \\end{array}\\right] = \\left[\\begin{array} {r} Z_{0} \\\\ Z_{1} \\\\ Z_{2} \\end{array}\\right] \\left[\\begin{array} {r} \\Psi \\end{array}\\right] \\left[\\begin{array} {r} Z_{0} &amp; Z_{1} &amp; Z_{2} \\end{array}\\right] + \\left[\\begin{array} {r} \\sigma^2_{e} \\end{array}\\right] \\left[\\begin{array} {r} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{array}\\right]\\] 12.4.9.4 Covariance Matrix in R #parsing the model variances &amp; covariances VarCorr(um_fit) ## id = pdLogChol(1) ## Variance StdDev ## (Intercept) 19.41339 4.406063 ## Residual 105.61668 10.276997 So, in order to reconstruct the implied variance-covariances, we need to find \\(\\mathbf{\\Psi}\\) and \\(\\mathbf{\\sigma^2}\\) to create \\(\\boldsymbol{\\Lambda}\\), and do some multiplication. Parse the between-person variances from the model output. Psi &lt;- matrix(c(as.numeric(VarCorr(um_fit)[1])),nrow = 1, ncol = 1) Psi ## [,1] ## [1,] 19.41339 Create the model design matrix, \\(\\mathbf{Z}\\), for the random effects. In this model this is a matrix of order \\(3 \\times 1\\) Z &lt;- matrix(c(1,1,1), nrow=3,ncol=1) Z ## [,1] ## [1,] 1 ## [2,] 1 ## [3,] 1 So, the implied variance-covariance matrix of the between-person random effects for the three occasions is: Cov1 = Z %*% Psi %*% t(Z) Cov1 ## [,1] [,2] [,3] ## [1,] 19.41339 19.41339 19.41339 ## [2,] 19.41339 19.41339 19.41339 ## [3,] 19.41339 19.41339 19.41339 Which in correlation units implies Delta = diag(3) Delta[1,1] = 1/sqrt(Cov1[1,1]) Delta[2,2] = 1/sqrt(Cov1[2,2]) Delta[3,3] = 1/sqrt(Cov1[3,3]) Delta %*% Cov1 %*% Delta ## [,1] [,2] [,3] ## [1,] 1 1 1 ## [2,] 1 1 1 ## [3,] 1 1 1 Now, let’s look at the residual error variance-covariance matrix, sigma2 &lt;- as.numeric(VarCorr(um_fit)[2]) Lambda &lt;- sigma2 * diag(1, nrow = 3, ncol = 3) Finally, can put the between- and within- pieces together to calculate the model-implied variance-covariance matrix as follows varcovmatrix_um &lt;- Z %*% Psi %*% t(Z) + (sigma2 * Lambda) varcovmatrix_um ## [,1] [,2] [,3] ## [1,] 11174.29648 19.41339 19.41339 ## [2,] 19.41339 11174.29648 19.41339 ## [3,] 19.41339 19.41339 11174.29648 Note, we have a compound symmetry structure in our model-implied covariance matrix. Together with the implied mean vector, we have the entire picture provided by all the model components. 12.4.10 Model Residuals Recall what the observed mean and var-cov were … meanvector ## verb2 verb4 verb6 ## 25.41534 32.60775 43.74990 varcovmatrix ## verb2 verb4 verb6 ## verb2 37.28784 33.81957 47.40488 ## verb4 33.81957 53.58070 62.25489 ## verb6 47.40488 62.25489 113.74332 For fun, let’s look at the misfit to the data (observed matrix - model implied matrix) meanvector - meanvector_um ## [,1] ## [1,] -8.508987 ## [2,] -1.316585 ## [3,] 9.825572 varcovmatrix - varcovmatrix_um ## verb2 verb4 verb6 ## verb2 -11137.00864 14.40618 27.99149 ## verb4 14.40618 -11120.71578 42.84150 ## verb6 27.99149 42.84150 -11060.55317 Fit is not so good. Let’s visualize the implied model. #Calculating predicted scores from the models verblong$pred_um &lt;- predict(um_fit) #Making the prototype from the implied means proto_um &lt;- data.frame(cbind(c(1000,1000,1000),c(0,1,2),meanvector_um)) names(proto_um) &lt;- c(&quot;id&quot;,&quot;time0&quot;,&quot;pred_um&quot;) #plotting implied individual scores ggplot(data = verblong, aes(x = time0, y = pred_um, group = id)) + ggtitle(&quot;Unconditional Means Model&quot;) + geom_point() + geom_line() + geom_point(data=proto_um, color=&quot;red&quot;, size=2) + geom_line(data=proto_um, color=&quot;red&quot;, size=1) + xlab(&quot;Time&quot;) + ylab(&quot;WISC Verbal Score&quot;) + ylim(0,100) + xlim(0,2) ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. "],["12.5-repeated-measures-anova.html", "12.5 Repeated Measures ANOVA", " 12.5 Repeated Measures ANOVA OK, now let’s move to an repeated measures (RM) ANOVA by adding in effects for categorical time. Recall, we must tell R that time0 is a categorical variable using factor() verblong$time0 &lt;- factor(verblong$time0, ordered=FALSE) str(verblong$time0) ## Factor w/ 3 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;: 1 2 3 1 2 3 1 2 3 1 ... Here is our RM ANOVA model with time as within-person factor timecat_fit &lt;- lme( fixed = verb ~ 1 + time0, random = ~ 1|id, data = verblong, na.action = na.exclude) summary(timecat_fit) ## Linear mixed-effects model fit by REML ## Data: verblong ## AIC BIC logLik ## 4013.176 4035.235 -2001.588 ## ## Random effects: ## Formula: ~1 | id ## (Intercept) Residual ## StdDev: 6.915667 4.514145 ## ## Fixed effects: verb ~ 1 + time0 ## Value Std.Error DF t-value p-value ## (Intercept) 25.415343 0.5782155 406 43.95480 0 ## time01 7.192402 0.4469670 406 16.09157 0 ## time02 18.334559 0.4469670 406 41.01994 0 ## Correlation: ## (Intr) time01 ## time01 -0.387 ## time02 -0.387 0.500 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -2.61263084 -0.54466998 -0.02451831 0.48627468 3.50536334 ## ## Number of Observations: 612 ## Number of Groups: 204 # timecat_fit2 &lt;- lmer(verb ~ 1 + time0 + (1|id), # data=verblong, # na.action = na.exclude) # summary(timecat_fit2) Remember that interpretation is with respect to time0, with the first category set as default intercept which is str(verblong$time0) ## Factor w/ 3 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;: 1 2 3 1 2 3 1 2 3 1 ... 12.5.1 Intra-Class Correlation The intra-class correlation (ICC) as the ratio of the random intercept variance (between-person) to the total variance, defined as the sum of the random intercept variance and residual variance (between + within). Specifically, \\[ICC_{between} = \\frac{\\sigma^{2}_{u0}}{\\sigma^{2}_{u0} + \\sigma^{2}_{e}}\\] 12.5.1.1 Calculating the ICC The ICC is the ratio of the random intercept variance (between-person var) over the total variance (between + within var): # Simple function for computing ICC from lme() output ICClme &lt;- function(out) { varests &lt;- as.numeric(VarCorr(out)[1:2]) return(paste(&quot;ICC =&quot;, varests[1]/sum(varests))) } ICClme(timecat_fit) ## [1] &quot;ICC = 0.701226878908497&quot; From the current model, the ICC was calculated, which indicated that of the total variance in verbal scores, approximately 70%, is attributable to between-person variation whereas 30% is attributable to within-person variation. 12.5.2 Model-Implied Mean Vector Let’s be explicit with our model \\[\\left[\\begin{array} {r} Y_{i0} \\\\ Y_{i1} \\\\ Y_{i2} \\end{array}\\right] = \\left[\\begin{array} {r} 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 \\end{array}\\right] \\left[\\begin{array} {r} \\beta_{0} \\\\ \\beta_{1} \\\\ \\beta_{2} \\end{array}\\right] + \\left[\\begin{array} {r} 1 \\\\ 1 \\\\ 1 \\end{array}\\right] \\left[\\begin{array} {r} u_{0i} \\end{array}\\right] + \\left[\\begin{array} {r} e_{i0} \\\\ e_{i1} \\\\ e_{i2} \\end{array}\\right]\\]. Making the implied mean vector fixef(timecat_fit) ## (Intercept) time01 time02 ## 25.415343 7.192402 18.334559 beta &lt;- matrix( c( fixef(timecat_fit)[1], fixef(timecat_fit)[2], fixef(timecat_fit)[3] ), nrow =3, ncol=1) beta ## [,1] ## [1,] 25.415343 ## [2,] 7.192402 ## [3,] 18.334559 Create the model design matrix for the fixed effects. In this model this is a matrix of order \\(3 \\times 3\\). X &lt;- matrix(c(1,1,1,0,1,0,0,0,1), nrow=3, ncol=3) X ## [,1] [,2] [,3] ## [1,] 1 0 0 ## [2,] 1 1 0 ## [3,] 1 0 1 Creating the model implied mean vector through multiplication \\[ \\mathbf{Y} = \\mathbf{X}\\mathbf{\\beta} + 0 + \\] meanvector_timecat &lt;- X %*% beta meanvector_timecat ## [,1] ## [1,] 25.41534 ## [2,] 32.60775 ## [3,] 43.74990 See the differences in the means across levels of time0. 12.5.3 Model-Implied Covariance Matrix Making the implied variance-covariance matrix. VarCorr(timecat_fit) ## id = pdLogChol(1) ## Variance StdDev ## (Intercept) 47.82645 6.915667 ## Residual 20.37751 4.514145 From this, we need to create the model implied variance-covariance. Parse the between-person variances from the model output. Psi &lt;- matrix(c(as.numeric(VarCorr(timecat_fit)[1])), nrow=1,ncol=1) Psi ## [,1] ## [1,] 47.82645 Create the model design matrix for the random effects. In this model this is a matrix of order \\(3 \\times 1\\) Z &lt;- matrix(c(1,1,1), nrow=3,ncol=1) Z ## [,1] ## [1,] 1 ## [2,] 1 ## [3,] 1 So, the implied variance-covariance matrix of the between-person random effects for the three occasions is: Z %*% Psi %*% t(Z) ## [,1] [,2] [,3] ## [1,] 47.82645 47.82645 47.82645 ## [2,] 47.82645 47.82645 47.82645 ## [3,] 47.82645 47.82645 47.82645 Next, we parse the residual/“error” variance-covariance. sigma2 &lt;- as.numeric(VarCorr(timecat_fit)[2]) Lambda &lt;- sigma2 * diag(1,nrow=3,ncol=3) So the residual within-person residual/“error” structure Lambda ## [,1] [,2] [,3] ## [1,] 20.37751 0.00000 0.00000 ## [2,] 0.00000 20.37751 0.00000 ## [3,] 0.00000 0.00000 20.37751 As before, we have homogeneity and uncorrelated errors. Finally, calculate the implied variance-covariances of total model varcovmatrix_timecat &lt;- Z %*% Psi %*% t(Z) + Lambda varcovmatrix_timecat ## [,1] [,2] [,3] ## [1,] 68.20396 47.82645 47.82645 ## [2,] 47.82645 68.20396 47.82645 ## [3,] 47.82645 47.82645 68.20396 Again, notice the compound symmetry structure. For fun, let’s look at the misfit (real matrix - model implied) #misfit of means meanvector - meanvector_timecat ## [,1] ## [1,] 4.618528e-14 ## [2,] -9.947598e-14 ## [3,] 1.421085e-14 #misfit of var-cov varcovmatrix - varcovmatrix_timecat ## verb2 verb4 verb6 ## verb2 -30.9161173 -14.00688 -0.4215677 ## verb4 -14.0068803 -14.62326 14.4284384 ## verb6 -0.4215677 14.42844 45.5393553 The means are now perfectly reproduced, however, the variances and covariances are not so good, particularly the variances Let’s make a picture of the implied model. #Calculating predicted scores from the models verblong$pred_timecat &lt;- predict(timecat_fit) #Making the prototype from the implied means proto_timecat &lt;- data.frame(cbind(c(1000,1000,1000),c(0,1,2),meanvector_timecat)) names(proto_timecat) &lt;- c(&quot;id&quot;,&quot;time0&quot;,&quot;pred_timecat&quot;) #need to convert time0 back into a continuous variable for plotting as intraindividual change verblong$time0 &lt;- as.numeric(unclass(verblong$time0))-1 #plotting implied individual scores ggplot(data = verblong, aes(x = time0, y = pred_timecat, group = id)) + ggtitle(&quot;RM ANOVA Model (time categorical)&quot;) + geom_point() + geom_line() + geom_point(data=proto_timecat, color=&quot;red&quot;, size=2) + geom_line(data=proto_timecat, color=&quot;red&quot;, size=1, linetype=2) + xlab(&quot;Time&quot;) + ylab(&quot;WISC Verbal Score&quot;) + ylim(0,100) + xlim(0,2) + theme_classic() Notice that the implied lines are all parallel. This is a model of mean differences. Can also see what the residuals look like. #Calculating residual scores from the models verblong$resid_timecat &lt;- residuals(timecat_fit) #plotting implied individual scores ggplot(data = verblong, aes(x = time0, y = resid_timecat, group = id)) + ggtitle(&quot;RM ANOVA Model Residuals (time categorical)&quot;) + geom_point() + xlab(&quot;Time&quot;) + ylab(&quot;WISC Verbal Score&quot;) + xlim(0,2) + theme_classic() Note, that the points are not connected - this is to highlight the model and the implication that the within-person residuals are from the occasion-specific mean (NOT from a trajectory).Note also the heteroskedasticity of the residuals. "],["12.6-repeated-measures-manova.html", "12.6 Repeated Measures MANOVA", " 12.6 Repeated Measures MANOVA Now let’s adjust the RM ANOVA error structure to get a RM MANOVA. The multivariate part of the MANOVA, has to do with relaxing the assumption on the error structure - this means more flexibility than compound symmetry. Here is our RM ANOVA model with time as within-person factor. The error structure is compound symmetry with heterogeneous variances. verblong$time0 &lt;- factor(verblong$time0, ordered=FALSE) timecathet_fit &lt;- lme( fixed= verb ~ 1 + time0, random= ~ 1|id, weights=varIdent(form=~1|time0), data=verblong, na.action = na.exclude ) summary(timecathet_fit) ## Linear mixed-effects model fit by REML ## Data: verblong ## AIC BIC logLik ## 3930.253 3961.136 -1958.127 ## ## Random effects: ## Formula: ~1 | id ## (Intercept) Residual ## StdDev: 6.086682 2.871709 ## ## Variance function: ## Structure: Different standard deviations per stratum ## Formula: ~1 | time0 ## Parameter estimates: ## 0 1 2 ## 1.000000 1.254186 2.341456 ## Fixed effects: verb ~ 1 + time0 ## Value Std.Error DF t-value p-value ## (Intercept) 25.415343 0.4712021 406 53.93724 0 ## time01 7.192402 0.3225105 406 22.30130 0 ## time02 18.334559 0.5119102 406 35.81597 0 ## Correlation: ## (Intr) time01 ## time01 -0.266 ## time02 -0.168 0.245 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -2.24995336 -0.55415412 -0.05694858 0.51324627 2.99087738 ## ## Number of Observations: 612 ## Number of Groups: 204 12.6.1 Model-Implied Mean Vector So what is the implied representation of the basic information Making the implied mean vector. fixef(timecathet_fit) ## (Intercept) time01 time02 ## 25.415343 7.192402 18.334559 beta &lt;- matrix( c( fixef(timecathet_fit)[1], fixef(timecathet_fit)[2], fixef(timecathet_fit)[3] ), nrow=3, ncol=1) beta ## [,1] ## [1,] 25.415343 ## [2,] 7.192402 ## [3,] 18.334559 Create the model design matrix, X, for the fixed effects. In this model this is a matrix of order \\(3 \\times 3\\). X &lt;- matrix(c(1,1,1,0,1,0,0,0,1), nrow=3,ncol=3) X ## [,1] [,2] [,3] ## [1,] 1 0 0 ## [2,] 1 1 0 ## [3,] 1 0 1 Creating the model implied mean vector through multiplication \\[ \\mathbf{Y} = \\mathbf{X}\\mathbf{\\beta} + 0 + 0 \\] meanvector_timecathet &lt;- X %*% beta meanvector_timecathet ## [,1] ## [1,] 25.41534 ## [2,] 32.60775 ## [3,] 43.74990 See the differences in the means across levels of time0. This is exactly the same as in the last model 12.6.2 Model-Implied Covariance Matrix #parse the between-person variances Psi &lt;- matrix(c(as.numeric(VarCorr(timecathet_fit)[1])),nrow=1,ncol=1) Psi ## [,1] ## [1,] 37.0477 #create the model design matrix Z &lt;- matrix(c(1,1,1), nrow=3,ncol=1) Z ## [,1] ## [1,] 1 ## [2,] 1 ## [3,] 1 So the implied variance covariance of the between-person random effects for the three repeated measures is: Z %*% Psi %*% t(Z) ## [,1] [,2] [,3] ## [1,] 37.0477 37.0477 37.0477 ## [2,] 37.0477 37.0477 37.0477 ## [3,] 37.0477 37.0477 37.0477 Now for the within-person residual var-cov. #parse the residual/&quot;error&quot; variance-covariance sigma &lt;- as.numeric(VarCorr(timecathet_fit)[4]) #note this is standard deviation sigma ## [1] 2.871709 Note: Now we have another step due to the heterogeneous variances. We have to get the heterogeneous weights for the residual standard deviation from the summary(timecathet_fit). # Variance function: # Structure: Different standard deviations per stratum # Formula: ~1 | time0 # Parameter estimates: # 0 1 2 # 1.000000 1.254186 2.341456 Pulling from the output. sigmahet &lt;- sigma * (diag(c(1.000000, 1.254186, 2.341456), nrow=3,ncol=3)) #Calculate the implied residual error variance-covariance Lambda &lt;- sigmahet^2 Finally calculating the model implied between- + within var-cov structure. varcovmatrix_timecathet &lt;- Z %*% Psi %*% t(Z) + Lambda varcovmatrix_timecathet ## [,1] [,2] [,3] ## [1,] 45.29441 37.04770 37.04770 ## [2,] 37.04770 50.01964 37.04770 ## [3,] 37.04770 37.04770 82.25961 Note the heterogeneity we now have along the diagonal. Again, Let’s look at the misfit (real matrix - model implied) meanvector - meanvector_timecathet ## [,1] ## [1,] -5.329071e-14 ## [2,] -4.973799e-14 ## [3,] 1.136868e-13 varcovmatrix - varcovmatrix_timecathet ## verb2 verb4 verb6 ## verb2 -8.006572 -3.228132 10.35718 ## verb4 -3.228132 3.561067 25.20719 ## verb6 10.357180 25.207186 31.48370 Getting better. Note, we can formally compare the models. anova(timecat_fit, timecathet_fit) ## Model df AIC BIC logLik Test L.Ratio p-value ## timecat_fit 1 5 4013.176 4035.235 -2001.588 ## timecathet_fit 2 7 3930.253 3961.136 -1958.127 1 vs 2 86.92304 &lt;.0001 Remember, the null hypothesis of the LRT states that the more constrained model provides as good a fit for the data as the less constrained model. If the null hypothesis is rejected, then the alternative, unconstrained model provides a significant improvement in fit over the smaller model. Thus, allowing for the additional heterogeneity improved our model. "],["12.7-repeated-measures-manova-unstructured.html", "12.7 Repeated Measures MANOVA (Unstructured)", " 12.7 Repeated Measures MANOVA (Unstructured) Let’s adjust the RM MANOVA error structure to have no constraints. To do this in lme() we can use the correlation argument. We can now use the weights argument to indicate that the residual variances will be a function of time. Option correlation = corSymm(form=~1|id) specifies that the correlation structure is unstructured. verblong$time0 &lt;- factor(verblong$time0, ordered=FALSE) timecatunst_fit &lt;- lme( fixed= verb ~ 1 + time0, random= ~ 1|id, weights=varIdent(form=~1|time0), correlation=corSymm(form=~1|id), data=verblong, na.action = na.exclude ) summary(timecatunst_fit) ## Linear mixed-effects model fit by REML ## Data: verblong ## AIC BIC logLik ## 3869.06 3913.178 -1924.53 ## ## Random effects: ## Formula: ~1 | id ## (Intercept) Residual ## StdDev: 5.506716 2.638933 ## ## Correlation Structure: General ## Formula: ~1 | id ## Parameter estimate(s): ## Correlation: ## 1 2 ## 2 0.275 ## 3 0.709 0.725 ## Variance function: ## Structure: Different standard deviations per stratum ## Formula: ~1 | time0 ## Parameter estimates: ## 0 1 2 ## 1.000000 1.827454 3.461032 ## Fixed effects: verb ~ 1 + time0 ## Value Std.Error DF t-value p-value ## (Intercept) 25.415343 0.4275322 406 59.44662 0 ## time01 7.192402 0.3374453 406 21.31428 0 ## time02 18.334559 0.5249720 406 34.92483 0 ## Correlation: ## (Intr) time01 ## time01 -0.118 ## time02 0.221 0.507 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -2.24169379 -0.62481084 -0.06273466 0.63549118 3.06610136 ## ## Number of Observations: 612 ## Number of Groups: 204 12.7.1 Model-Implied Mean Vector There will be no differences in the model implied mean vector in this model. beta &lt;- matrix(fixef(timecatunst_fit), nrow=3, ncol=1) X &lt;- matrix(c(1,1,1,0,1,0,0,0,1), nrow=3,ncol=3) meanvector_timecatunst &lt;- X %*% beta meanvector_timecatunst ## [,1] ## [1,] 25.41534 ## [2,] 32.60775 ## [3,] 43.74990 12.7.2 Model-Implied Covariance Matrix Again, we start with the between-part of our model. #parsing the model variances &amp; covariances VarCorr(timecatunst_fit) ## id = pdLogChol(1) ## Variance StdDev ## (Intercept) 30.323922 5.506716 ## Residual 6.963968 2.638933 Psi &lt;- matrix(c(as.numeric(VarCorr(timecatunst_fit)[1])),nrow=1,ncol=1) Z &lt;- matrix(c(1,1,1), nrow=3,ncol=1) Z %*% Psi %*% t(Z) ## [,1] [,2] [,3] ## [1,] 30.32392 30.32392 30.32392 ## [2,] 30.32392 30.32392 30.32392 ## [3,] 30.32392 30.32392 30.32392 Now it gets a bit more complicated, because we have a fully unstructured matrix for the within-person residual variance-covariance. #parse the residual/&quot;error&quot; variance-covariance sigma &lt;- as.numeric(VarCorr(timecatunst_fit)[4]) #note this is standard deviation #From the summary(timecatunst_fit) above # Variance function: # Structure: Different standard deviations per stratum # Formula: ~1 | time0 # Parameter estimates: # 0 1 2 # 1.000000 1.827573 3.461330 sigmaunst &lt;- sigma * (diag(c(1.000000, 1.827573, 3.461330), nrow=3,ncol=3)) #From the summary(timecatunst_fit) above # Correlation Structure: General # Formula: ~1 | id # Parameter estimate(s): # Correlation: # 1 2 # 2 0.275 # 3 0.709 0.725 cormatrixunst &lt;- matrix(c(1.000, 0.275, 0.709, 0.275, 1.000, 0.725, 0.709, 0.725, 1.000), nrow=3,ncol=3) #Pre and post multiply by SDs to convert Correlation matrix into #Covariance matrix covresidunst &lt;- sigmaunst %*% cormatrixunst %*% t(sigmaunst) covresidunst ## [,1] [,2] [,3] ## [1,] 6.963967 3.499969 17.09015 ## [2,] 3.499969 23.259812 31.93835 ## [3,] 17.090154 31.938350 83.43394 Finally, calculate the implied between- + within-person variance-covariances varcovmatrix_timecatunst &lt;- Z %*% Psi %*% t(Z) + covresidunst varcovmatrix_timecatunst ## [,1] [,2] [,3] ## [1,] 37.28789 33.82389 47.41408 ## [2,] 33.82389 53.58373 62.26227 ## [3,] 47.41408 62.26227 113.75786 Let’s look at the misfit (real matrix - model implied). meanvector - meanvector_timecatunst ## [,1] ## [1,] 1.421085e-13 ## [2,] 2.629008e-13 ## [3,] 2.557954e-13 varcovmatrix - varcovmatrix_timecatunst ## verb2 verb4 verb6 ## verb2 -4.663967e-05 -0.004320914 -0.009193447 ## verb4 -4.320914e-03 -0.003030440 -0.007383449 ## verb6 -9.193447e-03 -0.007383449 -0.014544496 It appears we have fully reproduced our observed data. We can formally test the improvement of this model using the anove() function. anova(timecathet_fit, timecatunst_fit) ## Model df AIC BIC logLik Test L.Ratio p-value ## timecathet_fit 1 7 3930.253 3961.136 -1958.127 ## timecatunst_fit 2 10 3869.060 3913.178 -1924.530 1 vs 2 67.19366 &lt;.0001 12.7.3 References "]]
