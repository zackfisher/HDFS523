[["6-chapter-6.html", "Chapter 6 Linear Regression", " Chapter 6 Linear Regression In Chapter 6 we will demonstrate how to estimate the linear regression model in R with an eye towards the longitudinal modeling to follow. "],["6.1-example-data-2.html", "6.1 Example Data", " 6.1 Example Data Chapter 6 make use of the same WISC data used in Chapter 3. Here we again read in, subset, and provide descriptives for the WISC data. We will also add a simulated variable childgrad indicating whether the student graduated highschool. filepath &lt;- &quot;https://quantdev.ssri.psu.edu/sites/qdev/files/wisc3raw.csv&quot; wisc3raw &lt;- read.csv(file=url(filepath),header=TRUE) colnames(wisc3raw) &lt;- c( &quot;id&quot;, &quot;verb1&quot;, &quot;verb2&quot;, &quot;verb4&quot;, &quot;verb6&quot;, &quot;perfo1&quot;, &quot;perfo2&quot;, &quot;perfo4&quot;, &quot;perfo6&quot;, &quot;info1&quot;, &quot;comp1&quot;, &quot;simu1&quot;, &quot;voca1&quot;, &quot;info6&quot;, &quot;comp6&quot;, &quot;simu6&quot;, &quot;voca6&quot;, &quot;daded&quot;, &quot;grad&quot;, &quot;constant&quot; ) var_names_sub &lt;- c( &quot;id&quot;, &quot;verb1&quot;, &quot;verb2&quot;, &quot;verb4&quot;, &quot;verb6&quot;, &quot;perfo1&quot;, &quot;perfo2&quot;, &quot;perfo4&quot;, &quot;perfo6&quot;, &quot;daded&quot;, &quot;grad&quot; ) wiscsub &lt;- wisc3raw[,var_names_sub] set.seed(1234) wiscsub$childgrad &lt;- sample(c(0,1), replace=TRUE, size=nrow(wiscsub)) psych::describe(wiscsub) ## vars n mean sd median trimmed mad min max range skew ## id 1 204 102.50 59.03 102.50 102.50 75.61 1.00 204.00 203.00 0.00 ## verb1 2 204 19.59 5.81 19.34 19.50 5.41 3.33 35.15 31.82 0.13 ## verb2 3 204 25.42 6.11 25.98 25.40 6.57 5.95 39.85 33.90 -0.06 ## verb4 4 204 32.61 7.32 32.82 32.42 7.18 12.60 52.84 40.24 0.23 ## verb6 5 204 43.75 10.67 42.55 43.46 11.30 17.35 72.59 55.24 0.24 ## perfo1 6 204 17.98 8.35 17.66 17.69 8.30 0.00 46.58 46.58 0.35 ## perfo2 7 204 27.69 9.99 26.57 27.34 10.51 7.83 59.58 51.75 0.39 ## perfo4 8 204 39.36 10.27 39.09 39.28 10.04 7.81 75.61 67.80 0.15 ## perfo6 9 204 50.93 12.48 51.76 51.07 13.27 10.26 89.01 78.75 -0.06 ## daded 10 204 10.81 2.70 11.50 11.00 2.97 5.50 18.00 12.50 -0.36 ## grad 11 204 0.23 0.42 0.00 0.16 0.00 0.00 1.00 1.00 1.30 ## childgrad 12 204 0.55 0.50 1.00 0.56 0.00 0.00 1.00 1.00 -0.20 ## kurtosis se ## id -1.22 4.13 ## verb1 -0.05 0.41 ## verb2 -0.34 0.43 ## verb4 -0.08 0.51 ## verb6 -0.36 0.75 ## perfo1 -0.11 0.58 ## perfo2 -0.21 0.70 ## perfo4 0.59 0.72 ## perfo6 0.18 0.87 ## daded 0.01 0.19 ## grad -0.30 0.03 ## childgrad -1.97 0.03 "],["6.2-intercept-only-model.html", "6.2 Intercept-Only Model", " 6.2 Intercept-Only Model For our first example, we focus on verbal ability at Grade 2 as an outcome (verb2 in the data frame wiscsub). Examining the distribution for ‘verb2’. library(&quot;ggplot2&quot;) psych::describe(wiscsub$verb2) ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 204 25.42 6.11 25.98 25.4 6.57 5.95 39.85 33.9 -0.06 -0.34 0.43 ggplot(data=wiscsub, aes(x=verb2)) + geom_histogram(binwidth=2.5, fill=&quot;white&quot;, color=&quot;black&quot;, boundary=0) + xlab(&quot;Verbal Ability Grade 2&quot;) + ylab(&quot;Count&quot;) + xlim(0,50) + theme_classic() 6.2.1 Intercept-Only Equation The simplest model is an intercept only model. In this case, we would fit the model \\[ verb_{2i} = b_0 + \\epsilon_{i}\\] Written out explicitly with the “silent” 1 in it, we get \\[ verb_{2i} = b_01_i + \\epsilon_{i}\\] This is helpful for explicit translation into the R code, specifically the formula within the lm() function. 6.2.2 Intercept-Only Model in R We fit the model using the following code. Note that the code has the ‘1’ predictor variable stated explicitly. model1 &lt;- lm(formula = verb2 ~ 1, data = wiscsub, na.action = na.exclude) summary(model1) ## ## Call: ## lm(formula = verb2 ~ 1, data = wiscsub, na.action = na.exclude) ## ## Residuals: ## Min 1Q Median 3Q Max ## -19.4653 -4.6403 0.5647 4.2822 14.4347 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 25.4153 0.4275 59.45 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.106 on 203 degrees of freedom Note that we used na.exclude instead of na.omit (default); practically speaking na.omit deletes missing data entries while na.exclude just excludes from the analysis.Therefore with na.exclude, in the residuals and fitted values, NA will show up where there were missing values. The output indicates that \\(b_0\\) = 25.4153, and its standard error = 0.4275. The intercept reflects the expected value of the outcome variable when all of the predictor variables (i.e. \\(\\left\\{ x_{1i}, ..., x_{qi}\\right\\}\\)) = 0. So, in the absence of any additional information other than the descriptive statistics of \\(verb_{2i}\\), what is our best guess for a person’s \\(verb_{2i}\\) score? It is the mean of \\(verb_{2i}\\). The regression above confirms this notion; regressing the outcome on a vector of 1s allows us to ‘recover’ the mean. 6.2.3 Intercept as Mean of Outcome Notice we can confirm this finding using matrix algebra, as well. \\[ \\mathbb{E}(verb_{2i}) = \\mathbb{E}(b_01_i + \\epsilon_{i}) \\] From the properties of expectation, we have \\(\\mathbb{E}(X+Y)=\\mathbb{E}(X) + \\mathbb{E}(Y)\\). \\[ \\mathbb{E}(verb_{2i}) = \\mathbb{E}(b_01_i) + \\mathbb{E}(\\epsilon_{i}) \\] Another property of expectation relates to taking the expectation of a constant, \\(\\mathbb{E}(c)=c\\), thus \\(\\mathbb{E}(b_01_i)=b_0\\), implying \\[ \\mathbb{E}(verb_{2i}) = b_01_i + \\mathbb{E}(\\epsilon_{i}). \\] Remembering Assumption 1, \\(\\mathbb{E}(\\epsilon_{i})=0\\), we have \\[ \\mathbb{E}(verb_{2i}) = b_0 \\] We can confirm this by looking at Verbal Scores at Wave 2. mean(wiscsub$verb2) ## [1] 25.41534 6.2.4 Intercept-Only Model \\(R^2\\) Yes - we recovered the mean, but we did not attempt to explain any of the variance. Let’s take a look at the variance explained for Model 1. summary(model1)$r.squared ## [1] 0 It thus makes sense that we get 0 as the R-square. From the properties of variance, we know that \\(\\mathbb{V}(c)=0\\). There is no variability due to the regression model because there are no predictors, only a constant. "],["6.3-simple-linear-regression.html", "6.3 Simple Linear Regression", " 6.3 Simple Linear Regression Let’s build up the model further. For example, we could attempt to explain some of the between-person variance in the Grade 2 verbal score from the Grade 1 verbal scores. But, before we do, let’s examine the distribution of the between-person differences in the Grade 1 verbal scores. ggplot(wiscsub, aes(x=verb1)) + geom_histogram(binwidth=2.5, fill=&quot;white&quot;, color=&quot;black&quot;, boundary=0) + xlab(&quot;Verbal Ability Grade 1&quot;) + ylab(&quot;Count&quot;) + xlim(0,50) + theme_classic() And the relation between the Grade 2 and Grade 1 verbal ability scores. ggplot(wiscsub, aes(x=verb1, y = verb2)) + geom_point() + stat_ellipse(color=&quot;blue&quot;, alpha=.7) + xlab(&quot;Verbal Ability Grade 1&quot;) + ylab(&quot;Verbal Ability Grade 2&quot;) + ylim(0,45) + xlim(0,45) + theme_classic() 6.3.1 Regression Equation and Model Fitting Our regression model becomes \\[ verb_{2i} = b_01_i + b_1verb_{1i} + \\epsilon_{i}\\] model2 &lt;- lm(verb2 ~ 1 + verb1, data = wiscsub, na.action = na.exclude) summary(model2) ## ## Call: ## lm(formula = verb2 ~ 1 + verb1, data = wiscsub, na.action = na.exclude) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.5305 -3.0362 0.2526 2.7147 12.5020 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.62965 1.05164 10.11 &lt;2e-16 *** ## verb1 0.75495 0.05149 14.66 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.261 on 202 degrees of freedom ## Multiple R-squared: 0.5156, Adjusted R-squared: 0.5132 ## F-statistic: 215 on 1 and 202 DF, p-value: &lt; 2.2e-16 6.3.2 Path Diagram We might also be interested in a graphical depiction of our model. This can be accomplished with the semPaths package. semPlot::semPaths(model2, what = &quot;paths&quot;) 6.3.3 Interpreting Model Parameters How do we interpret the parameters here? The intercept, \\(b_0\\), is the expected value for the outcome variable when all of the predictor variables equal zero. So, we would expect a child to have a Grade 2 verbal score of 10.62965 if they have a Grade 1 verbal score of 0. The slope, \\(b_1\\) is the expected difference in the outcome variable for each 1-unit difference in the predictor variable. So, across children, for each 1-point difference in a child’s Grade 1 verbal score, we would expect a 0.75 point difference in the Grade 2 verbal score. 6.3.4 Plotting Regression Line We can plot the relation between ‘verb1’ and ‘verb2’, and include the predicted line from the analysis. ggplot(data=wiscsub, aes(x=verb1,y=verb2)) + geom_point(size = 2, shape=19) + geom_smooth(method=lm,se=TRUE,fullrange=TRUE,colour=&quot;red&quot;, size=2) + labs(x= &quot;Verbal Ability Grade 1&quot;, y= &quot;Verbal Ability Grade 2&quot;) + xlim(0,50) + ylim(0,50) + theme_bw() + theme( plot.background = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.border = element_blank() ) + #draws x and y axis line theme(axis.line = element_line(color = &#39;black&#39;)) + #set size of axis labels and titles theme(axis.text = element_text(size=12), axis.title = element_text(size=14)) ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. ## `geom_smooth()` using formula = &#39;y ~ x&#39; "],["6.4-mean-centering-predictors.html", "6.4 Mean Centering Predictors", " 6.4 Mean Centering Predictors In this case, and in many other cases, the intercept does not have a ‘useful’ interpretation for the empirical example. This is because no students had a Grade 1 verbal score equal to 0. Therefore, if we want to make the intercept more meaningful, we need to make a Grade 1 verbal score with a more meaningful 0 point. Typically we center the predictor variables in regression analysis. For example, we create a centered variable, \\(x^{*}_{1i}\\) by subtracting the sample mean, \\(\\bar{x_1}\\) from each observation, \\[ x^{*}_{1i} = x_{1i} - \\bar{x_1} \\] Our model becomes \\[ y_i = b_0(1_i) + b_1(x^{*}_{1i}) + \\epsilon_i \\] We can sample-mean center \\(verb_{1i}\\) in R as follows #calculate the mean centered variable wiscsub$verb1_star &lt;- wiscsub$verb1 - mean(wiscsub$verb1, na.rm = TRUE) Then we can fit a new model using \\(verb^{*}_{1i}\\), such that \\[ verb_{2i} = b_0(1_i) + b_1(verb^{*}_{1i}) + \\epsilon_i \\] model3 &lt;- lm(verb2 ~ 1 + verb1_star, data = wiscsub, na.action = na.exclude) summary(model3) ## ## Call: ## lm(formula = verb2 ~ 1 + verb1_star, data = wiscsub, na.action = na.exclude) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.5305 -3.0362 0.2526 2.7147 12.5020 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 25.41534 0.29831 85.20 &lt;2e-16 *** ## verb1_star 0.75495 0.05149 14.66 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.261 on 202 degrees of freedom ## Multiple R-squared: 0.5156, Adjusted R-squared: 0.5132 ## F-statistic: 215 on 1 and 202 DF, p-value: &lt; 2.2e-16 Note: Mean centering should be used to aid interpretation. Historically, it has been suggested that mean centering will reduce multicollinearity, however this is not the case. See for more information. 6.4.1 Interpreting Model Parameters Note that the estimate for the slope \\(b_1\\) stays the same, but the estimate for the intercept is different. This is because the variable ‘verb1_star’ equals 0 when a child has an average 1st grade verbal score. Therefore the expected value for the 2nd grade verbal score, for a child with an average 1st grade verbal score, is 25.41534. 6.4.2 Plotting Regression Line ggplot(data=wiscsub, aes(x=verb1_star,y=verb2)) + geom_point(size = 2, shape=19) + geom_smooth(method=lm,se=TRUE,fullrange=TRUE,colour=&quot;red&quot;, size=2) + labs(x= &quot;Sample-Centered Verbal Ability Grade 1&quot;, y= &quot;Verbal Ability Grade 2&quot;) + xlim(-20,20) + ylim(0,50) + #theme with white background theme_bw() + #eliminate background, gridlines, and chart border theme( plot.background = element_blank() ,panel.grid.major = element_blank() ,panel.grid.minor = element_blank() ,panel.border = element_blank() ) + #draws x and y axis line theme(axis.line = element_line(color = &#39;black&#39;)) + #set size of axis labels and titles theme(axis.text = element_text(size=12), axis.title = element_text(size=14)) ## `geom_smooth()` using formula = &#39;y ~ x&#39; Note the change of scale on the x-axis. "],["6.5-multiple-linear-regression.html", "6.5 Multiple Linear Regression", " 6.5 Multiple Linear Regression Now, let’s include a second predictor. We have information on the number of years of education for the children’s fathers, variable daded. The values in daded indicate the number of years of education each father completed. First, let’s take a look at the distribution of this new predictor variable. psych::describe(wiscsub$daded) ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 204 10.81 2.7 11.5 11 2.97 5.5 18 12.5 -0.36 0.01 0.19 ggplot(data=wiscsub, aes(x=daded)) + geom_histogram(binwidth=2.5, fill=&quot;white&quot;, color=&quot;black&quot;) + xlim(0,20) + xlab(&quot;Number of Years of Father&#39;s Education&quot;) + ylab(&quot;Count&quot;) + theme_classic() ## Warning: Removed 2 rows containing missing values (`geom_bar()`). And the relation between Grade 2 verbal scores and daded. ggplot(data=wiscsub, aes(x=daded, y = verb2)) + geom_point() + xlim(0,20) + ylim(0,50) + xlab(&quot;Father&#39;s Education (Years)&quot;) + ylab(&quot;Verbal Ability Grade 2&quot;) + theme_classic() 6.5.1 Regression Equation Our model now becomes \\[ verb_{2i} = b_01_{i} + b_1verb^{*}_{1i} + b_2daded^{*}_{i} + \\epsilon_{i}\\] where \\(verb^{*}_{1i}\\) is the sample-centered version of \\(verb_{1i}\\), and \\(daded^{*}_{i}\\) is the sample-centered version of \\(daded_{i}\\). The slope, \\(b_2\\) is the expected difference in grade 2 verbal score for each 1 year difference in father’s education. We can also center the daded variable. # Calculate mean-centered version of father&#39;s education variable wiscsub$daded_star &lt;- wiscsub$daded - mean(wiscsub$daded) 6.5.2 Fit Model in R model4 &lt;- lm(verb2 ~ 1 + verb1_star + daded_star, data = wiscsub, na.action = na.exclude) summary(model4) ## ## Call: ## lm(formula = verb2 ~ 1 + verb1_star + daded_star, data = wiscsub, ## na.action = na.exclude) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.4354 -2.9189 -0.1542 2.3746 11.1678 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 25.41534 0.29069 87.430 &lt; 2e-16 *** ## verb1_star 0.66786 0.05626 11.872 &lt; 2e-16 *** ## daded_star 0.41454 0.12108 3.424 0.000749 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.152 on 201 degrees of freedom ## Multiple R-squared: 0.5422, Adjusted R-squared: 0.5377 ## F-statistic: 119.1 on 2 and 201 DF, p-value: &lt; 2.2e-16 Now we have an intercept and two slopes. 6.5.3 Path Diagram semPlot::semPaths(model4, what = &quot;paths&quot;) 6.5.3.1 Interpreting Model Parameters \\(b_0\\) is the expected value of the outcome variable when all other variables are 0. Therefore, in this case, \\(b_0\\) is the expected Grade 2 verbal score for a child with an average Grade 1 verbal score (i.e. \\(verb^{*}_{1i}\\) = 0) and whose father had an average education (i.e. \\(daded^{*}_{i}\\) = 0, \\(\\bar{daded_{i}}\\) = 10.81 years of education. \\(b_1\\) is the expected difference in the outcome for a 1-unit difference in \\(x_{1i}\\). In this example (i.e. ‘model4’), \\(b_1\\) is the expected difference in Grade 2 verbal score (outcome variable, \\(y_i\\) = \\(verb_{2i}\\)) for a 1 point difference in the Grade 1 verbal score (\\(x_{1i}\\) = \\(verb^{*}_{1i}\\)), holding constant the level of father’s education. \\(b_2\\) is the expected difference in the outcome for a 1-unit difference in \\(x_{2i}\\). For this example (i.e. ‘model4’), \\(b_2\\) is the expected difference in Grade 2 verbal score (outcome variable, \\(y_i\\) = \\(verb_{2i}\\)) for each year difference in father’s education (\\(x_{2i}\\) = \\(daded^{*}_i\\)), holding constant in Grade 1 verbal score. 6.5.4 A Note on Interpretation The distinguishing feature for interpretation in linear models without interactions or higher-order terms is that the effect of a given change in an independent variable is the same regardless of the value of that variable (at the start of its change) and regardless of the level of the other variables in the model. Interpretation only needs to specify which variable is changing, by how much, and that other variables are being held constant. In regard to our last example, years of father’s education does impact the relationship between Grade 1 and 2 verbal scores. Likewise, the effect of father’s education on Grade 1 verbal scores does not depend on Grade 1 verbal scores. Said in a different way, no matter what a child’s Grade 1 verbal score was, the effect of father’s education on Grade 2 verbal scores is the same. Consider another model with categorical and continuous predictors, grad and verb1_star, respectively. library(&quot;ggiraphExtra&quot;) model4b &lt;- lm(verb2 ~ 1 + verb1_star + grad, data = wiscsub, na.action = na.exclude) ggPredict(model4b,se=TRUE,interactive=FALSE) Notice in the plot, whether a student’s father graduated HS does not impact the relationship between Grade 1 and 2 verbal scores. Differences in Grade 1 and 2 verbal scores are not dependent on whether or not the father graduated HS. Most importantly, using the coefficients themselves we can easily interpret the model parameters. "],["6.6-categorical-variable-interaction.html", "6.6 Categorical Variable Interaction", " 6.6 Categorical Variable Interaction Ok, let’s move on to the topic of an interaction which uses the product of two predictor variables as a new predictor. Working up a slightly different example with the ‘grad’ variable (whether dad graduated high school), \\[ verb_{2i} = b_01_i + b_1verb^{*}_{1i} + b_2grad_{i} + b_3(verb^{*}_{1i}grad_{i}) + \\epsilon_{i}\\] Where \\(verb^{*}_{1i}\\) is the mean-centered version of \\(verb_{1i}\\), and \\(grad_i\\) is a dummy coded variable that equals 0 if the child’s father did not graduate high school, and equals 1 if the child’s father did graduate high school. We did not sample-mean center \\(grad_i\\) in this example because a value of 0 already has substantive meaning for the current example (i.e. when \\(grad_i\\) equals 0, the father did not graduate high school). 6.6.1 Interaction as Moderation Often, we describe phenomena in terms of moderation; or that the relation between two variables (i.e. \\(y_i\\) and \\(x_{1i}\\)) is moderated by a third variable (i.e. \\(x_{2i}\\)). For example, the relation between Grade 1 and Grade 2 verbal scores may be moderated by father’s graduation status. More specifically, the relation between 1st and 2nd grade verbal score may be different for children whose fathers’ did not or did graduate from high school. The inclusion of product terms (i.e. interactions) allows for a direct investigation of a moderation hypothesis. 6.6.1.1 Choosing a Moderator When we use a product term, we should define one of the variables as the moderator and one of the variables as the predictor of interest. Let’s call \\(verb^{*}_{1i}\\) the predictor of interest, and \\(grad_{i}\\) the moderator. 6.6.2 Moderation by Categorical Variable When the moderator is a dummy variable then the form of the moderation becomes fairly simple; we will have one equation for \\(grad_{i} = 0\\), and a second equation for \\(grad_i = 1\\). 6.6.2.1 Rewriting Equation To illustrate the notion of two equations, let’s rewrite the regression equation \\[ verb_{2i} = b_01_i + b_1verb^{*}_{1i} + b_2grad_{i} + b_3(verb^{*}_{1i}grad_{i}) + \\epsilon_{i}\\] as two separate regression equations, one for fathers who graduated from highschool and one for fathers that did not. We can accomplish this by plugging in \\(0\\) and \\(1\\) into the regression equation and rearranging some of the terms. Doing so we get Equation for Students whose father Graduated Highschool \\[ verb_{2i} = (b_0 + b_2) + (b_1 + b_3)verb^{*}_{1i} + \\epsilon_{i}\\] Equation for Students whose father Did Not Graduate from Highschool \\[ verb_{2i} = b_0 + b_1verb^{*}_{1i} + \\epsilon_{i}\\] 6.6.3 Interpretation Without an interaction, our linear regression model assumes that the only difference between the regression line for each group (graduate HS vs not) is the intercept. That is, it assumes that the relationship between verbal scores at Grades 1 and 2 is the same for both groups. Children Whose father’s Did Not Graduate HS The expected Grade 2 verbal score for a child whose father did not graduate high school and who had an average Grade 1 verbal score is \\(b_0\\). Also, for a child whose father did not graduate high school, \\(b_1\\) is the expected difference in their Grade 2 verbal score for a one-point difference in their Grade 1 verbal score. Children Whose father’s Did Not Graduate HS The parameter estimates \\(b_0\\) and \\(b_1\\) maintain their interpretation from before. But now each of them is moderated (i.e. shifted or altered) by \\(b_2\\) or \\(b_3\\). Specifically, the expected Grade 2 verbal score for a child whose father did graduate high school and who earned an average Grade 1 verbal score is \\(b_0 + b_2\\). And, for a child whose father did graduate high school, \\(b_1 + b_3\\) is the expected difference in their Grade 2 verbal score for a one-point change in their Grade 1 verbal score. 6.6.4 Fit Regression Model in R OK - let’s fit the model! Note that within this model we use the code I(verb1_star * grad). This produces the interaction term within the model. The wrapper function I() indicates to R to perform this data computation as-is, otherwise we would need to perform this computation (i.e. the multiplication of verb1_star by grad) outside of the function lm(). model5 &lt;- lm(verb2 ~ 1 + verb1_star + grad + I(verb1_star*grad), data = wiscsub, na.action = na.exclude) summary(model5) ## ## Call: ## lm(formula = verb2 ~ 1 + verb1_star + grad + I(verb1_star * grad), ## data = wiscsub, na.action = na.exclude) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.3433 -3.0761 -0.0825 2.5689 10.7289 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 25.2663 0.3416 73.956 &lt;2e-16 *** ## verb1_star 0.7861 0.0604 13.015 &lt;2e-16 *** ## grad 1.4632 0.8107 1.805 0.0726 . ## I(verb1_star * grad) -0.2430 0.1324 -1.836 0.0678 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.232 on 200 degrees of freedom ## Multiple R-squared: 0.5268, Adjusted R-squared: 0.5197 ## F-statistic: 74.22 on 3 and 200 DF, p-value: &lt; 2.2e-16 6.6.5 Path Diagram semPlot::semPaths(model5, what = &quot;paths&quot;) The parameter estimates from this model indicate that, for children whose father did not graduate high school, the expected Grade 2 verbal score for a child that earned an average 1st grade verbal score equals 25.2663 (\\(b_0\\)). Also, for children whose father did not graduate high school, a 1-point difference in their Grade 1 verbal score is expected to correspond with a 0.7861 (\\(b_1\\)) point difference in the Grade 2 verbal score. Moreover, the parameter estimates indicate that, for children whose father did graduate high school, the expected Grade 2 verbal score for a child that earned an average Grade 1 verbal score is 25.2663 + 1.4632 = 26.7295 (\\(b_0 + b_2\\)). Also, for children whose father graduated high school, a 1-point difference in their Grade 1 verbal score is expected to correspond with a (\\(b_1 + b_3\\)) = 0.7861 - 0.2430 = 0.5431 point difference in the Grade 2 verbal score. Even though the interaction is not significant, we can plot it for illustrating the moderation effect: #plot of moderation ggplot(data=wiscsub, aes(y=verb2,x=verb1_star, color = factor(grad))) + geom_jitter() + stat_smooth(method=&#39;lm&#39;, se=TRUE, fullrange=TRUE) + xlab(&quot;1st Grade Verbal Score&quot;) + ylab(&quot;2nd Grade Verbal Score&quot;) + guides(color=guide_legend(title=&quot;HS Grad&quot;)) + theme_bw() ## `geom_smooth()` using formula = &#39;y ~ x&#39; The example from ‘model5’ contained an interaction using a dummy variable (i.e., \\(grad_i\\)). Interactions may also occur between two continuous variables (i.e., \\(verb^{*}_{1i}\\) and \\(daded^{*}_{i}\\)). We will not cover here, but note that it is still very useful to consider and communicate those interactions as moderation. There are many resources on interactions of two (or more) continuous variables. "]]
